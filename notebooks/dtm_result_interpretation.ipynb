{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import operator\n",
    "import csv\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.cbook as cbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_prefix = 'run_01_01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_root = os.path.join('..')\n",
    "dir_data = os.path.join(dir_root, 'data_processing')\n",
    "dir_tickers = os.path.join(dir_data, 'tickers')\n",
    "dir_run = os.path.join(dir_data, 'runs', run_prefix)\n",
    "dir_result_dtm = os.path.join(dir_run, 'results', 'lda-seq')\n",
    "dir_result_interpretation = os.path.join(dir_run, 'interpretation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read topics amount and time slices amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "number_of_topics = None\n",
    "time_slices = None\n",
    "with open(os.path.join(dir_result_dtm, 'info.dat'), 'r') as f_r:\n",
    "    for text_line in f_r:\n",
    "        data = text_line.strip().split(' ')\n",
    "        if data[0] == 'NUM_TOPICS':\n",
    "            number_of_topics = int(data[1])\n",
    "        if data[0] == 'SEQ_LENGTH':\n",
    "            time_slices = int(data[1])\n",
    "            \n",
    "print(number_of_topics) \n",
    "print(time_slices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_list = []\n",
    "with open(os.path.join(dir_run, \"%s-terms.dat\" % run_prefix), 'r') as f_r:\n",
    "    for text_line in f_r:\n",
    "        term = text_line.strip()\n",
    "        if len(term):\n",
    "            terms_list.append(term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get top terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "take_top = 10\n",
    "flag_is_exp = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_result_top_terms = os.path.join(dir_result_interpretation, 'top_terms')\n",
    "if not os.path.exists(dir_result_top_terms):\n",
    "    os.makedirs(dir_result_top_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sorted_terms(topic_id):\n",
    "    #create time slice data\n",
    "    all_time_slices_data_unsorted = {}\n",
    "    for time_slice in range(time_slices):\n",
    "        all_time_slices_data_unsorted[time_slice] = list()\n",
    "\n",
    "    #open terms distribution file\n",
    "    with open(os.path.join(dir_result_dtm, \"topic-%s-var-e-log-prob.dat\" % topic_id), 'r') as f_r:\n",
    "        distributions = f_r.readlines()\n",
    "        dist_x = 0\n",
    "        #for each term read one line\n",
    "        for term in terms_list:\n",
    "            # for each time slice\n",
    "            for time_slice in range(time_slices):\n",
    "                float_text = distributions[dist_x].strip()\n",
    "                if len(float_text):\n",
    "                    if flag_is_exp:\n",
    "                        term_probability = math.exp(float(float_text))\n",
    "                    else:\n",
    "                        term_probability = float(float_text)\n",
    "                    all_time_slices_data_unsorted[time_slice].append((term, term_probability))\n",
    "                dist_x += 1\n",
    "\n",
    "    # sort list\n",
    "    all_time_slices_data_sorted = []\n",
    "    for time_slice in range(time_slices):\n",
    "        all_time_slices_data_sorted.append(sorted(all_time_slices_data_unsorted[time_slice],\n",
    "                                                  key=operator.itemgetter(1),\n",
    "                                                  reverse=True))\n",
    "    \n",
    "    return all_time_slices_data_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_sorted_terms(topic_id, all_time_slices_data_sorted):\n",
    "    #save as csv without probabilities for better visualisation and limited to take top\n",
    "    file_csv_sorted_terms_without_probabilities = os.path.join(\n",
    "        dir_result_top_terms, \"%s-top-n-terms.csv\" % topic_id)\n",
    "    with open(file_csv_sorted_terms_without_probabilities, 'w') as f_w:\n",
    "        for i in range(take_top):\n",
    "            terms_at_position_list = []\n",
    "            for time_slice in range(time_slices):\n",
    "                terms_at_position_list.append(all_time_slices_data_sorted[time_slice][i][0])\n",
    "            f_w.write(\"%s\\n\" % (','.join(terms_at_position_list)))\n",
    "\n",
    "    #save as csv with probabilities\n",
    "    file_csv_sorted_terms_with_probabilities = os.path.join(\n",
    "        dir_result_top_terms, \"%s-terms_sorted.csv\" % topic_id)\n",
    "    with open(file_csv_sorted_terms_with_probabilities, 'w') as f_w:\n",
    "        for i in range(len(terms_list)):\n",
    "            terms_at_position_list = list()\n",
    "            for time_slice in range(time_slices):\n",
    "                terms_at_position_list.append(all_time_slices_data_sorted[time_slice][i][0])\n",
    "                terms_at_position_list.append(all_time_slices_data_sorted[time_slice][i][1])\n",
    "            f_w.write(\"%s\\n\" % (','.join(map(str, terms_at_position_list))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run run_01_00 topic 0\n",
      "Run run_01_00 topic 1\n",
      "Run run_01_00 topic 2\n",
      "Run run_01_00 topic 3\n",
      "Run run_01_00 topic 4\n",
      "Run run_01_00 topic 5\n",
      "Run run_01_00 topic 6\n",
      "Run run_01_00 topic 7\n",
      "Run run_01_00 topic 8\n",
      "Run run_01_00 topic 9\n",
      "Run run_01_00 topic 10\n",
      "Run run_01_00 topic 11\n",
      "Run run_01_00 topic 12\n",
      "Run run_01_00 topic 13\n",
      "Run run_01_00 topic 14\n"
     ]
    }
   ],
   "source": [
    "for i in range(number_of_topics):\n",
    "    print(\"Run %s topic %s\" % (run_prefix, i))\n",
    "    sorted_terms = get_sorted_terms(\"%03d\" % i)\n",
    "    store_sorted_terms(\"%03d\" % i, sorted_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get gammas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read reports in list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports_list = []\n",
    "with open(os.path.join(dir_run, \"%s-documents.dat\" % run_prefix), 'r') as f_r:\n",
    "    for text_line in f_r:\n",
    "        report_name = text_line.strip()\n",
    "        if len(report_name):\n",
    "            reports_list.append(report_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_for_reports = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(dir_result_dtm, 'gam.dat'), 'r') as f_r:\n",
    "    distributions = f_r.readlines()\n",
    "    dist_x = 0\n",
    "    #for each term read one line\n",
    "    for report in reports_list:\n",
    "        # for each topic\n",
    "        normalization_sum = 0\n",
    "        topics_probabilities = []\n",
    "        topics_probabilities_normalize = []\n",
    "\n",
    "        for topic in range(number_of_topics):\n",
    "            float_text = distributions[dist_x].strip()\n",
    "            topic_probability = float(float_text)\n",
    "            topics_probabilities.append(topic_probability)\n",
    "            normalization_sum += topic_probability\n",
    "            dist_x += 1\n",
    "\n",
    "        for prob in topics_probabilities:\n",
    "            topics_probabilities_normalize.append(prob / normalization_sum)\n",
    "        gamma_for_reports[report] = topics_probabilities_normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store to scv file with gammas for every report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_result_topics_proportions = os.path.join(dir_result_interpretation, 'topics_proportions')\n",
    "if not os.path.exists(dir_result_topics_proportions):\n",
    "    os.makedirs(dir_result_topics_proportions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_csv_topic_proportion = os.path.join(dir_result_topics_proportions, 'topic-proportion-for-docs.csv')\n",
    "with open(file_csv_topic_proportion, 'w') as f_w:\n",
    "    for report in gamma_for_reports:\n",
    "        f_w.write(\"%s,%s\\n\" % (report, (','.join(map(str, gamma_for_reports[report])))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate reports by company and stare them by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_for_reports_by_years = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for report in gamma_for_reports:\n",
    "    report_data = report.split('-')\n",
    "    company_name = report_data[0]\n",
    "    report_year = report_data[1]\n",
    "    if report_year not in gamma_for_reports_by_years:\n",
    "        gamma_for_reports_by_years[report_year] = {}\n",
    "    if company_name not in gamma_for_reports_by_years[report_year]:\n",
    "        gamma_for_reports_by_years[report_year][company_name] = []\n",
    "    gamma_for_reports_by_years[report_year][company_name].append(gamma_for_reports[report])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average reports for every company's year, re-normalize gammas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_gammas = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for report_year in gamma_for_reports_by_years:\n",
    "    final_gammas[report_year] = {}\n",
    "    for ticker in gamma_for_reports_by_years[report_year]:\n",
    "        average_distribution = [0] * number_of_topics\n",
    "        for report in gamma_for_reports_by_years[report_year][ticker]:\n",
    "            for topic in range(number_of_topics):\n",
    "                average_distribution[topic] += report[topic]\n",
    "\n",
    "        reports_amount = len(gamma_for_reports_by_years[report_year][ticker])\n",
    "        normalization_sum = 0\n",
    "        for topic in range(number_of_topics):\n",
    "            average_distribution[topic] /= reports_amount\n",
    "            normalization_sum += average_distribution[topic]\n",
    "\n",
    "        for topic in range(number_of_topics):\n",
    "            average_distribution[topic] /= normalization_sum\n",
    "\n",
    "        final_gammas[report_year][ticker] = average_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define year series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_series = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save topic destribution for every compnay over the years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for report_year in final_gammas:\n",
    "    # append year to year series\n",
    "    year_series.append(int(report_year))\n",
    "    \n",
    "    tickers_list = sorted(list(final_gammas[report_year].keys()))\n",
    "\n",
    "    file_csv_topic_proportion_for_year = os.path.join(dir_result_topics_proportions, \"topic-proportion-%s.csv\" % report_year)\n",
    "    with open(file_csv_topic_proportion_for_year, 'w') as f_w:\n",
    "        f_w.write(\"%s\\n\" % ','.join(tickers_list))\n",
    "        for topic_id in range(number_of_topics):\n",
    "            topic_values = list()\n",
    "            for company_name in tickers_list:\n",
    "                topic_values.append(final_gammas[report_year][company_name][topic_id])\n",
    "            f_w.write(\"%s\\n\" % ','.join(list(map(str, topic_values))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "re-sort years series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year_series.sort()\n",
    "year_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get gammas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_result_weight_matrices = os.path.join(dir_result_interpretation, 'weight_matrices')\n",
    "if not os.path.exists(dir_result_weight_matrices):\n",
    "    os.makedirs(dir_result_weight_matrices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get weight matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in year_series:\n",
    "    tickers_list = []\n",
    "    file = os.path.join(dir_result_topics_proportions, \"topic-proportion-%d.csv\" % year)\n",
    "    line_index = 0\n",
    "    rows = []\n",
    "    with open(file, 'r') as f_r:\n",
    "        for text_line in f_r:\n",
    "            if line_index == 0:\n",
    "                tickers_list = text_line.strip().split(',')\n",
    "            else:\n",
    "                row = list(map(float, text_line.strip().split(',')))\n",
    "                rows.append(row)\n",
    "            line_index += 1\n",
    "\n",
    "    a = np.array(rows)\n",
    "    x_lists = list()\n",
    "\n",
    "    for topic_id in range(number_of_topics):\n",
    "        #build b matrix\n",
    "        b_list = [0.] * number_of_topics\n",
    "        b_list[topic_id] = 1.0\n",
    "        b = np.array(b_list)\n",
    "        x = np.linalg.lstsq(a, b, rcond = None)\n",
    "        x_lists.append(list(x[0]))\n",
    "    file = os.path.join(dir_result_weight_matrices, \"topic-weights-%d.csv\" % year)\n",
    "    with open(file, 'w') as f_w:\n",
    "        for index in range(len(tickers_list)):\n",
    "            entry = list()\n",
    "            entry.append(tickers_list[index])\n",
    "            for topic_id in range(number_of_topics):\n",
    "                entry.append(x_lists[topic_id][index])\n",
    "            f_w.write(\"%s\\n\" % ','.join(list(map(str, entry))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_amount_to_distribute = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_returns = pd.read_csv(os.path.join(dir_run, \"%s-returns.csv\" % run_prefix))\n",
    "df_returns.rename(index=str, columns={'Unnamed: 0': 'Date'}, inplace=True)\n",
    "df_returns['Date'] = pd.to_datetime(df_returns['Date'])\n",
    "df_returns.set_index('Date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_result_portfolio = os.path.join(dir_result_interpretation, 'portfolio')\n",
    "if not os.path.exists(dir_result_portfolio):\n",
    "    os.makedirs(dir_result_portfolio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write data for topic: 0 with result 0.6981952625116509\n",
      "Finish with topic: 0 with total result 0.6981952625116509\n",
      "Write data for topic: 1 with result 1.9022716790635628\n",
      "Finish with topic: 1 with total result 2.6004669415752137\n",
      "Write data for topic: 2 with result 6.452067625056078\n",
      "Finish with topic: 2 with total result 9.05253456663129\n",
      "Write data for topic: 3 with result 3.814517475098143\n",
      "Finish with topic: 3 with total result 12.867052041729433\n",
      "Write data for topic: 4 with result 3.995703343684413\n",
      "Finish with topic: 4 with total result 16.86275538541385\n",
      "Write data for topic: 5 with result 3.324067865324619\n",
      "Finish with topic: 5 with total result 20.186823250738467\n",
      "Write data for topic: 6 with result 1.049055835475466\n",
      "Finish with topic: 6 with total result 21.235879086213934\n",
      "Write data for topic: 7 with result 4.705968902581053\n",
      "Finish with topic: 7 with total result 25.941847988794986\n",
      "Write data for topic: 8 with result 3.4673227552069883\n",
      "Finish with topic: 8 with total result 29.409170744001976\n",
      "Write data for topic: 9 with result 3.628572793492624\n",
      "Finish with topic: 9 with total result 33.0377435374946\n",
      "Write data for topic: 10 with result 0.15449028370920742\n",
      "Finish with topic: 10 with total result 33.19223382120381\n",
      "Write data for topic: 11 with result 2.0647962930614185\n",
      "Finish with topic: 11 with total result 35.25703011426523\n",
      "Write data for topic: 12 with result 4.2808625857737175\n",
      "Finish with topic: 12 with total result 39.537892700038945\n",
      "Write data for topic: 13 with result 1.6365900066767056\n",
      "Finish with topic: 13 with total result 41.17448270671565\n",
      "Write data for topic: 14 with result 4.357332231852139\n",
      "Finish with topic: 14 with total result 45.53181493856779\n"
     ]
    }
   ],
   "source": [
    "all_returns_combined = {}\n",
    "for topic_id in range(number_of_topics):\n",
    "    #read all returns\n",
    "    returns_data = dict()\n",
    "    returns_companies_set = set()\n",
    "    result = list()\n",
    "\n",
    "    amount_to_distribute = initial_amount_to_distribute\n",
    "    last_date = None\n",
    "    #Year loop\n",
    "    for year in year_series:\n",
    "        #Get year matrix\n",
    "        file = os.path.join(dir_result_weight_matrices, \"topic-weights-%d.csv\" % year)\n",
    "        weights_data = dict()\n",
    "        weights_tickers = list()\n",
    "        with open(file, 'r') as f_r:\n",
    "            for text_line in f_r:\n",
    "                raw_weight = text_line.strip().split(',')\n",
    "                ticker = raw_weight[0]\n",
    "                weights_tickers.append(ticker)\n",
    "                weights_data[ticker] = list(map(float, raw_weight[1:]))\n",
    "\n",
    "        #days loop\n",
    "        is_first_day = True\n",
    "        a = 0 # ????\n",
    "        volume_data = dict()\n",
    "        current_year = year + 1\n",
    "        inrange = None\n",
    "        if current_year == 2018:\n",
    "            inrange = pd.date_range(start=\"01.01.%s\" % (year + 1), end=\"30.06.%s\" % (year + 1), freq='D')\n",
    "        else:\n",
    "            inrange = pd.date_range(start=\"01.01.%s\" % (year + 1), end=\"31.12.%s\" % (year + 1), freq='D')\n",
    "        for date in inrange:\n",
    "            date_return_data = df_returns.loc[date]\n",
    "            # distribute available amount for the first day of the year\n",
    "            if is_first_day:\n",
    "                is_first_day = False\n",
    "                #print(\"re-distribute new amount: %s\" % amount_to_distribute)\n",
    "                for ticker in weights_tickers:\n",
    "                    volume_data[ticker] = 1. * amount_to_distribute * weights_data[ticker][topic_id]\n",
    "                    #print(\"First Day Ticker: %s; volume: %f\" % (company_name,  volume_data[company_name]))\n",
    "            #propagate further\n",
    "            else:\n",
    "                for ticker in weights_tickers:\n",
    "                    return_changes = 1.\n",
    "                    if ticker in date_return_data:\n",
    "                        return_changes = date_return_data[ticker]\n",
    "                    volume_data[ticker] *= return_changes\n",
    "                    #print(\"%s Ticker: %s; volume: %f, return changes %f\" % (date, company_name,  volume_data[company_name], return_changes))\n",
    "\n",
    "\n",
    "            #totaling for a day\n",
    "            amount_to_distribute = 0\n",
    "            for ticker in weights_tickers:\n",
    "                amount_to_distribute += volume_data[ticker]\n",
    "\n",
    "            #print(\"%s Return for topic id = %d is %f\" % (date, topic_id, amount_to_distribute))\n",
    "            last_date = date\n",
    "            if date not in all_returns_combined:\n",
    "                all_returns_combined[date] = 0\n",
    "            all_returns_combined[date] += amount_to_distribute\n",
    "            result.append((date, amount_to_distribute))\n",
    "\n",
    "    print(\"Write data for topic: %s with result %s\" % (topic_id, amount_to_distribute))\n",
    "    print(\"Finish with topic: %s with total result %s\" % (topic_id, all_returns_combined[last_date]))\n",
    "\n",
    "    with open(os.path.join(dir_result_portfolio, \"results-%s.csv\" % topic_id), 'w') as f_w:\n",
    "        f_w.write(\"Date,Value\\n\")\n",
    "        for entry in result:\n",
    "            f_w.write(\"%s,%f\\n\" % (entry[0], entry[1]))\n",
    "            \n",
    "with open(os.path.join(dir_result_portfolio, \"results-all.csv\"), 'w') as f_w:\n",
    "    f_w.write(\"Date,Value\\n\")\n",
    "    for date in sorted(all_returns_combined.keys()):\n",
    "        f_w.write(\"%s,%f\\n\" % (date, all_returns_combined[date]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\n",
    "    \"#771155\",\n",
    "    \"#AA4488\",\n",
    "    \"#CC99BB\",\n",
    "    \"#114477\",\n",
    "    \"#4477AA\",\n",
    "    \"#77AADD\",\n",
    "    \"#117777\",\n",
    "    \"#44AAAA\",\n",
    "    \"#77CCCC\",\n",
    "    \"#117744\",\n",
    "    \"#44AA77\",\n",
    "    \"#88CCAA\",\n",
    "    \"#777711\",\n",
    "    \"#AAAA44\", \n",
    "    \"#DDDD77\", \n",
    "    \"#774411\", \n",
    "    \"#AA7744\", \n",
    "    \"#DDAA77\", \n",
    "    \"#771122\", \n",
    "    \"#AA4455\",\n",
    "    \"#DD7788\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
