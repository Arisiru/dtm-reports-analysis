{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import operator\n",
    "import math\n",
    "import multiprocessing\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set root directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_root = os.path.join('..')\n",
    "dir_data_raw = os.path.join(dir_root, \"data_raw\")\n",
    "dir_data_processing = os.path.join(dir_root, \"data_processing\")\n",
    "dir_data_runs = os.path.join(dir_root, \"data_runs\")\n",
    "dir_ticker_prices_source = os.path.join(dir_data_raw, \"prices\", \"ready\")\n",
    "dir_ticker_prices_destination = os.path.join(dir_data_processing, \"prices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set reports directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_reports_txt = os.path.join(dir_data_raw, \"reports_txt\")\n",
    "dir_reports_words = os.path.join(dir_data_processing, \"reports_words\")\n",
    "dir_reports_terms = os.path.join(dir_data_processing, \"reports_terms\")\n",
    "dir_reports_grams = os.path.join(dir_data_processing, \"reports_gramms\")\n",
    "dir_reports_ready =  os.path.join(dir_data_processing, \"reports_ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set terms directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_terms_eliminated = os.path.join(dir_data_processing, \"terms_elemenated\")\n",
    "dir_terms_counts = os.path.join(dir_data_processing, \"terms_counts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set experiment dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_start = dt.datetime.strptime('2005-01-01', '%Y-%m-%d')\n",
    "date_end = dt.datetime.strptime('2021-06-30', '%Y-%m-%d') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_extension_start = dt.datetime.strptime('2018-01-01', '%Y-%m-%d')\n",
    "date_extension_end = dt.datetime.strptime('2021-06-30', '%Y-%m-%d') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_series = list(range(date_start.year, date_end.year)) \n",
    "year_extension_series = list(range(date_extension_start.year, date_extension_end.year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020]\n",
      "[2018, 2019, 2020]\n"
     ]
    }
   ],
   "source": [
    "print(year_series)\n",
    "print(year_extension_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set report name RegExt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_report_name_reg_exp = \"(?P<ticker>[A-Z1-9]+)[_-](?P<type>[A-Z]+)(?P<number>[1-9A-Z]*)(?P<subnumber>[_-]+[0-9]+)?[_-](?P<year>[0-9]{4})[_-](?P<p_year>[0-9]{4})\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_debug = False\n",
    "flag_terms_filter_debug = False\n",
    "\n",
    "flag_fix_reports_names = False\n",
    "\n",
    "flag_extend_stopwords = True\n",
    "flag_test_report_names = True\n",
    "flag_filtering_with_bigramms = True\n",
    "\n",
    "flag_rerun_text_2_words = True\n",
    "flag_rerun_words_2_terms = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set tickers for which you want to run preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_to_run = {\n",
    "    'FTSE' : ['WPP'],\n",
    "    'DJIA' : ['TVE']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_tickers_for_analysis = set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reports pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First remove all but English letters and re-save reports as a sequence of lower case words consist only from letters a-z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "regexp_to_remove = re.compile(r\"[\\dâºâãï½ã\\_]\")\n",
    "regexp_to_keep = re.compile(r\"[^a-z\\s]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_raw_text_2_words(file_report_path):\n",
    "    words = []\n",
    "    try:\n",
    "        f_r = open(file_report_path, encoding=\"utf8\", errors='ignore')\n",
    "    except OSError:\n",
    "        print (\"Could not open/read file: %s\" % file_report_path)\n",
    "        return words\n",
    "    except UnicodeDecodeError:\n",
    "        print (\"Unicode not open/read file: %s\" % file_report_path)\n",
    "        return words\n",
    "    except OSError:\n",
    "        print (\"Unknown open/read file: %s\" % file_report_path)\n",
    "        return words\n",
    "        \n",
    "        \n",
    "    for text_line in f_r:\n",
    "        cleaned_text = re.sub(regexp_to_keep, \" \", text_line.lower())\n",
    "        words_in_line = re.split(\"\\W+\", cleaned_text)\n",
    "        for possible_word in words_in_line:\n",
    "            word = possible_word.strip()\n",
    "            if len(word) > 1:\n",
    "                words.append(word)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reports_2_words_processing(dir_findex, ticker, findex, years_set, check_for_inclussion=False):\n",
    "    ticker_code = \"%s_%s\" % (ticker, findex)\n",
    "    \n",
    "    if check_for_inclussion and ticker_code not in possible_tickers_for_analysis:\n",
    "        print(\"Skip %s\" % ticker_code)\n",
    "        return\n",
    "    \n",
    "    dir_ticker = os.path.join(dir_findex, ticker)\n",
    "    good_documents_amount = 0\n",
    "    empty_documents_amount = 0\n",
    "    bad_years = set()\n",
    "    if os.path.isdir(dir_ticker):\n",
    "        for report_file_name in os.listdir(dir_ticker):\n",
    "            if report_file_name == \".DS_Store\":\n",
    "                continue\n",
    "            match = re.search(check_report_name_reg_exp, report_file_name)\n",
    "            if not match:\n",
    "                print(\"filename %s doesn't fit pattern\" % report_file_name)\n",
    "            else:\n",
    "                year = int(match.group(\"p_year\"))\n",
    "                if year not in years_set:\n",
    "                    bad_years.add(year)\n",
    "                    continue\n",
    "                list_words = convert_raw_text_2_words(os.path.join(dir_ticker, report_file_name))\n",
    "                if len(list_words):\n",
    "                    good_documents_amount += 1\n",
    "                    new_file_name = \"%s_%s.txt\" % (year, good_documents_amount)\n",
    "                    new_path = os.path.join(dir_reports_words, findex, ticker)\n",
    "                    if not os.path.exists(new_path):\n",
    "                        os.makedirs(new_path)\n",
    "                    with open(os.path.join(new_path, new_file_name), \"w\") as f_w:\n",
    "                        f_w.write(\"%s\" % ' '.join(list_words))\n",
    "                else: \n",
    "                    empty_documents_amount += 1\n",
    "                    print(\"report %s is empty after cleaning\" % report_file_name)\n",
    "        print(\"Done on %s, reports: %s, empty: %s, extra years available: [%s]\" % \n",
    "              (ticker_code, good_documents_amount, empty_documents_amount, \", \".join(map(str, sorted(bad_years)))))                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run previously defined function words_2_terms_processing in pool of 4 processes to speedup the cleaning, The following cell takes quite a while, be carefull and do not rerun it without a reason, results are stored at file system\n",
    "\n",
    "**please make sure that flag_rerun_text_2_words is set to True if you want to run/re-run this preprosessing step**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done on WPP_FTSE, reports: 67, empty: 0, extra years available: [1998, 1999, 2000, 2001, 2002, 2003, 2004, 2021]\n",
      "Done on TVE_DJIA, reports: 35, empty: 0, extra years available: []\n"
     ]
    }
   ],
   "source": [
    "if flag_rerun_text_2_words:\n",
    "    filtering_years_set = set(year_series)\n",
    "    for findex in tickers_to_run.keys():\n",
    "        dir_findex = os.path.join(dir_reports_txt, findex)\n",
    "        if os.path.isdir(dir_findex):\n",
    "            for ticker in tickers_to_run[findex]:\n",
    "                reports_2_words_processing(dir_findex, ticker, findex, filtering_years_set)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lematization and english words filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that all nltk data sets are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/Alan_Spark/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Alan_Spark/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/Alan_Spark/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create stop words set and white words set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create stop words set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "english_words = set(nltk.corpus.words.words())\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "stop_words_set = set(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extend stop words with custom stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if flag_extend_stopwords:\n",
    "    with open(os.path.join(dir_data_raw, \"english\", \"extra_stopwords.txt\"), \"r\") as f_r:\n",
    "        for text_line in f_r:\n",
    "            term = text_line.strip()\n",
    "            stop_words_set.add(term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a white list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "white_list_set = set()\n",
    "with open(os.path.join(dir_data_raw, \"english\", \"white_stopwords.txt\"), \"r\") as f_r:\n",
    "    for text_line in f_r:\n",
    "        term = text_line.strip()\n",
    "        white_list_set.add(term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a funtion checker for stop words. A word is stop word if any of the folowing true:\n",
    "- it's length shorter then 3 char\n",
    "- it contains digits\n",
    "- it appears in nltk stop words set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_stop_term(term):\n",
    "    if term in white_list_set:\n",
    "        return False\n",
    "    if len(term) < 3:\n",
    "        return True\n",
    "    return term in stop_words_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a funtion lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that en spacy data set is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')#, disable=['parser', 'ner'])\n",
    "#lemmatizer = WordNetLemmatizer().lemmatize\n",
    "#stemmer = SnowballStemmer(\"english\").stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bank bank\n",
      "banking banking\n",
      "go go\n",
      "go going\n"
     ]
    }
   ],
   "source": [
    "tokens = nlp('bank banking go going')\n",
    "for token in tokens:\n",
    "    print(token.lemma_ + ' ' + token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text_line, terms, eliminated_terms):\n",
    "    doc = nlp(text_line)\n",
    "    for token in doc:\n",
    "        term = token.lemma_ if token.lemma_ != \"-PRON-\" else token.text\n",
    "        if is_stop_term(term) or term not in english_words:\n",
    "            if term not in eliminated_terms:\n",
    "                eliminated_terms[term] = 0\n",
    "            eliminated_terms[term] += 1  \n",
    "        else:\n",
    "            terms.append(term)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_words_2_terms(file_report_path, eliminated_terms):\n",
    "    terms = []\n",
    "    chunk_size = 30\n",
    "    with open(os.path.join(file_report_path), \"r\",  encoding=\"utf-8\") as f_r:\n",
    "        for text_line in f_r:\n",
    "            words_in_line = re.split(\"\\W+\", text_line)\n",
    "            size = len(words_in_line)\n",
    "            steps = int(size / chunk_size)\n",
    "            for i in range(steps):\n",
    "                tokenize(\" \".join(words_in_line[i*chunk_size:(i+1)*chunk_size]), terms, eliminated_terms)\n",
    "            tokenize(\" \".join(words_in_line[steps*chunk_size:]), terms, eliminated_terms)    \n",
    "    \n",
    "    print(\"Done with %s\" % file_report_path)\n",
    "    return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_2_terms_processing(dir_findex, ticker):\n",
    "    start = time.time()\n",
    "    print(\"\\n Start on %s %s\" % (dir_findex, ticker))\n",
    "    \n",
    "    dict_eliminated_terms = {}\n",
    "    dir_ticker = os.path.join(dir_findex, ticker)\n",
    "    good_documents_amount = 0\n",
    "    empty_documents_amount = 0\n",
    "    if os.path.isdir(dir_ticker):\n",
    "        for report_file_name in os.listdir(dir_ticker):\n",
    "            if report_file_name == \".DS_Store\":\n",
    "                continue\n",
    "            terms_list = convert_words_2_terms(os.path.join(dir_ticker, report_file_name), dict_eliminated_terms)\n",
    "            if len(terms_list):   \n",
    "                good_documents_amount += 1\n",
    "                new_path = os.path.join(dir_reports_terms, findex, ticker)\n",
    "                if not os.path.exists(new_path):\n",
    "                    os.makedirs(new_path)\n",
    "                with open(os.path.join(new_path, report_file_name), \"w\") as f_w:\n",
    "                    f_w.write(\"%s\" % ' '.join(terms_list))\n",
    "            else: \n",
    "                empty_documents_amount += 1\n",
    "                if flag_debug:\n",
    "                    print(\"report %s is empty after cleaning\" % report_file_name)\n",
    "        \n",
    "        if len(dict_eliminated_terms):\n",
    "            if not os.path.exists(dir_terms_eliminated):\n",
    "                os.makedirs(dir_terms_eliminated)\n",
    "            with open(os.path.join(dir_terms_eliminated, \"%s_%s.json\" % (ticker, findex)), \"w\") as f_w:\n",
    "                json.dump(dict_eliminated_terms, f_w)\n",
    "        \n",
    "        end = time.time()\n",
    "        print(\"\\n Done in %s minutes on %s, good reports: %s, empty reports: %s\" % \n",
    "              (\"{:0.2f}\".format((end - start) / 60), dir_ticker, good_documents_amount, empty_documents_amount))\n",
    "    else:\n",
    "        print(\"\\n Skip non folder %s %s\" % (dir_findex, ticker))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run previously defined function words_2_terms_processing in pool of 4 processes to speedup the cleaning, The following cell takes quite a while, be carefull and do not rerun it without a reason, results are stored at file system\n",
    "\n",
    "**please make sure that flag_rerun_words_2_terms is set to True if you want to run/re-run this preprosessing step**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Start on ../data_processing/reports_words/FTSE WPP\n",
      "Done with ../data_processing/reports_words/FTSE/WPP/2009_51.txt\n",
      "Done with ../data_processing/reports_words/FTSE/WPP/2013_34.txt\n",
      "Done with ../data_processing/reports_words/FTSE/WPP/2010_65.txt\n",
      "Done with ../data_processing/reports_words/FTSE/WPP/2012_48.txt\n",
      "Done with ../data_processing/reports_words/FTSE/WPP/2016_10.txt\n",
      "Done with ../data_processing/reports_words/FTSE/WPP/2005_50.txt\n",
      "Done with ../data_processing/reports_words/FTSE/WPP/2017_5.txt\n",
      "Done with ../data_processing/reports_words/FTSE/WPP/2007_57.txt\n",
      "Done with ../data_processing/reports_words/FTSE/WPP/2013_23.txt\n",
      "Done with ../data_processing/reports_words/FTSE/WPP/2011_22.txt\n",
      "Done with ../data_processing/reports_words/FTSE/WPP/2006_12.txt\n",
      "Done with ../data_processing/reports_words/FTSE/WPP/2007_46.txt\n",
      "Done with ../data_processing/reports_words/FTSE/WPP/2013_4.txt\n",
      "Done with ../data_processing/reports_words/FTSE/WPP/2010_63.txt\n",
      "Done with ../data_processing/reports_words/FTSE/WPP/2015_3.txt\n",
      "Done with ../data_processing/reports_words/FTSE/WPP/2006_39.txt\n",
      "Done with ../data_processing/reports_words/FTSE/WPP/2009_54.txt\n",
      "Done with ../data_processing/reports_words/FTSE/WPP/2019_44.txt\n",
      "Done with ../data_processing/reports_words/FTSE/WPP/2017_2.txt\n",
      "Done with ../data_processing/reports_words/FTSE/WPP/2018_38.txt\n",
      "Done with ../data_processing/reports_words/FTSE/WPP/2017_18.txt\n",
      "Done with ../data_processing/reports_words/FTSE/WPP/2015_19.txt\n",
      "Done with ../data_processing/reports_words/FTSE/WPP/2020_61.txt\n",
      "Done with ../data_processing/reports_words/FTSE/WPP/2005_21.txt\n",
      "Done with ../data_processing/reports_words/FTSE/WPP/2016_1.txt\n",
      "Done with ../data_processing/reports_words/FTSE/WPP/2016_62.txt\n",
      "Done with ../data_processing/reports_words/FTSE/WPP/2012_8.txt\n",
      "Done with ../data_processing/reports_words/FTSE/WPP/2014_42.txt\n",
      "Done with ../data_processing/reports_words/FTSE/WPP/2005_7.txt\n",
      "Done with ../data_processing/reports_words/FTSE/WPP/2006_43.txt\n",
      "Done with ../data_processing/reports_words/FTSE/WPP/2011_59.txt\n",
      "Done with ../data_processing/reports_words/FTSE/WPP/2015_15.txt\n",
      "Done with ../data_processing/reports_words/FTSE/WPP/2019_29.txt\n",
      "Done with ../data_processing/reports_words/FTSE/WPP/2009_11.txt\n",
      "Done with ../data_processing/reports_words/FTSE/WPP/2010_24.txt\n",
      "Done with ../data_processing/reports_words/FTSE/WPP/2015_28.txt\n",
      "Done with ../data_processing/reports_words/FTSE/WPP/2015_14.txt\n",
      "Done with ../data_processing/reports_words/FTSE/WPP/2016_45.txt\n",
      "Done with ../data_processing/reports_words/FTSE/WPP/2018_40.txt\n",
      "Done with ../data_processing/reports_words/FTSE/WPP/2016_41.txt\n",
      "Done with ../data_processing/reports_words/FTSE/WPP/2012_31.txt\n",
      "Done with ../data_processing/reports_words/FTSE/WPP/2020_55.txt\n",
      "Done with ../data_processing/reports_words/FTSE/WPP/2016_56.txt\n",
      "Done with ../data_processing/reports_words/FTSE/WPP/2018_53.txt\n",
      "Done with ../data_processing/reports_words/FTSE/WPP/2018_47.txt\n",
      "Done with ../data_processing/reports_words/FTSE/WPP/2018_6.txt\n",
      "Done with ../data_processing/reports_words/FTSE/WPP/2017_16.txt\n",
      "Done with ../data_processing/reports_words/FTSE/WPP/2007_13.txt\n",
      "Done with ../data_processing/reports_words/FTSE/WPP/2014_52.txt\n",
      "Done with ../data_processing/reports_words/FTSE/WPP/2018_35.txt\n",
      "Done with ../data_processing/reports_words/FTSE/WPP/2017_64.txt\n",
      "Done with ../data_processing/reports_words/FTSE/WPP/2019_49.txt\n",
      "Done with ../data_processing/reports_words/FTSE/WPP/2017_58.txt\n",
      "Done with ../data_processing/reports_words/FTSE/WPP/2014_20.txt\n",
      "Done with ../data_processing/reports_words/FTSE/WPP/2019_60.txt\n",
      "Done with ../data_processing/reports_words/FTSE/WPP/2020_30.txt\n",
      "Done with ../data_processing/reports_words/FTSE/WPP/2008_25.txt\n",
      "Done with ../data_processing/reports_words/FTSE/WPP/2008_9.txt\n",
      "Done with ../data_processing/reports_words/FTSE/WPP/2016_27.txt\n",
      "Done with ../data_processing/reports_words/FTSE/WPP/2008_33.txt\n",
      "Done with ../data_processing/reports_words/FTSE/WPP/2020_26.txt\n",
      "Done with ../data_processing/reports_words/FTSE/WPP/2018_37.txt\n",
      "Done with ../data_processing/reports_words/FTSE/WPP/2011_17.txt\n",
      "Done with ../data_processing/reports_words/FTSE/WPP/2019_67.txt\n",
      "Done with ../data_processing/reports_words/FTSE/WPP/2020_36.txt\n",
      "Done with ../data_processing/reports_words/FTSE/WPP/2015_66.txt\n",
      "Done with ../data_processing/reports_words/FTSE/WPP/2018_32.txt\n",
      "\n",
      " Done in 52.83 minutes on ../data_processing/reports_words/FTSE/WPP, good reports: 67, empty reports: 0\n",
      "\n",
      " Start on ../data_processing/reports_words/DJIA TVE\n",
      "Done with ../data_processing/reports_words/DJIA/TVE/2018_28.txt\n",
      "Done with ../data_processing/reports_words/DJIA/TVE/2016_11.txt\n",
      "Done with ../data_processing/reports_words/DJIA/TVE/2019_9.txt\n",
      "Done with ../data_processing/reports_words/DJIA/TVE/2017_6.txt\n",
      "Done with ../data_processing/reports_words/DJIA/TVE/2013_20.txt\n",
      "Done with ../data_processing/reports_words/DJIA/TVE/2017_7.txt\n",
      "Done with ../data_processing/reports_words/DJIA/TVE/2020_16.txt\n",
      "Done with ../data_processing/reports_words/DJIA/TVE/2020_29.txt\n",
      "Done with ../data_processing/reports_words/DJIA/TVE/2016_15.txt\n",
      "Done with ../data_processing/reports_words/DJIA/TVE/2017_26.txt\n",
      "Done with ../data_processing/reports_words/DJIA/TVE/2015_34.txt\n",
      "Done with ../data_processing/reports_words/DJIA/TVE/2014_4.txt\n",
      "Done with ../data_processing/reports_words/DJIA/TVE/2018_8.txt\n",
      "Done with ../data_processing/reports_words/DJIA/TVE/2010_1.txt\n",
      "Done with ../data_processing/reports_words/DJIA/TVE/2015_25.txt\n",
      "Done with ../data_processing/reports_words/DJIA/TVE/2019_31.txt\n",
      "Done with ../data_processing/reports_words/DJIA/TVE/2019_19.txt\n",
      "Done with ../data_processing/reports_words/DJIA/TVE/2017_21.txt\n",
      "Done with ../data_processing/reports_words/DJIA/TVE/2015_24.txt\n",
      "Done with ../data_processing/reports_words/DJIA/TVE/2007_32.txt\n",
      "Done with ../data_processing/reports_words/DJIA/TVE/2019_33.txt\n",
      "Done with ../data_processing/reports_words/DJIA/TVE/2012_5.txt\n",
      "Done with ../data_processing/reports_words/DJIA/TVE/2005_13.txt\n",
      "Done with ../data_processing/reports_words/DJIA/TVE/2018_2.txt\n",
      "Done with ../data_processing/reports_words/DJIA/TVE/2005_12.txt\n",
      "Done with ../data_processing/reports_words/DJIA/TVE/2009_10.txt\n",
      "Done with ../data_processing/reports_words/DJIA/TVE/2015_14.txt\n",
      "Done with ../data_processing/reports_words/DJIA/TVE/2016_30.txt\n",
      "Done with ../data_processing/reports_words/DJIA/TVE/2008_18.txt\n",
      "Done with ../data_processing/reports_words/DJIA/TVE/2006_22.txt\n",
      "Done with ../data_processing/reports_words/DJIA/TVE/2020_27.txt\n",
      "Done with ../data_processing/reports_words/DJIA/TVE/2011_17.txt\n",
      "Done with ../data_processing/reports_words/DJIA/TVE/2020_23.txt\n",
      "Done with ../data_processing/reports_words/DJIA/TVE/2020_3.txt\n",
      "Done with ../data_processing/reports_words/DJIA/TVE/2016_35.txt\n",
      "\n",
      " Done in 71.60 minutes on ../data_processing/reports_words/DJIA/TVE, good reports: 35, empty reports: 0\n"
     ]
    }
   ],
   "source": [
    "if flag_rerun_words_2_terms:\n",
    "    for findex in tickers_to_run.keys():\n",
    "        dir_findex = os.path.join(dir_reports_words, findex)\n",
    "        if os.path.isdir(dir_findex):\n",
    "            for ticker in tickers_to_run[findex]:\n",
    "                words_2_terms_processing(dir_findex, ticker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Condence bigramms and trigarams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read all documents as data: list of list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**!Important** rerun Gramms in the main notebook before preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EnD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
