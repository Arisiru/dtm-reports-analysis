{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set experiment dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2005 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_prefix = 'run_%s_xx' % run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_root = os.path.join('..')\n",
    "dir_data_runs = os.path.join(dir_root, 'data_runs')\n",
    "dir_run = os.path.join(dir_data_runs, run_prefix)\n",
    "dir_reports =  os.path.join(dir_run, 'reports')\n",
    "dir_plots = os.path.join(dir_run, 'interpretation', 't_search_plots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(dir_plots):\n",
    "    os.makedirs(dir_plots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_debug = False\n",
    "flag_serialize_findings = False\n",
    "flag_print_tables = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data reports from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_report(dir_name, file_name):\n",
    "    file_path = os.path.join(dir_name, file_name)\n",
    "    result = ''\n",
    "    with open(file_path, 'r') as f_r:\n",
    "        for text_line in f_r:\n",
    "            result = result + text_line.strip()\n",
    "    return result        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done on 2005, number of documents: 234\n"
     ]
    }
   ],
   "source": [
    "reports_list = []\n",
    "\n",
    "regExp = re.compile('[A-Z\\d]+\\_[A-Z\\d]+\\-' + str(year) + '\\_[\\d]+\\.txt$')\n",
    "reports_of_year = [f for f in os.listdir(dir_reports) if re.search(regExp, f)]\n",
    "reports_of_year.sort()\n",
    "# for every reports of the year\n",
    "for report_name in reports_of_year:\n",
    "    reports_list.append(read_report(dir_reports, report_name))\n",
    "\n",
    "print('Done on %s, number of documents: %s' % (year, len(reports_of_year)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mining headline platinum diamo'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reports_list[0][:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle reporst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best randon seed is 99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.Random(seed).shuffle(reports_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split between training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_size = len(reports_list) // 10\n",
    "training_size = len(reports_list) - testing_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 234 reprorts\n",
      "Training 211 reprorts\n",
      "Testing 23 reprorts\n"
     ]
    }
   ],
   "source": [
    "print('Total %s reprorts' % len(reports_list))\n",
    "print('Training %s reprorts' % training_size)\n",
    "print('Testing %s reprorts' % testing_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create vectorize reports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer='word',       \n",
    "                             min_df=1,                        # minimum reqd occurences of a word \n",
    "                             stop_words='english',             # remove stop words\n",
    "                             lowercase=True,                   # convert all words to lowercase\n",
    "                             token_pattern='[a-zA-Z0-9]{3,}',  # num chars > 3\n",
    "                             # max_features=50000,             # max number of uniq words\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports_vectorized_training = vectorizer.fit_transform(reports_list[:training_size])\n",
    "reports_vectorized_testing = vectorizer.fit_transform(reports_list[:-testing_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find optimal topic number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_T_sklearn(training_corpus, \n",
    "                   test_corpus, \n",
    "                   limit, \n",
    "                   start=2, \n",
    "                   step=3, \n",
    "                   passes=10, \n",
    "                   random_state=100):\n",
    "    \n",
    "    train_p_values = []\n",
    "    test_p_values = []\n",
    "    model_list = []\n",
    "\n",
    "    i = 0\n",
    "    \n",
    "    for num_topics in range(start, limit + 1, step):\n",
    "\n",
    "        lda_model = LatentDirichletAllocation(batch_size=128,\n",
    "              doc_topic_prior=None,\n",
    "              evaluate_every=-1, \n",
    "              learning_decay=0.7,\n",
    "              learning_method='online',\n",
    "              learning_offset=10.0,\n",
    "              max_doc_update_iter=100,\n",
    "              max_iter=passes, \n",
    "              mean_change_tol=0.001,\n",
    "              n_components=num_topics,\n",
    "              n_jobs=-1,\n",
    "              perp_tol=0.1,\n",
    "              random_state=random_state,\n",
    "              topic_word_prior=None,\n",
    "              total_samples=1000000.0,\n",
    "              verbose=0)\n",
    "        \n",
    "        model = lda_model.fit_transform(training_corpus)\n",
    "\n",
    "        model_list.append(model)\n",
    "                \n",
    "        train_perplexity = lda_model.perplexity(training_corpus)\n",
    "        train_p_values.append(train_perplexity)\n",
    "        \n",
    "        test_perplexity = lda_model.perplexity(test_corpus)\n",
    "        test_p_values.append(test_perplexity)\n",
    "        \n",
    "        print('Done on indx: %s T: %s, Test perplexity: %s' % (i, num_topics, test_perplexity))\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "    \n",
    "    return model_list, train_p_values, test_p_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_T = 1\n",
    "max_T = 100\n",
    "step_T = 1\n",
    "passes_T = 51\n",
    "random_state = 193748"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best run params:\n",
    "\n",
    "- min_T = 1\n",
    "- max_T = 100\n",
    "- step_T = 1\n",
    "- passes_T = 51\n",
    "- random_state = 193748"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = None\n",
    "train_perplexity_values = None\n",
    "test_perplexity_values = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run with heldout test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done on indx: 0 T: 1, Test perplexity: 3056.28085984862\n",
      "Done on indx: 1 T: 2, Test perplexity: 2469.5003018620955\n",
      "Done on indx: 2 T: 3, Test perplexity: 2180.813524410251\n",
      "Done on indx: 3 T: 4, Test perplexity: 1995.437053477579\n",
      "Done on indx: 4 T: 5, Test perplexity: 1846.5647635321268\n",
      "Done on indx: 5 T: 6, Test perplexity: 1679.4916970156414\n",
      "Done on indx: 6 T: 7, Test perplexity: 1566.083050111676\n",
      "Done on indx: 7 T: 8, Test perplexity: 1496.421698683583\n",
      "Done on indx: 8 T: 9, Test perplexity: 1430.635875790689\n",
      "Done on indx: 9 T: 10, Test perplexity: 1347.7329497769658\n",
      "Done on indx: 10 T: 11, Test perplexity: 1314.1259852339397\n",
      "Done on indx: 11 T: 12, Test perplexity: 1289.7431908536016\n",
      "Done on indx: 12 T: 13, Test perplexity: 1309.7411195588218\n",
      "Done on indx: 13 T: 14, Test perplexity: 1220.6905613614665\n",
      "Done on indx: 14 T: 15, Test perplexity: 1239.0359361599806\n",
      "Done on indx: 15 T: 16, Test perplexity: 1207.0335155374983\n",
      "Done on indx: 16 T: 17, Test perplexity: 1262.1486047463131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Alan_Spark/opt/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done on indx: 17 T: 18, Test perplexity: 1147.9825915943968\n",
      "Done on indx: 18 T: 19, Test perplexity: 1140.2421563423566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Alan_Spark/opt/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done on indx: 19 T: 20, Test perplexity: 1176.6691187449162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Alan_Spark/opt/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done on indx: 20 T: 21, Test perplexity: 1180.2194610156791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Alan_Spark/opt/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done on indx: 21 T: 22, Test perplexity: 1152.0914542522469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Alan_Spark/opt/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "if True:\n",
    "    model_list, train_perplexity_values, test_perplexity_values = scan_T_sklearn(\n",
    "        training_corpus=reports_vectorized_training, \n",
    "        test_corpus=reports_vectorized_testing,\n",
    "        start=min_T, \n",
    "        limit=max_T, \n",
    "        step=step_T,\n",
    "        passes=passes_T,\n",
    "        random_state=random_state)\n",
    "    \n",
    "end = time.time()\n",
    "\n",
    "print('Execution time: %s minutes' % (end - start)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "for i in range(len(coherence_values)):\n",
    "    if math.isnan(coherence_values[i]):\n",
    "        coherence_values[i] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### serialize findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scat_T_results = {\n",
    "    'train_perplexity_values': train_perplexity_values,\n",
    "    'test_perplexity_values': test_perplexity_values,\n",
    "}\n",
    "\n",
    "if flag_serialize_findings:\n",
    "    with open(os.path.join(dir_run, 'scan_T_values_sklearn.json'), 'w') as f_w:\n",
    "        f_w.write(json.dumps(scat_T_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_T_on_perplexity = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_range = np.array(range(min_T, max_T + 1, step_T))\n",
    "best_T_on_perplexity = topics_range[np.argmin(train_perplexity_values)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_coherence = '#1f94f9'#hsl(208, 95%, 55%) # #1f94f9\n",
    "color_perplexity = '#f9591f' #hsl(16, 95%, 55%) # #f9591f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_topics_metric(ax, data, metric = '', label = '', color = 'red', best_is_max = True, with_interest_area = False, interest_area_center = 0, interest_area_size = 5):\n",
    "    x = np.array(range(min_T, max_T + 1, step_T))\n",
    "    y = np.array(data)\n",
    "\n",
    "    ax.set_xlabel('Number of Topics, T')\n",
    "    ax.set_ylabel(metric, color=color)\n",
    "    ax.plot(x, y, color=color, label = label)   \n",
    "    \n",
    "    x_best = None\n",
    "    y_best = None\n",
    "    if with_interest_area:\n",
    "        interest_area_min = interest_area_center - interest_area_size;\n",
    "        interest_area_max = interest_area_center + interest_area_size;\n",
    "        ax.axvspan(interest_area_min, interest_area_max, color='green', alpha=0.2) \n",
    "\n",
    "        x_best = interest_area_min + np.argmax(y[interest_area_min - min_T : (interest_area_max + 1) - min_T])\n",
    "        y_best = y[interest_area_min - min_T : (interest_area_max + 1) - min_T].max()\n",
    "    else: \n",
    "        if best_is_max:\n",
    "            x_best = x[np.argmax(y)]\n",
    "            y_best = y.max()\n",
    "        else:\n",
    "            x_best = x[np.argmin(y)]\n",
    "            y_best = y.min()            \n",
    "\n",
    "    text = 'Num of Topics=%s, score=%f' % (x_best, y_best)\n",
    "\n",
    "    bbox_props = dict(boxstyle='square,pad=0.3', fc='w', ec='k', lw=0.72)\n",
    "    arrowprops=dict(arrowstyle='->')\n",
    "    kw = dict(xycoords='data',textcoords='axes fraction',\n",
    "          arrowprops=arrowprops, bbox=bbox_props, ha='right', va='top')\n",
    "    \n",
    "    ax.annotate(text, xy=(x_best, y_best), xytext=(0.99,1.1), **kw)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Perplexity test and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(10,5))\n",
    "\n",
    "plot_topics_metric(\n",
    "    axs[0], \n",
    "    train_perplexity_values,\n",
    "    metric = 'Perplexity',\n",
    "    label = 'Training Perplexity',\n",
    "    color = color_perplexity,\n",
    "    best_is_max = False\n",
    ")\n",
    "plot_topics_metric(\n",
    "    axs[1], \n",
    "    test_perplexity_values,\n",
    "    metric = 'Perplexity',\n",
    "    label = 'Test Perplexity',\n",
    "    color = color_perplexity,\n",
    "    best_is_max = False\n",
    ")\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
