{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import operator\n",
    "import math\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set experiment dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_start = dt.datetime.strptime('2005-01-01', '%Y-%m-%d')\n",
    "date_end = dt.datetime.strptime('2018-06-30', '%Y-%m-%d') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set run prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_prefix = 'run_01_01'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set root directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_root = os.path.join('..')\n",
    "dir_data_raw = os.path.join(dir_root, 'data_raw')\n",
    "dir_data_processing = os.path.join(dir_root, 'data_processing')\n",
    "dir_prices = os.path.join(dir_data_processing, 'prices')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_debug = False\n",
    "flag_rerun_cleaning = True\n",
    "flag_test_report_names = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create a run directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_run = os.path.join(dir_data_processing, 'runs', run_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(dir_run):\n",
    "    os.makedirs(dir_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that all nltk data sets are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/alan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/alan/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/alan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw to /Users/alan/nltk_data...\n",
      "[nltk_data]   Package omw is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_words_set = set(nltk.corpus.words.words())\n",
    "lemmatizer = WordNetLemmatizer().lemmatize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create stop words set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_set = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extend stop words with custom stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(dir_data_raw, 'english', 'extra_stopwords.txt'), 'r') as f_r:\n",
    "    for text_line in f_r:\n",
    "        stop_words_set.add(text_line.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a funtion checker for stop words. A word is stop word if any of the folowing true:\n",
    "- it's length shorter then 4 char\n",
    "- it contains digits\n",
    "- it appears in nltk stop words set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_stop_term(term):\n",
    "    if len(term) < 3:\n",
    "        return True\n",
    "    if re.search('[\\d]+', term):\n",
    "        return True\n",
    "    return term in stop_words_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a funtion lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_lemma(possible_word):\n",
    "    possible_lemma = lemmatizer(possible_word)\n",
    "    possible_lemma = lemmatizer(possible_lemma, 'v')\n",
    "    if not is_stop_term(possible_lemma) and possible_lemma in english_words_set:\n",
    "        return possible_lemma\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a clean report function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_report(file_report_path):\n",
    "    document = dict()\n",
    "    with open(os.path.join(file_report_path), 'r',  encoding='ISO-8859-1') as f_r:\n",
    "        for text_line in f_r:\n",
    "            words = re.split('\\W+', text_line)\n",
    "            for word in words:\n",
    "                word_lower = word.lower()\n",
    "                term = get_word_lemma(word_lower)\n",
    "                if term is not None:\n",
    "                    if term in document:\n",
    "                        document[term] += 1\n",
    "                    else:\n",
    "                        document[term] = 1\n",
    "                elif flag_debug:\n",
    "                    print(\"%s is excluded from analisys\" % word_lower)\n",
    "    return document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate over all reports, clean and convert them to term frequency map, store as csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_report_name_reg_exp = '(?P<ticker>[A-Z1-9]+)[_-](?P<type>[A-Z]+)(?P<number>[1-9]*)(?P<subnumber>-[1-9]+)?[-_](?P<year>[0-9]{4})[_-](?P<p_year>[0-9]{4})'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_reports_raw = os.path.join(dir_data_raw, 'reports_txt')\n",
    "dir_reports =  os.path.join(dir_data_processing, 'reports')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell takes quite a while, be carefull and do not rerun it without a reason, results are stored at file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ticker_reports_processing(dir_findex, ticker):\n",
    "    dir_ticker = os.path.join(dir_findex, ticker)\n",
    "    if os.path.isdir(dir_ticker):\n",
    "        ticker_documents_amount = 0\n",
    "        ticker_code = \"%s_%s\" % (ticker, findex)\n",
    "        print(\"working on %s at %s\" % (ticker_code, dt.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "\n",
    "        for report_file_name in os.listdir(dir_ticker):\n",
    "            match = re.search(check_report_name_reg_exp, report_file_name)\n",
    "            if not match:\n",
    "                print(\"filename %s doesn't fit pattern\" % report_file_name)\n",
    "            else:\n",
    "                dict_report = clean_report(os.path.join(dir_ticker, report_file_name))\n",
    "                if len(dict_report):\n",
    "                    ticker_documents_amount += 1\n",
    "                    new_file_name = \"%s-%s.csv\" % (match.group('p_year'), ticker_documents_amount)\n",
    "                    new_path = os.path.join(dir_reports, ticker_code)\n",
    "                    if not os.path.exists(new_path):\n",
    "                        os.makedirs(new_path)\n",
    "                    with open(os.path.join(new_path, new_file_name), 'w') as f_w:\n",
    "                        for term, tf in dict_report.items():\n",
    "                            f_w.write(\"%s,%s\\n\" % (term, tf))\n",
    "                elif flag_debug:\n",
    "                    print(\"report %s is empty after cleaning\" % report_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_reports_names(dir_findex, ticker):\n",
    "    dir_ticker = os.path.join(dir_findex, ticker)\n",
    "    if os.path.isdir(dir_ticker):\n",
    "        ticker_documents_amount = 0\n",
    "        ticker_code = \"%s_%s\" % (ticker, findex)\n",
    "        for report_file_name in os.listdir(dir_ticker):\n",
    "            match = re.search(check_report_name_reg_exp, report_file_name)\n",
    "            if not match:\n",
    "                print(\"working on %s, filename %s doesn't fit pattern\" % (ticker_code, report_file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on TUI1_XETRA at 2019-01-10 07:33:12\n",
      "working on CA_CAC at 2019-01-10 07:33:35\n",
      "working on AC_CAC at 2019-01-10 07:33:35\n",
      "working on EDF_CAC at 2019-01-10 07:33:35\n",
      "working on ACA_CAC at 2019-01-10 07:33:35\n",
      "working on CS_CAC at 2019-01-10 07:34:11\n",
      "working on ORA_CAC at 2019-01-10 07:34:46\n",
      "working on AIR_CAC at 2019-01-10 07:34:59\n",
      "working on ML_CAC at 2019-01-10 07:35:14\n",
      "working on AI_CAC at 2019-01-10 07:35:25\n",
      "working on MC_CAC at 2019-01-10 07:35:28\n",
      "working on CAP_CAC at 2019-01-10 07:36:23\n",
      "working on BN_CAC at 2019-01-10 07:37:11\n",
      "working on RI_CAC at 2019-01-10 07:37:21\n",
      "working on BNP_CAC at 2019-01-10 07:37:48\n",
      "working on EN_CAC at 2019-01-10 07:37:55\n",
      "working on EI_CAC at 2019-01-10 07:37:59\n",
      "working on OR_CAC at 2019-01-10 07:38:50\n",
      "working on ENGI_CAC at 2019-01-10 07:39:02\n",
      "working on LR_CAC at 2019-01-10 07:39:09\n",
      "working on ALO_CAC at 2019-01-10 07:39:39\n",
      "working on KER_CAC at 2019-01-10 07:40:20\n",
      "working on UU_FTSE at 2019-01-10 07:41:12\n",
      "working on WEIR_FTSE at 2019-01-10 07:41:12\n",
      "working on WPP_FTSE at 2019-01-10 07:41:12\n",
      "working on BP_FTSE at 2019-01-10 07:41:12\n",
      "working on LAND_FTSE at 2019-01-10 07:41:48\n",
      "working on TLW_FTSE at 2019-01-10 07:41:55\n",
      "working on KGF_FTSE at 2019-01-10 07:42:27\n",
      "working on IHG_FTSE at 2019-01-10 07:42:32\n",
      "working on PFC_FTSE at 2019-01-10 07:42:48\n",
      "working on SKY_FTSE at 2019-01-10 07:43:03\n",
      "working on BARC_FTSE at 2019-01-10 07:43:05\n",
      "working on CRDA_FTSE at 2019-01-10 07:43:27\n",
      "working on BLND_FTSE at 2019-01-10 07:43:27\n",
      "working on IMB_FTSE at 2019-01-10 07:43:45\n",
      "working on AV_FTSE at 2019-01-10 07:43:52\n",
      "working on ANTO_FTSE at 2019-01-10 07:43:52\n",
      "working on ITRK_FTSE at 2019-01-10 07:44:01\n",
      "working on SSE_FTSE at 2019-01-10 07:44:34\n",
      "working on AAL_FTSE at 2019-01-10 07:44:40\n",
      "working on HL_FTSE at 2019-01-10 07:45:06\n",
      "working on NG_FTSE at 2019-01-10 07:45:23\n",
      "working on CUK_FTSE at 2019-01-10 07:45:34\n",
      "working on IAG_FTSE at 2019-01-10 07:46:01\n",
      "working on SNN_FTSE at 2019-01-10 07:46:06\n",
      "working on MGGT_FTSE at 2019-01-10 07:46:23\n",
      "filename .DS_Store doesn't fit pattern\n",
      "working on BAB_FTSE at 2019-01-10 07:46:28\n",
      "working on BNZL_FTSE at 2019-01-10 07:46:56\n",
      "working on SHP_FTSE at 2019-01-10 07:47:07\n",
      "working on CPI_FTSE at 2019-01-10 07:47:09\n",
      "working on DGE_FTSE at 2019-01-10 07:47:21\n",
      "working on SDR_FTSE at 2019-01-10 07:47:34\n",
      "working on MKS_FTSE at 2019-01-10 07:47:39\n",
      "filename .DS_Store doesn't fit pattern\n",
      "working on EVR_FTSE at 2019-01-10 07:48:10\n",
      "working on BRBY_FTSE at 2019-01-10 07:48:17\n",
      "working on SVT_FTSE at 2019-01-10 07:48:23\n",
      "working on JMAT_FTSE at 2019-01-10 07:48:27\n",
      "working on WTB_FTSE at 2019-01-10 07:48:43\n",
      "working on RSA_FTSE at 2019-01-10 07:48:52\n",
      "filename .DS_Store doesn't fit pattern\n",
      "working on PRU_FTSE at 2019-01-10 07:48:57\n",
      "working on SMIN_FTSE at 2019-01-10 07:49:08\n",
      "working on STAN_FTSE at 2019-01-10 07:49:35\n",
      "working on EXPN_FTSE at 2019-01-10 07:49:38\n",
      "working on CPG_FTSE at 2019-01-10 07:50:16\n",
      "working on RR_FTSE at 2019-01-10 07:50:25\n",
      "working on RIO_FTSE at 2019-01-10 07:50:46\n",
      "working on VOD_FTSE at 2019-01-10 07:50:47\n",
      "working on RBS_FTSE at 2019-01-10 07:50:48\n",
      "working on GLEN_FTSE at 2019-01-10 07:50:52\n",
      "working on MRW_FTSE at 2019-01-10 07:51:12\n",
      "filename .DS_Store doesn't fit pattern\n",
      "working on CNA_FTSE at 2019-01-10 07:51:42\n",
      "working on ABF_FTSE at 2019-01-10 07:51:42\n",
      "working on GSK_FTSE at 2019-01-10 07:51:59\n",
      "working on FERG_FTSE at 2019-01-10 07:52:01\n",
      "working on GFS_FTSE at 2019-01-10 07:52:08\n",
      "working on IMI_FTSE at 2019-01-10 07:52:30\n",
      "working on RDSB_FTSE at 2019-01-10 07:52:40\n",
      "working on ITV_FTSE at 2019-01-10 07:52:55\n",
      "working on REL_FTSE at 2019-01-10 07:53:01\n",
      "working on AZN_FTSE at 2019-01-10 07:53:11\n",
      "working on SGE_FTSE at 2019-01-10 07:53:29\n",
      "working on FRES_FTSE at 2019-01-10 07:53:30\n",
      "working on LLOY_FTSE at 2019-01-10 07:54:13\n",
      "working on TATE_FTSE at 2019-01-10 07:54:24\n",
      "filename .DS_Store doesn't fit pattern\n",
      "working on TSCO_FTSE at 2019-01-10 07:54:53\n",
      "working on BA_FTSE at 2019-01-10 07:55:05\n",
      "working on NXT_FTSE at 2019-01-10 07:55:15\n",
      "filename .DS_Store doesn't fit pattern\n",
      "working on AGK_FTSE at 2019-01-10 07:55:58\n",
      "working on MRO_FTSE at 2019-01-10 07:56:02\n",
      "filename .DS_Store doesn't fit pattern\n",
      "working on RRS_FTSE at 2019-01-10 07:56:07\n",
      "filename .DS_Store doesn't fit pattern\n",
      "working on VED_FTSE at 2019-01-10 07:56:28\n",
      "working on ULVR_FTSE at 2019-01-10 07:56:40\n",
      "working on CRH_FTSE at 2019-01-10 07:57:27\n",
      "working on BT_FTSE at 2019-01-10 07:57:35\n",
      "working on ADM_FTSE at 2019-01-10 07:57:39\n",
      "working on LGEN_FTSE at 2019-01-10 07:58:11\n",
      "working on SLA_FTSE at 2019-01-10 07:59:05\n",
      "working on SRP_FTSE at 2019-01-10 07:59:11\n",
      "working on HMSO_FTSE at 2019-01-10 07:59:12\n",
      "working on BLT_FTSE at 2019-01-10 07:59:33\n",
      "working on SBRY_FTSE at 2019-01-10 07:59:50\n",
      "working on CCH_FTSE at 2019-01-10 08:00:23\n",
      "working on WG_FTSE at 2019-01-10 08:01:07\n",
      "working on RB_FTSE at 2019-01-10 08:01:14\n",
      "working on VZ_DJIA at 2019-01-10 08:01:28\n",
      "working on HD_DJIA at 2019-01-10 08:01:28\n",
      "working on MMM_DJIA at 2019-01-10 08:01:28\n",
      "working on CVX_DJIA at 2019-01-10 08:01:28\n",
      "working on WMT_DJIA at 2019-01-10 08:01:46\n",
      "working on MRK_DJIA at 2019-01-10 08:02:05\n",
      "working on INTC_DJIA at 2019-01-10 08:02:16\n",
      "working on JNJ_DJIA at 2019-01-10 08:02:33\n",
      "working on GS_DJIA at 2019-01-10 08:02:33\n",
      "working on MSFT_DJIA at 2019-01-10 08:02:34\n",
      "working on CAT_DJIA at 2019-01-10 08:02:37\n",
      "working on PFE_DJIA at 2019-01-10 08:02:46\n",
      "working on IBM_DJIA at 2019-01-10 08:02:47\n",
      "working on MCD_DJIA at 2019-01-10 08:02:49\n",
      "working on T_DJIA at 2019-01-10 08:03:15\n",
      "working on BTI_DJIA at 2019-01-10 08:03:27\n",
      "working on PG_DJIA at 2019-01-10 08:04:08\n",
      "working on XOM_DJIA at 2019-01-10 08:04:20\n",
      "working on HSBC_DJIA at 2019-01-10 08:04:27\n",
      "working on GE_DJIA at 2019-01-10 08:04:33\n",
      "working on CSCO_DJIA at 2019-01-10 08:04:43\n",
      "working on TVE_DJIA at 2019-01-10 08:04:59\n",
      "working on UNH_DJIA at 2019-01-10 08:05:10\n",
      "working on DIS_DJIA at 2019-01-10 08:05:14\n",
      "working on KO_DJIA at 2019-01-10 08:05:25\n",
      "working on NKE_DJIA at 2019-01-10 08:06:21\n",
      "working on V_DJIA at 2019-01-10 08:06:22\n",
      "working on PSO_DJIA at 2019-01-10 08:06:32\n",
      "working on UTX_DJIA at 2019-01-10 08:06:57\n",
      "working on AXP_DJIA at 2019-01-10 08:07:01\n",
      "working on JPM_DJIA at 2019-01-10 08:07:43\n",
      "working on BA_DJIA at 2019-01-10 08:08:50\n",
      "working on HEN3_DAX at 2019-01-10 08:09:15\n",
      "working on VOW3_DAX at 2019-01-10 08:09:15\n",
      "working on SAP_DAX at 2019-01-10 08:09:15\n",
      "working on SDF_DAX at 2019-01-10 08:09:15\n",
      "working on LXS_DAX at 2019-01-10 08:09:56\n",
      "working on ADS_DAX at 2019-01-10 08:10:29\n",
      "working on LIN_DAX at 2019-01-10 08:10:33\n",
      "working on DB1_DAX at 2019-01-10 08:10:34\n",
      "working on MRK_DAX at 2019-01-10 08:11:07\n",
      "working on CON_DAX at 2019-01-10 08:11:08\n",
      "working on DAI_DAX at 2019-01-10 08:11:14\n",
      "working on ALV_DAX at 2019-01-10 08:11:52\n",
      "working on DTE_DAX at 2019-01-10 08:11:55\n",
      "working on BAYN_DAX at 2019-01-10 08:12:09\n",
      "filename .DS_Store doesn't fit pattern\n",
      "working on FRE_DAX at 2019-01-10 08:13:49\n",
      "working on RWE_DAX at 2019-01-10 08:14:07\n",
      "working on CBK_DAX at 2019-01-10 08:14:32\n",
      "working on BEI_DAX at 2019-01-10 08:14:35\n",
      "working on EOAN_DAX at 2019-01-10 08:14:52\n",
      "working on HEI_DAX at 2019-01-10 08:15:39\n",
      "working on MUV2_DAX at 2019-01-10 08:16:02\n",
      "working on DBK_DAX at 2019-01-10 08:16:16\n",
      "working on FME_DAX at 2019-01-10 08:16:23\n",
      "working on SIE_DAX at 2019-01-10 08:16:46\n",
      "working on LHA_DAX at 2019-01-10 08:17:17\n",
      "working on TKA_DAX at 2019-01-10 08:18:02\n",
      "working on IFX_DAX at 2019-01-10 08:18:39\n",
      "working on BMW_DAX at 2019-01-10 08:19:13\n",
      "working on DPW_DAX at 2019-01-10 08:19:43\n",
      "working on BAS_DAX at 2019-01-10 08:21:03\n"
     ]
    }
   ],
   "source": [
    "if flag_rerun_cleaning or flag_test_report_names:\n",
    "    for findex in os.listdir(dir_reports_raw):\n",
    "        dir_findex = os.path.join(dir_reports_raw, findex)\n",
    "        if os.path.isdir(dir_findex):\n",
    "            with multiprocessing.Pool(processes=4) as pool:\n",
    "                if flag_test_report_names: \n",
    "                    pool.starmap(test_reports_names, [(dir_findex, ticker) for ticker in os.listdir(dir_findex)])\n",
    "                else:\n",
    "                    pool.starmap(ticker_reports_processing, [(dir_findex, ticker) for ticker in os.listdir(dir_findex)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get tickers for analisys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to find all tickers of companies which have reports for the experiment's timeframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_tickers_years = os.path.join(dir_data_processing, 'tickers','tickers_years.datajson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = dict()\n",
    "for ticker in os.listdir(dir_reports):\n",
    "    dir_ticker = os.path.join(dir_reports, ticker)\n",
    "    if os.path.isdir(dir_ticker):\n",
    "        ticker_years_set = set()\n",
    "        for report in os.listdir(dir_ticker):\n",
    "            ticker_years_set.add(int(report[:4]))\n",
    "        tickers[ticker] = sorted(ticker_years_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save years of companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_tickers_years, 'w') as f_w:\n",
    "    for ticker in tickers:\n",
    "        f_w.write(\"%s\\n\" % json.dumps({\"ticker\": ticker, \"available_years\": tickers[ticker]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find tickers with reports for every year in the experiment timeframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_series = list(range(date_start.year, date_end.year)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if flag_debug:\n",
    "    print(year_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_data_tickers = os.path.join(dir_data_processing, 'tickers')\n",
    "file_tickers_for_analysis = os.path.join(dir_data_tickers,'ticker_for_analysis.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_fits_for_analysis = set()\n",
    "for ticker, available_years in tickers.items():\n",
    "    flag_complete_series = True\n",
    "    available_years_set = set(available_years)\n",
    "    for year in year_series:\n",
    "        if year not in available_years_set:\n",
    "            flag_complete_series = False\n",
    "            break\n",
    "    if flag_complete_series and os.path.exists(os.path.join(dir_prices, \"%s.csv\" % ticker)):\n",
    "        tickers_fits_for_analysis.add(ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'there are 78 tickers available for the experiment'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"there are %s tickers available for the experiment\" % len(tickers_fits_for_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save companies with complete years series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_tickers_for_analysis, 'w') as f_w:\n",
    "    for ticker in tickers_fits_for_analysis:\n",
    "        f_w.write(\"%s\\n\" % ticker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Return table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_ticker_prices = os.path.join(dir_data_processing, 'prices')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate over tickers available for analisys and build a return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_prices_table = {}\n",
    "for ticker in tickers_fits_for_analysis:\n",
    "    file_ticker_prices = os.path.join(dir_ticker_prices, ticker + '.csv')\n",
    "    if os.path.isfile(file_ticker_prices):\n",
    "        price_df = pd.read_csv(file_ticker_prices)\n",
    "        price_df['Date'] = pd.to_datetime(price_df['Date'])\n",
    "        price_df.sort_values(by=['Date'], inplace=True)\n",
    "        price_df.set_index('Date', inplace=True)\n",
    "        ticker_data = {}\n",
    "        \n",
    "        prev_day = None\n",
    "        date_stat_price = dt.datetime.strptime(\"%s-01-01\" % (date_start.year + 1), '%Y-%m-%d')\n",
    "        \n",
    "        for index, day in price_df[date_stat_price : date_end].iterrows():\n",
    "            if prev_day is None:\n",
    "                ticker_data[index] = 1\n",
    "            else:\n",
    "                ticker_data[index] = day['Adj Close'] / prev_day['Adj Close']\n",
    "            prev_day = day\n",
    "        tickers_prices_table[ticker] = ticker_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to pandas data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_return = pd.DataFrame.from_dict(tickers_prices_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_return_table = os.path.join(dir_run, run_prefix + '-returns.csv')\n",
    "df_return.to_csv(file_return_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get corpus statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_documents = 0\n",
    "term_in_documents_amount = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ticker in tickers_fits_for_analysis:\n",
    "    dir_ticker = os.path.join(dir_reports, ticker)\n",
    "    if os.path.isdir(dir_ticker):\n",
    "        for report in os.listdir(dir_ticker):\n",
    "            if '.csv' in report:\n",
    "                number_of_documents += 1  \n",
    "                with open(os.path.join(dir_ticker, report), 'r') as f_r:\n",
    "                    for text_line in f_r:\n",
    "                        (term, amount) = text_line.strip().split(',')\n",
    "                        if term not in term_in_documents_amount:\n",
    "                            term_in_documents_amount[term] = {\n",
    "                                'term': term,\n",
    "                                'total_usage': 0,\n",
    "                                'in_documents_amount': 0}\n",
    "                        term_in_documents_amount[term]['total_usage'] += int(amount)\n",
    "                        term_in_documents_amount[term]['in_documents_amount'] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8696"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'there are 27441 terms available for the experiment'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"there are %s terms available for the experiment\" % len(term_in_documents_amount) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtered term set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set terms limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_number_of_doc = 1\n",
    "remove_n_top_terms = 0\n",
    "max_partition_of_doc = 0.5\n",
    "max_number_of_doc = number_of_documents * max_partition_of_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get top N most common terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_top_n_terms_filter = set()\n",
    "sorted_terms = sorted(list(term_in_documents_amount.values()),\n",
    "                      key=operator.itemgetter('total_usage'),\n",
    "                      reverse=True)\n",
    "for i in range(remove_n_top_terms):\n",
    "    set_top_n_terms_filter.add(sorted_terms[i]['term'])\n",
    "    if flag_debug:\n",
    "        print(\"Excluded term: %s\" % sorted_terms[i]['term'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build terms set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_set = set()\n",
    "for term_o in sorted_terms:\n",
    "    term = term_o['term']\n",
    "    in_documents_amount = term_o['in_documents_amount']\n",
    "    total_usage = term_o['total_usage']\n",
    "    if term not in set_top_n_terms_filter:\n",
    "        if in_documents_amount > min_number_of_doc:\n",
    "            if in_documents_amount < max_number_of_doc:\n",
    "                terms_set.add(term)\n",
    "            elif flag_debug:\n",
    "                print(\"'%s' removed by max_number_of_doc\" % term)\n",
    "        elif flag_debug:\n",
    "            print(\"'%s' removed by min_number_of_doc\" % term)\n",
    "    elif flag_debug:\n",
    "        print(\"'%s' removed by top_n_term_filter\" % term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'there are 25368 filtered terms available for the experiment'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"there are %s filtered terms available for the experiment\" % len(terms_set) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build run data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create term -> id dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_term2id = {}\n",
    "id_counter = 0\n",
    "terms_list = sorted(terms_set)\n",
    "for term in terms_list:\n",
    "    dict_term2id[term] = id_counter\n",
    "    id_counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function report vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_report(file_report):\n",
    "    vector_report = list()\n",
    "    with open(file_report, 'r') as f_r:\n",
    "        for text_line in f_r:\n",
    "            term, tf = text_line.strip().split(',')\n",
    "            if term in dict_term2id:\n",
    "                vector_report.append(\"%s:%s\" % (dict_term2id[term], tf))\n",
    "    return vector_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read all reports (terms quantity map) for every ticker for every year in analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "amount_documents_in_series_dict = dict()\n",
    "documents_name_list = list()\n",
    "documents_vector_list = list()\n",
    "for year in year_series:\n",
    "    amount_documents_in_series = 0\n",
    "    #for every company read reports of a year\n",
    "    for ticker in sorted(tickers_fits_for_analysis):\n",
    "        dir_ticker_reports = os.path.join(dir_reports, ticker)\n",
    "        for report in os.listdir(dir_ticker_reports):\n",
    "            if int(report[:4]) == year:\n",
    "                #read a report\n",
    "                amount_documents_in_series += 1\n",
    "                documents_vector_list.append(vectorize_report(os.path.join(dir_ticker_reports, report)))\n",
    "                documents_name_list.append(ticker + '-' + report)\n",
    "\n",
    "    #keep track of documents in series\n",
    "    amount_documents_in_series_dict[int(year)] = amount_documents_in_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write results into files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save prefix-seq.dat\n",
    "with open(os.path.join(dir_run, run_prefix + '-seq.dat'), 'w') as f_w:\n",
    "    f_w.write(\"%s\\n\" % len(year_series))\n",
    "    for year in sorted(amount_documents_in_series_dict.keys()):\n",
    "        f_w.write(\"%s\\n\" % amount_documents_in_series_dict[year])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save prefix-mult.dat, every document in SVM\n",
    "with open(os.path.join(dir_run, run_prefix + '-mult.dat'), 'w') as f_w:\n",
    "    for document in documents_vector_list:\n",
    "        f_w.write(\"%s %s\\n\" % (len(document), ' '.join(document)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save prefix-documents.dat, every document the same order with mult.dat\n",
    "with open(os.path.join(dir_run, run_prefix + '-documents.dat'), 'w') as f_w:\n",
    "    for document in documents_name_list:\n",
    "        f_w.write(\"%s\\n\" % document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save prefix-documents.dat, every document the same order with mult.dat\n",
    "with open(os.path.join(dir_run, run_prefix + '-terms.dat'), 'w') as f_w:\n",
    "    for term in terms_list:\n",
    "        f_w.write(\"%s\\n\" % term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create result directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_results = os.path.join(dir_run, 'results')\n",
    "\n",
    "if not os.path.exists(dir_results):\n",
    "    os.makedirs(dir_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_results = os.path.join(dir_run, 'plots')\n",
    "\n",
    "if not os.path.exists(dir_results):\n",
    "    os.makedirs(dir_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_results = os.path.join(dir_run, 'interpretation')\n",
    "\n",
    "if not os.path.exists(dir_results):\n",
    "    os.makedirs(dir_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Go and run experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gogogogogo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
