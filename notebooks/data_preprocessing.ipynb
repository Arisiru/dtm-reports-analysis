{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import operator\n",
    "import math\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import multiprocessing\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set experiment dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_start = dt.datetime.strptime(\"2005-01-01\", \"%Y-%m-%d\")\n",
    "date_end = dt.datetime.strptime(\"2018-06-30\", \"%Y-%m-%d\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set run prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_prefix = \"run_02_22\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set root directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_root = os.path.join('..')\n",
    "dir_data_raw = os.path.join(dir_root, \"data_raw\")\n",
    "dir_data_processing = os.path.join(dir_root, \"data_processing\")\n",
    "dir_data_runs = os.path.join(dir_root, \"data_runs\")\n",
    "dir_prices = os.path.join(dir_data_processing, \"prices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_debug = False\n",
    "flag_use_strist_word_set = True\n",
    "flag_rerun_cleaning = False\n",
    "flag_test_report_names = False\n",
    "flag_run_topics_analysis = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create a run directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_run = os.path.join(dir_data_runs, run_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(dir_run):\n",
    "    os.makedirs(dir_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that all nltk data sets are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/alan.spark/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/alan.spark/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/alan.spark/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw to /Users/alan.spark/nltk_data...\n",
      "[nltk_data]   Package omw is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"words\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"omw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_words_set = None\n",
    "english_words_set_soft = set(nltk.corpus.words.words())\n",
    "english_words_set_strict = set([x.strip() for x in open(os.path.join(dir_data_raw, \"english\", \"strict_words.txt\"), \"r\")])\n",
    "lemmatizer = WordNetLemmatizer().lemmatize\n",
    "\n",
    "if flag_use_strist_word_set:\n",
    "    english_words_set = english_words_set_strict\n",
    "else:\n",
    "    english_words_set = english_words_set_soft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create stop words set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_set = set(nltk.corpus.stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extend stop words with custom stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(dir_data_raw, \"english\", \"extra_stopwords.txt\"), \"r\") as f_r:\n",
    "    for text_line in f_r:\n",
    "        stop_words_set.add(text_line.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a funtion checker for stop words. A word is stop word if any of the folowing true:\n",
    "- it's length shorter then 4 char\n",
    "- it contains digits\n",
    "- it appears in nltk stop words set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_stop_term(term):\n",
    "    if len(term) < 3:\n",
    "        return True\n",
    "    if re.search(\"[\\d]+\", term):\n",
    "        return True\n",
    "    return term in stop_words_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a funtion lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_lemma(possible_word):\n",
    "    possible_lemma = lemmatizer(possible_word)\n",
    "    possible_lemma = lemmatizer(possible_lemma, \"v\")\n",
    "    if not is_stop_term(possible_lemma) and possible_lemma in english_words_set:\n",
    "        return possible_lemma\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a clean report function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_report(file_report_path):\n",
    "    document = dict()\n",
    "    with open(os.path.join(file_report_path), \"r\",  encoding=\"ISO-8859-1\") as f_r:\n",
    "        for text_line in f_r:\n",
    "            words = re.split(\"\\W+\", text_line)\n",
    "            for word in words:\n",
    "                word_lower = word.lower()\n",
    "                term = get_word_lemma(word_lower)\n",
    "                if term is not None:\n",
    "                    if term in document:\n",
    "                        document[term] += 1\n",
    "                    else:\n",
    "                        document[term] = 1\n",
    "                elif flag_debug:\n",
    "                    print(\"%s is excluded from analisys\" % word_lower)\n",
    "    return document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate over all reports, clean and convert them to term frequency map, store as csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_report_name_reg_exp = \"(?P<ticker>[A-Z1-9]+)[_-](?P<type>[A-Z]+)(?P<number>[1-9]*)(?P<subnumber>-[1-9]+)?[-_](?P<year>[0-9]{4})[_-](?P<p_year>[0-9]{4})\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_reports_raw = os.path.join(dir_data_raw, \"reports_txt\")\n",
    "dir_reports =  os.path.join(dir_data_processing, \"reports\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell takes quite a while, be carefull and do not rerun it without a reason, results are stored at file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ticker_reports_processing(dir_findex, ticker):\n",
    "    dir_ticker = os.path.join(dir_findex, ticker)\n",
    "    if os.path.isdir(dir_ticker):\n",
    "        ticker_documents_amount = 0\n",
    "        ticker_code = \"%s_%s\" % (ticker, findex)\n",
    "        print(\"working on %s at %s\" % (ticker_code, dt.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "\n",
    "        for report_file_name in os.listdir(dir_ticker):\n",
    "            match = re.search(check_report_name_reg_exp, report_file_name)\n",
    "            if not match:\n",
    "                print(\"filename %s doesn't fit pattern\" % report_file_name)\n",
    "            else:\n",
    "                dict_report = clean_report(os.path.join(dir_ticker, report_file_name))\n",
    "                if len(dict_report):\n",
    "                    ticker_documents_amount += 1\n",
    "                    new_file_name = \"%s-%s.csv\" % (match.group(\"p_year\"), ticker_documents_amount)\n",
    "                    new_path = os.path.join(dir_reports, ticker_code)\n",
    "                    if not os.path.exists(new_path):\n",
    "                        os.makedirs(new_path)\n",
    "                    with open(os.path.join(new_path, new_file_name), \"w\") as f_w:\n",
    "                        for term, tf in dict_report.items():\n",
    "                            f_w.write(\"%s,%s\\n\" % (term, tf))\n",
    "                elif flag_debug:\n",
    "                    print(\"report %s is empty after cleaning\" % report_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_reports_names(dir_findex, ticker):\n",
    "    dir_ticker = os.path.join(dir_findex, ticker)\n",
    "    if os.path.isdir(dir_ticker):\n",
    "        ticker_documents_amount = 0\n",
    "        ticker_code = \"%s_%s\" % (ticker, findex)\n",
    "        for report_file_name in os.listdir(dir_ticker):\n",
    "            match = re.search(check_report_name_reg_exp, report_file_name)\n",
    "            if not match:\n",
    "                print(\"working on %s, filename %s doesn't fit pattern\" % (ticker_code, report_file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if flag_rerun_cleaning or flag_test_report_names:\n",
    "    for findex in os.listdir(dir_reports_raw):\n",
    "        dir_findex = os.path.join(dir_reports_raw, findex)\n",
    "        if os.path.isdir(dir_findex):\n",
    "            with multiprocessing.Pool(processes=4) as pool:\n",
    "                if flag_test_report_names: \n",
    "                    pool.starmap(test_reports_names, [(dir_findex, ticker) for ticker in os.listdir(dir_findex)])\n",
    "                else:\n",
    "                    pool.starmap(ticker_reports_processing, [(dir_findex, ticker) for ticker in os.listdir(dir_findex)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get tickers for analisys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to find all tickers of companies which have reports for the experiment's timeframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_tickers_years = os.path.join(dir_data_processing, \"tickers\", \"tickers_years.datajson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = dict()\n",
    "for ticker in os.listdir(dir_reports):\n",
    "    dir_ticker = os.path.join(dir_reports, ticker)\n",
    "    if os.path.isdir(dir_ticker):\n",
    "        ticker_years_set = set()\n",
    "        for report in os.listdir(dir_ticker):\n",
    "            ticker_years_set.add(int(report[:4]))\n",
    "        tickers[ticker] = sorted(ticker_years_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save years of companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_tickers_years, \"w\") as f_w:\n",
    "    for ticker in tickers:\n",
    "        f_w.write(\"%s\\n\" % json.dumps({\"ticker\": ticker, \"available_years\": tickers[ticker]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find tickers with reports for every year in the experiment timeframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_series = list(range(date_start.year, date_end.year)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if flag_debug:\n",
    "    print(year_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_data_tickers = os.path.join(dir_data_processing, \"tickers\")\n",
    "file_tickers_for_analysis = os.path.join(dir_data_tickers, \"ticker_for_analysis.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_fits_for_analysis = set()\n",
    "for ticker, available_years in tickers.items():\n",
    "    flag_complete_series = True\n",
    "    available_years_set = set(available_years)\n",
    "    for year in year_series:\n",
    "        if year not in available_years_set:\n",
    "            flag_complete_series = False\n",
    "            break\n",
    "    if flag_complete_series and os.path.exists(os.path.join(dir_prices, \"%s.csv\" % ticker)):\n",
    "        tickers_fits_for_analysis.add(ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'there are 78 tickers available for the experiment'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"there are %s tickers available for the experiment\" % len(tickers_fits_for_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save companies with complete years series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_tickers_for_analysis, \"w\") as f_w:\n",
    "    for ticker in tickers_fits_for_analysis:\n",
    "        f_w.write(\"%s\\n\" % ticker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Return table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_ticker_prices = os.path.join(dir_data_processing, \"prices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate over tickers available for analisys and build a return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_prices_table = {}\n",
    "for ticker in tickers_fits_for_analysis:\n",
    "    file_ticker_prices = os.path.join(dir_ticker_prices, ticker + \".csv\")\n",
    "    if os.path.isfile(file_ticker_prices):\n",
    "        price_df = pd.read_csv(file_ticker_prices)\n",
    "        price_df[\"Date\"] = pd.to_datetime(price_df[\"Date\"])\n",
    "        price_df.sort_values(by=[\"Date\"], inplace=True)\n",
    "        price_df.set_index(\"Date\", inplace=True)\n",
    "        ticker_data = {}\n",
    "        \n",
    "        prev_day = None\n",
    "        date_stat_price = dt.datetime.strptime(\"%s-01-01\" % (date_start.year + 1), \"%Y-%m-%d\")\n",
    "        \n",
    "        for index, day in price_df[date_stat_price : date_end].iterrows():\n",
    "            if prev_day is None:\n",
    "                ticker_data[index] = 1\n",
    "            else:\n",
    "                ticker_data[index] = day[\"Adj Close\"] / prev_day[\"Adj Close\"]\n",
    "            prev_day = day\n",
    "        tickers_prices_table[ticker] = ticker_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to pandas data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_return = pd.DataFrame.from_dict(tickers_prices_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_return_table = os.path.join(dir_run, run_prefix + \"-returns.csv\")\n",
    "df_return.to_csv(file_return_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get corpus statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_documents = 0\n",
    "term_in_documents_amount = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ticker in tickers_fits_for_analysis:\n",
    "    dir_ticker = os.path.join(dir_reports, ticker)\n",
    "    if os.path.isdir(dir_ticker):\n",
    "        for report in os.listdir(dir_ticker):\n",
    "            if '.csv' in report:\n",
    "                number_of_documents += 1  \n",
    "                with open(os.path.join(dir_ticker, report), 'r') as f_r:\n",
    "                    for text_line in f_r:\n",
    "                        (term, amount) = text_line.strip().split(',')\n",
    "                        if term not in term_in_documents_amount:\n",
    "                            term_in_documents_amount[term] = {\n",
    "                                'term': term,\n",
    "                                'total_usage': 0,\n",
    "                                'in_documents_amount': 0}\n",
    "                        term_in_documents_amount[term]['total_usage'] += int(amount)\n",
    "                        term_in_documents_amount[term]['in_documents_amount'] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5129"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'there are 21209 terms available for the experiment'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"there are %s terms available for the experiment\" % len(term_in_documents_amount) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtered term set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set terms limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_number_of_doc = 1\n",
    "remove_n_top_terms = 0\n",
    "max_partition_of_doc = 0.5\n",
    "max_number_of_doc = number_of_documents * max_partition_of_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get top N most common terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_top_n_terms_filter = set()\n",
    "sorted_terms = sorted(list(term_in_documents_amount.values()),\n",
    "                      key=operator.itemgetter('total_usage'),\n",
    "                      reverse=True)\n",
    "for i in range(remove_n_top_terms):\n",
    "    set_top_n_terms_filter.add(sorted_terms[i]['term'])\n",
    "    if flag_debug:\n",
    "        print(\"Excluded term: %s\" % sorted_terms[i]['term'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build terms set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_set = set()\n",
    "for term_o in sorted_terms:\n",
    "    term = term_o['term']\n",
    "    in_documents_amount = term_o['in_documents_amount']\n",
    "    total_usage = term_o['total_usage']\n",
    "    if term not in set_top_n_terms_filter:\n",
    "        if in_documents_amount > min_number_of_doc:\n",
    "            if in_documents_amount < max_number_of_doc:\n",
    "                terms_set.add(term)\n",
    "            elif flag_debug:\n",
    "                print(\"'%s' removed by max_number_of_doc\" % term)\n",
    "        elif flag_debug:\n",
    "            print(\"'%s' removed by min_number_of_doc\" % term)\n",
    "    elif flag_debug:\n",
    "        print(\"'%s' removed by top_n_term_filter\" % term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'there are 17899 filtered terms available for the experiment'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"there are %s filtered terms available for the experiment\" % len(terms_set) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create term -> id  and id -> term dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_term2id = {}\n",
    "dict_id2term = {}\n",
    "\n",
    "id_counter = 0\n",
    "terms_list = sorted(terms_set)\n",
    "for term in terms_list:\n",
    "    dict_term2id[term] = id_counter\n",
    "    dict_id2term[id_counter] = term\n",
    "    id_counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a function report vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_report_dtm(file_report):\n",
    "    vector_report = list()\n",
    "    with open(file_report, 'r') as f_r:\n",
    "        for text_line in f_r:\n",
    "            term, tf = text_line.strip().split(',')\n",
    "            if term in dict_term2id:\n",
    "                vector_report.append(\"%s:%s\" % (dict_term2id[term], tf))\n",
    "    return vector_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_report_test(file_report, dict_term2id_local, dict_id2term_local):\n",
    "    local_term_id = 0\n",
    "    vector_report = list()\n",
    "    with open(file_report, 'r') as f_r:\n",
    "        for text_line in f_r:\n",
    "            term, tf = text_line.strip().split(',')\n",
    "            if term in dict_term2id:\n",
    "                if term not in dict_term2id_local:\n",
    "                    dict_term2id_local[term] = local_term_id\n",
    "                    dict_id2term_local[local_term_id] = term\n",
    "                    local_term_id += 1\n",
    "                vector_report.append((dict_term2id_local[term], int(tf)))\n",
    "    return vector_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find optimal topic number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2005"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_year = year_series[0]\n",
    "experiment_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_term2id_year = {}\n",
    "dict_id2term_year = {}\n",
    "\n",
    "corpus_of_year = []\n",
    "for ticker in sorted(tickers_fits_for_analysis):\n",
    "    dir_ticker_reports = os.path.join(dir_reports, ticker)\n",
    "    for report in os.listdir(dir_ticker_reports):\n",
    "        if int(report[:4]) == year:\n",
    "            file_report = os.path.join(dir_ticker_reports, report)\n",
    "            corpus_of_year.append(vectorize_report_test(file_report, dict_term2id_year, dict_id2term_year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quality_measurements(dictionary, corpus, limit, start=2, step=3):\n",
    "    topics_nums = []\n",
    "    coherence_values = []\n",
    "    perplexity_values = []\n",
    "    for num_topics in range(start, limit + 1, step):\n",
    "        topics_nums.append(num_topics)\n",
    "        lda_model = gensim.models.ldamodel.LdaModel(\n",
    "            corpus=corpus,\n",
    "            id2word=dictionary,\n",
    "            num_topics=num_topics, \n",
    "            random_state=100,\n",
    "            passes=10,\n",
    "            iterations=50,\n",
    "            alpha=\"auto\",\n",
    "            per_word_topics=True)\n",
    "        perplexity = lda_model.log_perplexity(corpus)\n",
    "        perplexity_values.append(perplexity)\n",
    "        coherence_model_lda = CoherenceModel(\n",
    "            model=lda_model,\n",
    "            corpus=corpus,\n",
    "            dictionary=dictionary,\n",
    "            coherence='u_mass')\n",
    "        coherence = coherence_model_lda.get_coherence()\n",
    "        coherence_values.append(coherence)\n",
    "        \n",
    "        print(\"Topics: %s, coh: %s, perplex: %s\" % (num_topics, coherence, perplexity))\n",
    "    return {\n",
    "        \"topics_num\": topics_nums,\n",
    "        \"coherence_values\": coherence_values,\n",
    "        \"perplexity_values\": perplexity_values\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_min = 10\n",
    "topic_max = 50\n",
    "topic_step = 2\n",
    "measures = None\n",
    "if flag_run_topics_analysis:\n",
    "    measures = get_quality_measurements(dict_id2term_year, corpus_of_year, topic_max, topic_min, topic_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "if measures is not None:\n",
    "    plt.plot(measures[\"topics_num\"], measures[\"coherence_values\"])\n",
    "    plt.xlabel(\"Num Topics\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "if measures is not None:\n",
    "    plt.plot(measures[\"topics_num\"], measures[\"perplexity_values\"])\n",
    "    plt.xlabel(\"Num Topics\")\n",
    "    plt.ylabel(\"Perplexity score\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build run data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read all reports (terms quantity map) for every ticker for every year in analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "amount_documents_in_series_dict = dict()\n",
    "documents_name_list = list()\n",
    "documents_vector_list = list()\n",
    "for year in year_series:\n",
    "    amount_documents_in_series = 0\n",
    "    #for every company read reports of a year\n",
    "    for ticker in sorted(tickers_fits_for_analysis):\n",
    "        dir_ticker_reports = os.path.join(dir_reports, ticker)\n",
    "        for report in os.listdir(dir_ticker_reports):\n",
    "            if int(report[:4]) == year:\n",
    "                #read a report\n",
    "                amount_documents_in_series += 1\n",
    "                documents_vector_list.append(vectorize_report_dtm(os.path.join(dir_ticker_reports, report)))\n",
    "                documents_name_list.append(ticker + '-' + report)\n",
    "\n",
    "    #keep track of documents in series\n",
    "    amount_documents_in_series_dict[int(year)] = amount_documents_in_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write results into files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save prefix-seq.dat\n",
    "with open(os.path.join(dir_run, run_prefix + '-seq.dat'), 'w') as f_w:\n",
    "    f_w.write(\"%s\\n\" % len(year_series))\n",
    "    for year in sorted(amount_documents_in_series_dict.keys()):\n",
    "        f_w.write(\"%s\\n\" % amount_documents_in_series_dict[year])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save prefix-mult.dat, every document in SVM\n",
    "with open(os.path.join(dir_run, run_prefix + '-mult.dat'), 'w') as f_w:\n",
    "    for document in documents_vector_list:\n",
    "        f_w.write(\"%s %s\\n\" % (len(document), ' '.join(document)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save prefix-documents.dat, every document the same order with mult.dat\n",
    "with open(os.path.join(dir_run, run_prefix + '-documents.dat'), 'w') as f_w:\n",
    "    for document in documents_name_list:\n",
    "        f_w.write(\"%s\\n\" % document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save prefix-documents.dat, every document the same order with mult.dat\n",
    "with open(os.path.join(dir_run, run_prefix + '-terms.dat'), 'w') as f_w:\n",
    "    for term in terms_list:\n",
    "        f_w.write(\"%s\\n\" % term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create result directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_results = os.path.join(dir_run, 'results')\n",
    "\n",
    "if not os.path.exists(dir_results):\n",
    "    os.makedirs(dir_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_results = os.path.join(dir_run, 'interpretation')\n",
    "\n",
    "if not os.path.exists(dir_results):\n",
    "    os.makedirs(dir_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Go and run experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "go go go go go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
