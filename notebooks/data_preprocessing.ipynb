{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import operator\n",
    "import math\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import multiprocessing\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set experiment dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_start = dt.datetime.strptime(\"2005-01-01\", \"%Y-%m-%d\")\n",
    "date_end = dt.datetime.strptime(\"2018-06-30\", \"%Y-%m-%d\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set run prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_prefix = \"run_16_11\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set root directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_root = os.path.join('..')\n",
    "dir_data_raw = os.path.join(dir_root, \"data_raw\")\n",
    "dir_data_processing = os.path.join(dir_root, \"data_processing\")\n",
    "dir_data_runs = os.path.join(dir_root, \"data_runs\")\n",
    "dir_prices = os.path.join(dir_data_processing, \"prices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_debug = False\n",
    "flag_terms_filter_debug = False\n",
    "flag_use_strist_word_set = False\n",
    "flag_rerun_cleaning = False\n",
    "flag_test_report_names = False\n",
    "flag_run_topics_analysis = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create a run directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_run = os.path.join(dir_data_runs, run_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(dir_run):\n",
    "    os.makedirs(dir_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that all nltk data sets are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/alan.spark/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/alan.spark/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/alan.spark/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw to /Users/alan.spark/nltk_data...\n",
      "[nltk_data]   Package omw is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"words\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"omw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_words_set = None\n",
    "english_words_set_soft = set(nltk.corpus.words.words())\n",
    "english_words_set_strict = set([x.strip() for x in open(os.path.join(dir_data_raw, \"english\", \"strict_words.txt\"), \"r\")])\n",
    "lemmatizer = WordNetLemmatizer().lemmatize\n",
    "stemmer = SnowballStemmer(\"english\").stem\n",
    "\n",
    "if flag_use_strist_word_set:\n",
    "    english_words_set = english_words_set_strict\n",
    "else:\n",
    "    english_words_set = english_words_set_soft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create stop words set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_set = set(nltk.corpus.stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extend stop words with custom stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(dir_data_raw, \"english\", \"extra_stopwords.txt\"), \"r\") as f_r:\n",
    "    for text_line in f_r:\n",
    "        term = text_line.strip()\n",
    "        stop_words_set.add(term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Craeate a white list of stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "white_list_set = set()\n",
    "with open(os.path.join(dir_data_raw, \"english\", \"white_stopwords.txt\"), \"r\") as f_r:\n",
    "    for text_line in f_r:\n",
    "        term = text_line.strip()\n",
    "        white_list_set.add(term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a funtion checker for stop words. A word is stop word if any of the folowing true:\n",
    "- it's length shorter then 4 char\n",
    "- it contains digits\n",
    "- it appears in nltk stop words set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_stop_term(term):\n",
    "    if term in white_list_set:\n",
    "        return False\n",
    "    if len(term) < 4:\n",
    "        return True\n",
    "    return term in stop_words_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a funtion lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_lemma(possible_word):\n",
    "    if possible_word not in english_words_set:\n",
    "        return None\n",
    "    \n",
    "    possible_lemma = lemmatizer(possible_word)\n",
    "    #possible_lemma = stemmer(possible_lemma)    \n",
    "\n",
    "    if not is_stop_term(possible_lemma):\n",
    "        return possible_lemma\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "regexp_to_remove = re.compile(r\"[\\dâºâãï½ã\\_]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a clean report function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_report(file_report_path, set_eliminated_terms):\n",
    "    document = dict()\n",
    "    with open(os.path.join(file_report_path), \"r\",  encoding=\"ISO-8859-1\") as f_r:\n",
    "        for text_line in f_r:\n",
    "            cleaned_text = re.sub(regexp_to_remove, \" \", text_line)\n",
    "            words = re.split(\"\\W+\", cleaned_text)\n",
    "            for word in words:\n",
    "                word_lower = word.lower()\n",
    "                term = get_word_lemma(word_lower)\n",
    "                if term is not None:\n",
    "                    if term in document:\n",
    "                        document[term] += 1\n",
    "                    else:\n",
    "                        document[term] = 1    \n",
    "                elif flag_terms_filter_debug:\n",
    "                    set_eliminated_terms.add(word_lower)\n",
    "    return document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate over all reports, clean and convert them to term frequency map, store as csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_report_name_reg_exp = \"(?P<ticker>[A-Z1-9]+)[_-](?P<type>[A-Z]+)(?P<number>[1-9]*)(?P<subnumber>-[1-9]+)?[-_](?P<year>[0-9]{4})[_-](?P<p_year>[0-9]{4})\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_reports_raw = os.path.join(dir_data_raw, \"reports_txt\")\n",
    "dir_reports =  os.path.join(dir_data_processing, \"reports\")\n",
    "dir_eliminated_terms = os.path.join(dir_data_processing, \"terms_elemenated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell takes quite a while, be carefull and do not rerun it without a reason, results are stored at file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ticker_reports_processing(dir_findex, ticker):\n",
    "    set_eliminated_terms = set()\n",
    "    dir_ticker = os.path.join(dir_findex, ticker)\n",
    "    print(\"working on %s at %s\" % (dir_ticker, dt.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "    if os.path.isdir(dir_ticker):\n",
    "        ticker_documents_amount = 0\n",
    "        ticker_code = \"%s_%s\" % (ticker, findex)\n",
    "        for report_file_name in os.listdir(dir_ticker):\n",
    "            match = re.search(check_report_name_reg_exp, report_file_name)\n",
    "            if not match:\n",
    "                print(\"filename %s doesn't fit pattern\" % report_file_name)\n",
    "            else:\n",
    "                dict_report = clean_report(os.path.join(dir_ticker, report_file_name), set_eliminated_terms)\n",
    "                if len(dict_report):\n",
    "                    ticker_documents_amount += 1\n",
    "                    new_file_name = \"%s-%s.csv\" % (match.group(\"p_year\"), ticker_documents_amount)\n",
    "                    new_path = os.path.join(dir_reports, ticker_code)\n",
    "                    if not os.path.exists(new_path):\n",
    "                        os.makedirs(new_path)\n",
    "                    with open(os.path.join(new_path, new_file_name), \"w\") as f_w:\n",
    "                        for term, tf in dict_report.items():\n",
    "                            f_w.write(\"%s,%s\\n\" % (term, tf))\n",
    "                elif flag_debug:\n",
    "                    print(\"report %s is empty after cleaning\" % report_file_name)\n",
    "        if len(set_eliminated_terms):\n",
    "            with open(os.path.join(dir_eliminated_terms, \"%s.txt\" % ticker_code), \"w\") as f_w:\n",
    "                for eliminated in sorted(set_eliminated_terms):\n",
    "                    f_w.write(\"%s\\n\" % (eliminated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_reports_names(dir_findex, ticker):\n",
    "    dir_ticker = os.path.join(dir_findex, ticker)\n",
    "    if os.path.isdir(dir_ticker):\n",
    "        ticker_documents_amount = 0\n",
    "        ticker_code = \"%s_%s\" % (ticker, findex)\n",
    "        for report_file_name in os.listdir(dir_ticker):\n",
    "            match = re.search(check_report_name_reg_exp, report_file_name)\n",
    "            if not match:\n",
    "                print(\"working on %s, filename %s doesn't fit pattern\" % (ticker_code, report_file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if flag_rerun_cleaning or flag_test_report_names:\n",
    "    for findex in os.listdir(dir_reports_raw):\n",
    "        dir_findex = os.path.join(dir_reports_raw, findex)\n",
    "        if os.path.isdir(dir_findex):\n",
    "            with multiprocessing.Pool(processes=4) as pool:\n",
    "                if flag_test_report_names: \n",
    "                    pool.starmap(test_reports_names, [(dir_findex, ticker) for ticker in os.listdir(dir_findex)])\n",
    "                else:\n",
    "                    pool.starmap(ticker_reports_processing, [(dir_findex, ticker) for ticker in os.listdir(dir_findex)])\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "eliminated_words = set()\n",
    "for file in os.listdir(dir_eliminated_terms):\n",
    "    if not file.endswith('.DS_Store'):\n",
    "        with open(os.path.join(dir_eliminated_terms, file), \"r\") as f_r:\n",
    "            for text in f_r:\n",
    "                eliminated_words.add(text.strip())\n",
    "if flag_terms_filter_debug:\n",
    "    print(\"list of eliminated terms, size(%s)\" % len(eliminated_words))\n",
    "    for eliminated in sorted(eliminated_words):\n",
    "        print(\" %s \" % eliminated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get tickers for analisys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to find all tickers of companies which have reports for the experiment's timeframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_tickers_years = os.path.join(dir_data_processing, \"tickers\", \"tickers_years.datajson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = dict()\n",
    "for ticker in os.listdir(dir_reports):\n",
    "    dir_ticker = os.path.join(dir_reports, ticker)\n",
    "    if os.path.isdir(dir_ticker):\n",
    "        ticker_years_set = set()\n",
    "        for report in os.listdir(dir_ticker):\n",
    "            ticker_years_set.add(int(report[:4]))\n",
    "        tickers[ticker] = sorted(ticker_years_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save years of companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_tickers_years, \"w\") as f_w:\n",
    "    for ticker in tickers:\n",
    "        f_w.write(\"%s\\n\" % json.dumps({\"ticker\": ticker, \"available_years\": tickers[ticker]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find tickers with reports for every year in the experiment timeframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_series = list(range(date_start.year, date_end.year)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if flag_debug:\n",
    "    print(year_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_data_tickers = os.path.join(dir_data_processing, \"tickers\")\n",
    "file_tickers_for_analysis = os.path.join(dir_data_tickers, \"ticker_for_analysis.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_fits_for_analysis = set()\n",
    "for ticker, available_years in tickers.items():\n",
    "    flag_complete_series = True\n",
    "    available_years_set = set(available_years)\n",
    "    for year in year_series:\n",
    "        if year not in available_years_set:\n",
    "            flag_complete_series = False\n",
    "            break\n",
    "    if flag_complete_series and os.path.exists(os.path.join(dir_prices, \"%s.csv\" % ticker)):\n",
    "        tickers_fits_for_analysis.add(ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'there are 78 tickers available for the experiment'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"there are %s tickers available for the experiment\" % len(tickers_fits_for_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save companies with complete years series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_tickers_for_analysis, \"w\") as f_w:\n",
    "    for ticker in tickers_fits_for_analysis:\n",
    "        f_w.write(\"%s\\n\" % ticker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Return table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_ticker_prices = os.path.join(dir_data_processing, \"prices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate over tickers available for analisys and build a return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_prices_table = {}\n",
    "for ticker in tickers_fits_for_analysis:\n",
    "    file_ticker_prices = os.path.join(dir_ticker_prices, ticker + \".csv\")\n",
    "    if os.path.isfile(file_ticker_prices):\n",
    "        price_df = pd.read_csv(file_ticker_prices)\n",
    "        price_df[\"Date\"] = pd.to_datetime(price_df[\"Date\"])\n",
    "        price_df.sort_values(by=[\"Date\"], inplace=True)\n",
    "        price_df.set_index(\"Date\", inplace=True)\n",
    "        ticker_data = {}\n",
    "        \n",
    "        prev_day = None\n",
    "        date_stat_price = dt.datetime.strptime(\"%s-01-01\" % (date_start.year + 1), \"%Y-%m-%d\")\n",
    "        \n",
    "        for index, day in price_df[date_stat_price : date_end].iterrows():\n",
    "            if prev_day is None:\n",
    "                ticker_data[index] = 1\n",
    "            else:\n",
    "                ticker_data[index] = day[\"Adj Close\"] / prev_day[\"Adj Close\"]\n",
    "            prev_day = day\n",
    "        tickers_prices_table[ticker] = ticker_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to pandas data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_return = pd.DataFrame.from_dict(tickers_prices_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_return_table = os.path.join(dir_run, run_prefix + \"-returns.csv\")\n",
    "df_return.to_csv(file_return_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get corpus statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_documents = 0\n",
    "term_in_documents_amount = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ticker in tickers_fits_for_analysis:\n",
    "    dir_ticker = os.path.join(dir_reports, ticker)\n",
    "    if os.path.isdir(dir_ticker):\n",
    "        for report in os.listdir(dir_ticker):\n",
    "            if '.csv' in report:\n",
    "                number_of_documents += 1  \n",
    "                with open(os.path.join(dir_ticker, report), 'r') as f_r:\n",
    "                    for text_line in f_r:\n",
    "                        (term, amount) = text_line.strip().split(',')\n",
    "                        if term not in term_in_documents_amount:\n",
    "                            term_in_documents_amount[term] = {\n",
    "                                'term': term,\n",
    "                                'total_usage': 0,\n",
    "                                'in_documents_amount': 0}\n",
    "                        term_in_documents_amount[term]['total_usage'] += int(amount)\n",
    "                        term_in_documents_amount[term]['in_documents_amount'] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5129"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'there are 27891 terms available for the experiment'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"there are %s terms available for the experiment\" % len(term_in_documents_amount) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtered term set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set terms limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_number_of_doc = 3\n",
    "remove_n_top_terms = 0\n",
    "max_partition_of_doc = 0.5\n",
    "max_number_of_doc = number_of_documents * max_partition_of_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get top N most common terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_top_n_terms_filter = set()\n",
    "sorted_terms = sorted(list(term_in_documents_amount.values()),\n",
    "                      key=operator.itemgetter('total_usage'),\n",
    "                      reverse=True)\n",
    "for i in range(remove_n_top_terms):\n",
    "    set_top_n_terms_filter.add(sorted_terms[i]['term'])\n",
    "    if flag_debug:\n",
    "        print(\"Excluded term: %s\" % sorted_terms[i]['term'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build terms set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_eliminated_by_max_df = set()\n",
    "set_eliminated_by_min_df = set()\n",
    "set_eliminated_by_max_tf = set()\n",
    "\n",
    "terms_set = set()\n",
    "for term_o in sorted_terms:\n",
    "    term = term_o['term']\n",
    "    in_documents_amount = term_o['in_documents_amount']\n",
    "    total_usage = term_o['total_usage']\n",
    "    if term not in set_top_n_terms_filter:\n",
    "        if in_documents_amount < max_number_of_doc:\n",
    "            if in_documents_amount > min_number_of_doc:\n",
    "                terms_set.add(term)\n",
    "            elif flag_terms_filter_debug:\n",
    "                set_eliminated_by_min_df.add(term)        \n",
    "        elif flag_terms_filter_debug:\n",
    "            set_eliminated_by_max_df.add(term)\n",
    "    elif flag_terms_filter_debug:\n",
    "            set_eliminated_by_max_tf.add(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'there are 18750 filtered terms available for the experiment'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"there are %s filtered terms available for the experiment\" % len(terms_set) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminated by exiding maximum terms frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eliminated: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Eliminated: %s\" % len(set_eliminated_by_max_tf))\n",
    "for eliminated in sorted(set_eliminated_by_max_tf):\n",
    "    print(\" %s \" % eliminated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminated by exiding maximum document frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eliminated: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Eliminated: %s\" % len(set_eliminated_by_max_df))\n",
    "for eliminated in sorted(set_eliminated_by_max_df):\n",
    "    print(\" %s \" % eliminated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminated by no reaching minimum document frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eliminated: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Eliminated: %s\" % len(set_eliminated_by_min_df))\n",
    "for eliminated in sorted(set_eliminated_by_min_df):\n",
    "    print( \"%s \" % eliminated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create term -> id  and id -> term dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_term2id = {}\n",
    "dict_id2term = {}\n",
    "\n",
    "id_counter = 0\n",
    "terms_list = sorted(terms_set)\n",
    "for term in terms_list:\n",
    "    dict_term2id[term] = id_counter\n",
    "    dict_id2term[id_counter] = term\n",
    "    id_counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a function report vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_report_dtm(file_report):\n",
    "    vector_report = list()\n",
    "    with open(file_report, 'r') as f_r:\n",
    "        for text_line in f_r:\n",
    "            term, tf = text_line.strip().split(',')\n",
    "            if term in dict_term2id:\n",
    "                vector_report.append(\"%s:%s\" % (dict_term2id[term], tf))\n",
    "    return vector_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_report_test(file_report, dict_term2id_local, dict_id2term_local):\n",
    "    local_term_id = 0\n",
    "    vector_report = list()\n",
    "    with open(file_report, 'r') as f_r:\n",
    "        for text_line in f_r:\n",
    "            term, tf = text_line.strip().split(',')\n",
    "            if term in dict_term2id:\n",
    "                if term not in dict_term2id_local:\n",
    "                    dict_term2id_local[term] = local_term_id\n",
    "                    dict_id2term_local[local_term_id] = term\n",
    "                    local_term_id += 1\n",
    "                vector_report.append((dict_term2id_local[term], int(tf)))\n",
    "    return vector_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find optimal topic number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quality_corpus(year):\n",
    "    dict_term2id_year = {}\n",
    "    dict_id2term_year = {}\n",
    "\n",
    "    corpus_of_year = []\n",
    "    for ticker in sorted(tickers_fits_for_analysis):\n",
    "        dir_ticker_reports = os.path.join(dir_reports, ticker)\n",
    "        for report in os.listdir(dir_ticker_reports):\n",
    "            if int(report[:4]) == year:\n",
    "                file_report = os.path.join(dir_ticker_reports, report)\n",
    "                corpus_of_year.append(vectorize_report_test(file_report, dict_term2id_year, dict_id2term_year))\n",
    "    \n",
    "    return dict_id2term_year, corpus_of_year      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quality_measurements(year, dictionary, corpus, limit, start=2, step=3):\n",
    "    topics_nums = []\n",
    "    coherence_values = []\n",
    "    perplexity_values = []\n",
    "    for num_topics in range(start, limit + 1, step):\n",
    "        topics_nums.append(num_topics)\n",
    "        lda_model = gensim.models.ldamodel.LdaModel(\n",
    "            corpus=corpus,\n",
    "            id2word=dictionary,\n",
    "            num_topics=num_topics, \n",
    "            random_state=100,\n",
    "            iterations=100,\n",
    "            alpha=\"auto\",\n",
    "            per_word_topics=True)\n",
    "        perplexity = lda_model.log_perplexity(corpus)\n",
    "        perplexity_values.append(perplexity)\n",
    "        coherence_model_lda = CoherenceModel(\n",
    "            model=lda_model,\n",
    "            corpus=corpus,\n",
    "            dictionary=dictionary,\n",
    "            coherence='u_mass')\n",
    "        coherence = coherence_model_lda.get_coherence()\n",
    "        coherence_values.append(coherence)\n",
    "        \n",
    "        #print(\"Topics: %s, coh: %s, perplex: %s\" % (num_topics, coherence, perplexity))\n",
    "    return {\n",
    "        \"year\": year,\n",
    "        \"topics_num\": topics_nums,\n",
    "        \"coherence_values\": coherence_values,\n",
    "        \"perplexity_values\": perplexity_values\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_min = 10\n",
    "topic_max = 40\n",
    "topic_step = 1\n",
    "measures = []\n",
    "if flag_run_topics_analysis:\n",
    "    #for year in year_series:\n",
    "    year = year_series[0]\n",
    "    dict_id2term_year, corpus_of_year = get_quality_corpus(year)\n",
    "    print(\"Work on %s year, dictionary size: %s\" % (year, len(dict_id2term_year)))\n",
    "    measures.append(\n",
    "        get_quality_measurements(\n",
    "            year, \n",
    "            dict_id2term_year, \n",
    "            corpus_of_year, \n",
    "            topic_max, \n",
    "            topic_min, \n",
    "            topic_step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_topics = os.path.join(dir_run, 'topics')\n",
    "\n",
    "if not os.path.exists(dir_topics):\n",
    "    os.makedirs(dir_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(measures):\n",
    "    plt.figure(figsize=(10,10))\n",
    "                                        # Plot all years\n",
    "                                        #for year_measures in measures:\n",
    "                                            #plt.plot(year_measures[\"topics_num\"], year_measures[\"coherence_values\"], label=year_measures[\"year\"])\n",
    "                                            #for i, v in enumerate(year_measures[\"coherence_values\"]):\n",
    "                                            #    print(\"topic: %s :%s\" % (year_measures[\"topics_num\"][i], v))\n",
    "    # Plot a single year\n",
    "    year_measures = measures[0]\n",
    "    y = np.array(year_measures[\"coherence_values\"])\n",
    "    x = np.array(year_measures[\"topics_num\"])\n",
    "    plt.plot(x, y)    \n",
    "    \n",
    "    \n",
    "    xmax = x[np.argmax(y)]\n",
    "    ymax = y.max()\n",
    "    text= \"Num of Topics=%s, score=%f\" % (xmax, ymax)\n",
    "\n",
    "    bbox_props = dict(boxstyle=\"square,pad=0.3\", fc=\"w\", ec=\"k\", lw=0.72)\n",
    "    arrowprops=dict(arrowstyle=\"->\",connectionstyle=\"angle,angleA=0,angleB=359\")\n",
    "    kw = dict(xycoords='data',textcoords=\"axes fraction\",\n",
    "              arrowprops=arrowprops, bbox=bbox_props, ha=\"right\", va=\"top\")\n",
    "    plt.annotate(text, xy=(xmax, ymax), xytext=(0.94,0.96), **kw)\n",
    "\n",
    "    \n",
    "    \n",
    "    plt.xlabel(\"Num Topics\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    \n",
    "    pp = PdfPages(os.path.join(dir_topics, \"cohearence_score.pdf\"))\n",
    "    \n",
    "    plt.savefig(pp, format='pdf', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    pp.close() \n",
    "    plt.close()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(measures):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    # Plot a single year\n",
    "    year_measures = measures[0]    \n",
    "    plt.plot(year_measures[\"topics_num\"], year_measures[\"perplexity_values\"], label=year_measures[\"year\"])\n",
    "    plt.xlabel(\"Num Topics\")\n",
    "    plt.ylabel(\"Perplexity score\")\n",
    "    plt.legend(bbox_to_anchor=(1.002, 1.002))\n",
    "\n",
    "    pp = PdfPages(os.path.join(dir_topics, \"perplexity_score.pdf\"))\n",
    "\n",
    "    plt.savefig(pp, format='pdf', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    pp.close() \n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(0, 45, 1):\n",
    "#    print(\"topics_num:%s  c:%s p:%s\" % (measures[0][\"topics_num\"][i], measures[0][\"coherence_values\"][i],measures[0][\"perplexity_values\"][i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build run data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read all reports (terms quantity map) for every ticker for every year in analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "amount_documents_in_series_dict = dict()\n",
    "documents_name_list = list()\n",
    "documents_vector_list = list()\n",
    "for year in year_series:\n",
    "    amount_documents_in_series = 0\n",
    "    #for every company read reports of a year\n",
    "    for ticker in sorted(tickers_fits_for_analysis):\n",
    "        dir_ticker_reports = os.path.join(dir_reports, ticker)\n",
    "        for report in os.listdir(dir_ticker_reports):\n",
    "            if int(report[:4]) == year:\n",
    "                #read a report\n",
    "                amount_documents_in_series += 1\n",
    "                documents_vector_list.append(vectorize_report_dtm(os.path.join(dir_ticker_reports, report)))\n",
    "                documents_name_list.append(ticker + '-' + report)\n",
    "\n",
    "    #keep track of documents in series\n",
    "    amount_documents_in_series_dict[int(year)] = amount_documents_in_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write results into files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save prefix-seq.dat\n",
    "with open(os.path.join(dir_run, run_prefix + '-seq.dat'), 'w') as f_w:\n",
    "    f_w.write(\"%s\\n\" % len(year_series))\n",
    "    for year in sorted(amount_documents_in_series_dict.keys()):\n",
    "        f_w.write(\"%s\\n\" % amount_documents_in_series_dict[year])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save prefix-mult.dat, every document in SVM\n",
    "with open(os.path.join(dir_run, run_prefix + '-mult.dat'), 'w') as f_w:\n",
    "    for document in documents_vector_list:\n",
    "        f_w.write(\"%s %s\\n\" % (len(document), ' '.join(document)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save prefix-documents.dat, every document the same order with mult.dat\n",
    "with open(os.path.join(dir_run, run_prefix + '-documents.dat'), 'w') as f_w:\n",
    "    for document in documents_name_list:\n",
    "        f_w.write(\"%s\\n\" % document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save prefix-documents.dat, every document the same order with mult.dat\n",
    "with open(os.path.join(dir_run, run_prefix + '-terms.dat'), 'w') as f_w:\n",
    "    for term in terms_list:\n",
    "        f_w.write(\"%s\\n\" % term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create result directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_results = os.path.join(dir_run, 'results')\n",
    "\n",
    "if not os.path.exists(dir_results):\n",
    "    os.makedirs(dir_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_results = os.path.join(dir_run, 'interpretation')\n",
    "\n",
    "if not os.path.exists(dir_results):\n",
    "    os.makedirs(dir_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Go and run experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "go go go go go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
