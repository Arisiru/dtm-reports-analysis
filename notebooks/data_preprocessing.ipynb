{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import operator\n",
    "import math\n",
    "import multiprocessing\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set experiment dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_start = dt.datetime.strptime(\"2005-01-01\", \"%Y-%m-%d\")\n",
    "date_end = dt.datetime.strptime(\"2021-06-30\", \"%Y-%m-%d\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set root directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_root = os.path.join('..')\n",
    "dir_data_raw = os.path.join(dir_root, \"data_raw\")\n",
    "dir_data_processing = os.path.join(dir_root, \"data_processing\")\n",
    "dir_data_runs = os.path.join(dir_root, \"data_runs\")\n",
    "dir_ticker_prices_source = os.path.join(dir_data_raw, \"prices\", \"ready\")\n",
    "dir_ticker_prices_destination = os.path.join(dir_data_processing, \"prices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set reports directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_reports_txt = os.path.join(dir_data_raw, \"reports_txt\")\n",
    "dir_reports_words = os.path.join(dir_data_processing, \"reports_words\")\n",
    "dir_reports_terms = os.path.join(dir_data_processing, \"reports_terms\")\n",
    "dir_reports_grams = os.path.join(dir_data_processing, \"reports_gramms\")\n",
    "dir_reports_ready =  os.path.join(dir_data_processing, \"reports_ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set terms directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_terms_eliminated = os.path.join(dir_data_processing, \"terms_elemenated\")\n",
    "dir_terms_counts = os.path.join(dir_data_processing, \"terms_counts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set report name RegExt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_report_name_reg_exp = \"(?P<ticker>[A-Z1-9]+)[_-](?P<type>[A-Z]+)(?P<number>[1-9A-Z]*)(?P<subnumber>[_-]+[0-9]+)?[_-](?P<year>[0-9]{4})[_-](?P<p_year>[0-9]{4})\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_debug = False\n",
    "flag_terms_filter_debug = False\n",
    "\n",
    "flag_fix_reports_names = False\n",
    "\n",
    "flag_extend_stopwords = True\n",
    "flag_test_report_names = True\n",
    "flag_filtering_with_bigramms = True\n",
    "\n",
    "flag_rerun_text_2_words = False\n",
    "flag_rerun_words_2_terms = False\n",
    "# keep it True, whenever you run it after adding some \n",
    "# tickers with limited textual procesing notebook\n",
    "flag_rerun_terms_2_gramms = True \n",
    "# keep it True, it generates the last reports ready data\n",
    "flag_rerun_filter_terms = True \n",
    "flag_filter_by_fixed_terms = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_series = list(range(date_start.year, date_end.year)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020]\n"
     ]
    }
   ],
   "source": [
    "print(year_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test reports names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_reports_names(dir_findex, ticker):\n",
    "    dir_ticker = os.path.join(dir_findex, ticker)\n",
    "    if os.path.isdir(dir_ticker):\n",
    "        ticker_documents_amount = 0\n",
    "        ticker_code = \"%s_%s\" % (ticker, findex)\n",
    "        for report_file_name in os.listdir(dir_ticker):\n",
    "            if report_file_name == \".DS_Store\":\n",
    "                continue\n",
    "            match = re.search(check_report_name_reg_exp, report_file_name)\n",
    "            if not match:\n",
    "                print(\"working on %s, filename %s doesn't fit pattern\" % (ticker_code, report_file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if flag_test_report_names:\n",
    "    for findex in os.listdir(dir_reports_txt):\n",
    "        dir_findex = os.path.join(dir_reports_txt, findex)\n",
    "        if os.path.isdir(dir_findex):\n",
    "            # single thread\n",
    "            for ticker in os.listdir(dir_findex): \n",
    "                test_reports_names(dir_findex, ticker)\n",
    "            \n",
    "            \n",
    "            # multiprocessing\n",
    "            # with multiprocessing.Pool(processes=4) as pool:\n",
    "            #    if flag_test_report_names: \n",
    "            #        pool.starmap(test_reports_names, [(dir_findex, ticker) for ticker in os.listdir(dir_findex)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posseble Reports completeness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to find all tickers of companies which have reports for the experiment's timeframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_tickers_years = os.path.join(dir_data_processing, \"tickers\", \"possible_tickers_years.datajson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate over all reprots and collect years of publishing for every company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = dict()\n",
    "list_of_errors = []\n",
    "for findex in os.listdir(dir_reports_txt):\n",
    "    dir_findex = os.path.join(dir_reports_txt, findex)\n",
    "    if os.path.isdir(dir_findex):\n",
    "        for ticker in os.listdir(dir_findex):\n",
    "            dir_ticker = os.path.join(dir_findex, ticker)\n",
    "            if os.path.isdir(dir_ticker):\n",
    "                ticker_years_set = set()\n",
    "                for report_file_name in os.listdir(dir_ticker):\n",
    "                    if report_file_name == \".DS_Store\":\n",
    "                        continue\n",
    "                    \n",
    "                    match = re.search(check_report_name_reg_exp, report_file_name)\n",
    "                    if not match:\n",
    "                        print(\"%s doesn't fit the pattern\" % report_file_name)\n",
    "                        list_of_errors.append({\"path\": dir_ticker, \"file_name\": report_file_name}) \n",
    "                        continue\n",
    "                        \n",
    "                    ticker_years_set.add(int(match.group(\"p_year\")))\n",
    "                tickers[\"%s_%s\" % (ticker, findex)] = sorted(ticker_years_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renamer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def renamer(to_rename, reg_exp):\n",
    "    renamed = 0\n",
    "    for report in to_rename:    \n",
    "        match = re.search(wrong_order_reg_exp, report[\"file_name\"])\n",
    "        if not match:\n",
    "            print(\"%s doesn't fit the pattern\" % report[\"file_name\"])\n",
    "            continue\n",
    "        new_file_name = \"%s_%s%s_%s_%s-%s-%s.txt\" % (\n",
    "            match[\"ticker\"],\n",
    "            match[\"type\"],\n",
    "            match[\"number\"],\n",
    "            match[\"year\"],\n",
    "            match[\"p_year\"],\n",
    "            match[\"p_month\"],\n",
    "            match['p_date']     \n",
    "        )   \n",
    "\n",
    "        if os.path.exists(new_file_name):\n",
    "            subnumber = (int(match[\"number\"]) + 1) if match[\"number\"] else 0\n",
    "            new_file_name = \"%s_%s%s-%s_%s_%s-%s-%s.txt\" % (\n",
    "                match[\"ticker\"],\n",
    "                match[\"type\"],\n",
    "                match[\"number\"],\n",
    "                subnumber,\n",
    "                match[\"year\"],\n",
    "                match[\"p_year\"],\n",
    "                match[\"p_month\"],\n",
    "                match['p_date']     \n",
    "            )    \n",
    "        os.rename(\n",
    "            os.path.join(report[\"path\"], report[\"file_name\"]),\n",
    "            os.path.join(report[\"path\"], new_file_name)\n",
    "        )\n",
    "        renamed = renamed + 1\n",
    "        \n",
    "    print(\"Renamed %s\" % renamed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix issue with YYYY-MM-DD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if flag_fix_reports_names and len(list_of_errors) > 1:\n",
    "    wrong_order_reg_exp = \"(?P<ticker>[A-Z1-9]+)[_-](?P<type>[A-Z]+)(?P<number>[1-9A-Z]*)(?P<subnumber>-[1-9]+)?[-_](?P<year>[0-9]{4})_(?P<p_date>[0-9X]{2})-(?P<p_month>[0-9X]{2})-(?P<p_year>[0-9]{4})\"\n",
    "    renamer(list_of_errors, wrong_order_reg_exp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if flag_fix_reports_names and len(list_of_errors) > 1:\n",
    "    wrong_order_reg_exp = \"(?P<ticker>[A-Z1-9]+)[_-](?P<type>[A-Z]+)(?P<number>[0-9A-Z]*)(?P<subnumber>-[0-9]+)?[-_](?P<year>[0-9]{4})_(?P<p_year>[0-9]{4})-(?P<p_month>[0-9X]{2})-(?P<p_date>[0-9X]{2})\"\n",
    "    renamer(list_of_errors, wrong_order_reg_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After fix of all the reports *RERUN* Posseble Reports completeness from beginning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Years complmentnes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save years of companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_tickers_years, \"w\") as f_w:\n",
    "    for ticker in tickers:\n",
    "        f_w.write(\"%s\\n\" % json.dumps({\"ticker\": ticker, \"available_years\": tickers[ticker]}))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find tickers with reports for every year in the experiment timeframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_tickers_for_analysis = set()\n",
    "tickers_all = 0\n",
    "for ticker, available_years in tickers.items():\n",
    "    flag_complete_series = True\n",
    "    available_years_set = set(available_years)\n",
    "    for year in year_series:\n",
    "        if year not in available_years_set:\n",
    "            flag_complete_series = False\n",
    "            break\n",
    "    if flag_complete_series and os.path.exists(os.path.join(dir_ticker_prices_source, \"%s.csv\" % ticker)):\n",
    "        possible_tickers_for_analysis.add(ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 80 tickers possible for the experiment\n",
      "AAL_FTSE\n",
      "ACA_CAC\n",
      "AC_CAC\n",
      "ADM_FTSE\n",
      "AGK_FTSE\n",
      "AIR_CAC\n",
      "ALV_DAX\n",
      "ANTO_FTSE\n",
      "BAB_FTSE\n",
      "BARC_FTSE\n",
      "BAYN_DAX\n",
      "BA_FTSE\n",
      "BMW_DAX\n",
      "BNP_CAC\n",
      "CAP_CAC\n",
      "CBK_DAX\n",
      "CSCO_DJIA\n",
      "CS_CAC\n",
      "CVX_DJIA\n",
      "DAI_DAX\n",
      "DBK_DAX\n",
      "DIS_DJIA\n",
      "DPW_DAX\n",
      "DTE_DAX\n",
      "EDF_CAC\n",
      "EI_CAC\n",
      "EOAN_DAX\n",
      "FRE_DAX\n",
      "GS_DJIA\n",
      "HD_DJIA\n",
      "HEN3_DAX\n",
      "HSBC_DJIA\n",
      "IFX_DAX\n",
      "JPM_DJIA\n",
      "KO_DJIA\n",
      "LHA_DAX\n",
      "LIN_DAX\n",
      "LLOY_FTSE\n",
      "LR_CAC\n",
      "MCD_DJIA\n",
      "MC_CAC\n",
      "MKS_FTSE\n",
      "MRO_FTSE\n",
      "MRW_FTSE\n",
      "MUV2_DAX\n",
      "OR_CAC\n",
      "PFC_FTSE\n",
      "PFE_DJIA\n",
      "PG_DJIA\n",
      "PRU_FTSE\n",
      "RBS_FTSE\n",
      "RB_FTSE\n",
      "REL_FTSE\n",
      "RI_CAC\n",
      "RRS_FTSE\n",
      "RSA_FTSE\n",
      "RWE_DAX\n",
      "SAP_DAX\n",
      "SBRY_FTSE\n",
      "SDF_DAX\n",
      "SHP_FTSE\n",
      "SIE_DAX\n",
      "SKY_FTSE\n",
      "SLA_FTSE\n",
      "SNN_FTSE\n",
      "SRP_FTSE\n",
      "SSE_FTSE\n",
      "STAN_FTSE\n",
      "TKA_DAX\n",
      "TSCO_FTSE\n",
      "TVE_DJIA\n",
      "ULVR_FTSE\n",
      "UTX_DJIA\n",
      "VED_FTSE\n",
      "VOD_FTSE\n",
      "VZ_DJIA\n",
      "WEIR_FTSE\n",
      "WMT_DJIA\n",
      "WPP_FTSE\n",
      "WTB_FTSE\n"
     ]
    }
   ],
   "source": [
    "print(\"there are %s tickers possible for the experiment\" % len(possible_tickers_for_analysis))\n",
    "for ticker in sorted(possible_tickers_for_analysis):\n",
    "    print(ticker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reports pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First remove all but English letters and re-save reports as a sequence of lower case words consist only from letters a-z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "regexp_to_remove = re.compile(r\"[\\dâºâãï½ã\\_]\")\n",
    "regexp_to_keep = re.compile(r\"[^a-z\\s]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_raw_text_2_words(file_report_path):\n",
    "    words = []\n",
    "    try:\n",
    "        f_r = open(file_report_path, encoding=\"utf8\", errors='ignore')\n",
    "    except OSError:\n",
    "        print (\"Could not open/read file: %s\" % file_report_path)\n",
    "        return words\n",
    "    except UnicodeDecodeError:\n",
    "        print (\"Unicode not open/read file: %s\" % file_report_path)\n",
    "        return words\n",
    "    except OSError:\n",
    "        print (\"Unknown open/read file: %s\" % file_report_path)\n",
    "        return words\n",
    "        \n",
    "        \n",
    "    for text_line in f_r:\n",
    "        cleaned_text = re.sub(regexp_to_keep, \" \", text_line.lower())\n",
    "        words_in_line = re.split(\"\\W+\", cleaned_text)\n",
    "        for possible_word in words_in_line:\n",
    "            word = possible_word.strip()\n",
    "            if len(word) > 1:\n",
    "                words.append(word)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reports_2_words_processing(dir_findex, ticker, findex, years_set):\n",
    "    ticker_code = \"%s_%s\" % (ticker, findex)\n",
    "    \n",
    "    if ticker_code not in possible_tickers_for_analysis:\n",
    "        print(\"Skip %s\" % ticker_code)\n",
    "        return\n",
    "    \n",
    "    dir_ticker = os.path.join(dir_findex, ticker)\n",
    "    good_documents_amount = 0\n",
    "    empty_documents_amount = 0\n",
    "    bad_years = set()\n",
    "    if os.path.isdir(dir_ticker):\n",
    "        for report_file_name in os.listdir(dir_ticker):\n",
    "            if report_file_name == \".DS_Store\":\n",
    "                continue\n",
    "            match = re.search(check_report_name_reg_exp, report_file_name)\n",
    "            if not match:\n",
    "                print(\"filename %s doesn't fit pattern\" % report_file_name)\n",
    "            else:\n",
    "                year = int(match.group(\"p_year\"))\n",
    "                if year not in years_set:\n",
    "                    bad_years.add(year)\n",
    "                    continue\n",
    "                list_words = convert_raw_text_2_words(os.path.join(dir_ticker, report_file_name))\n",
    "                if len(list_words):\n",
    "                    good_documents_amount += 1\n",
    "                    new_file_name = \"%s_%s.txt\" % (year, good_documents_amount)\n",
    "                    new_path = os.path.join(dir_reports_words, findex, ticker)\n",
    "                    if not os.path.exists(new_path):\n",
    "                        os.makedirs(new_path)\n",
    "                    with open(os.path.join(new_path, new_file_name), \"w\") as f_w:\n",
    "                        f_w.write(\"%s\" % ' '.join(list_words))\n",
    "                else: \n",
    "                    empty_documents_amount += 1\n",
    "                    print(\"report %s is empty after cleaning\" % report_file_name)\n",
    "        print(\"Done on %s, reports: %s, empty: %s, extra years available: [%s]\" % \n",
    "              (ticker_code, good_documents_amount, empty_documents_amount, \", \".join(map(str, sorted(bad_years)))))                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run previously defined function words_2_terms_processing in pool of 4 processes to speedup the cleaning, The following cell takes quite a while, be carefull and do not rerun it without a reason, results are stored at file system\n",
    "\n",
    "**please make sure that flag_rerun_text_2_words is set to True if you want to run/re-run this preprosessing step**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if flag_rerun_text_2_words:\n",
    "    filtering_years_set = set(year_series)\n",
    "    for findex in os.listdir(dir_reports_txt):\n",
    "        dir_findex = os.path.join(dir_reports_txt, findex)\n",
    "        if os.path.isdir(dir_findex):\n",
    "            for ticker in os.listdir(dir_findex):\n",
    "                reports_2_words_processing(dir_findex, ticker, findex, filtering_years_set)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lematization and english words filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that all nltk data sets are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/Alan_Spark/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Alan_Spark/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/Alan_Spark/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create stop words set and white words set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create stop words set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "english_words = set(nltk.corpus.words.words())\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "stop_words_set = set(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extend stop words with custom stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if flag_extend_stopwords:\n",
    "    with open(os.path.join(dir_data_raw, \"english\", \"extra_stopwords.txt\"), \"r\") as f_r:\n",
    "        for text_line in f_r:\n",
    "            term = text_line.strip()\n",
    "            stop_words_set.add(term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a white list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "white_list_set = set()\n",
    "with open(os.path.join(dir_data_raw, \"english\", \"white_stopwords.txt\"), \"r\") as f_r:\n",
    "    for text_line in f_r:\n",
    "        term = text_line.strip()\n",
    "        white_list_set.add(term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a funtion checker for stop words. A word is stop word if any of the folowing true:\n",
    "- it's length shorter then 3 char\n",
    "- it contains digits\n",
    "- it appears in nltk stop words set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_stop_term(term):\n",
    "    if term in white_list_set:\n",
    "        return False\n",
    "    if len(term) < 3:\n",
    "        return True\n",
    "    return term in stop_words_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a funtion lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that en spacy data set is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')#, disable=['parser', 'ner'])\n",
    "#lemmatizer = WordNetLemmatizer().lemmatize\n",
    "#stemmer = SnowballStemmer(\"english\").stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bank bank\n",
      "banking banking\n",
      "go go\n",
      "go going\n"
     ]
    }
   ],
   "source": [
    "tokens = nlp('bank banking go going')\n",
    "for token in tokens:\n",
    "    print(token.lemma_ + ' ' + token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text_line, terms, eliminated_terms):\n",
    "    doc = nlp(text_line)\n",
    "    for token in doc:\n",
    "        term = token.lemma_ if token.lemma_ != \"-PRON-\" else token.text\n",
    "        if is_stop_term(term) or term not in english_words:\n",
    "            if term not in eliminated_terms:\n",
    "                eliminated_terms[term] = 0\n",
    "            eliminated_terms[term] += 1  \n",
    "        else:\n",
    "            terms.append(term)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_words_2_terms(file_report_path, eliminated_terms):\n",
    "    terms = []\n",
    "    chunk_size = 30\n",
    "    with open(os.path.join(file_report_path), \"r\",  encoding=\"utf-8\") as f_r:\n",
    "        for text_line in f_r:\n",
    "            words_in_line = re.split(\"\\W+\", text_line)\n",
    "            size = len(words_in_line)\n",
    "            steps = int(size / chunk_size)\n",
    "            for i in range(steps):\n",
    "                tokenize(\" \".join(words_in_line[i*chunk_size:(i+1)*chunk_size]), terms, eliminated_terms)\n",
    "            tokenize(\" \".join(words_in_line[steps*chunk_size:]), terms, eliminated_terms)    \n",
    "                                 \n",
    "    return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_2_terms_processing(dir_findex, ticker):\n",
    "    start = time.time()\n",
    "    print(\"\\n Start on %s %s\" % (dir_findex, ticker))\n",
    "    \n",
    "    dict_eliminated_terms = {}\n",
    "    dir_ticker = os.path.join(dir_findex, ticker)\n",
    "    good_documents_amount = 0\n",
    "    empty_documents_amount = 0\n",
    "    if os.path.isdir(dir_ticker):\n",
    "        for report_file_name in os.listdir(dir_ticker):\n",
    "            if report_file_name == \".DS_Store\":\n",
    "                continue\n",
    "            terms_list = convert_words_2_terms(os.path.join(dir_ticker, report_file_name), dict_eliminated_terms)\n",
    "            if len(terms_list):   \n",
    "                good_documents_amount += 1\n",
    "                new_path = os.path.join(dir_reports_terms, findex, ticker)\n",
    "                if not os.path.exists(new_path):\n",
    "                    os.makedirs(new_path)\n",
    "                with open(os.path.join(new_path, report_file_name), \"w\") as f_w:\n",
    "                    f_w.write(\"%s\" % ' '.join(terms_list))\n",
    "            else: \n",
    "                empty_documents_amount += 1\n",
    "                if flag_debug:\n",
    "                    print(\"report %s is empty after cleaning\" % report_file_name)\n",
    "        \n",
    "        if len(dict_eliminated_terms):\n",
    "            if not os.path.exists(dir_terms_eliminated):\n",
    "                os.makedirs(dir_terms_eliminated)\n",
    "            with open(os.path.join(dir_terms_eliminated, \"%s_%s.json\" % (ticker, findex)), \"w\") as f_w:\n",
    "                json.dump(dict_eliminated_terms, f_w)\n",
    "        \n",
    "        end = time.time()\n",
    "        print(\"\\n Done in %s minutes on %s, good reports: %s, empty reports: %s\" % \n",
    "              (\"{:0.2f}\".format((end - start) / 60), dir_ticker, good_documents_amount, empty_documents_amount))\n",
    "    else:\n",
    "        print(\"\\n Skip non folder %s %s\" % (dir_findex, ticker))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run previously defined function words_2_terms_processing in pool of 4 processes to speedup the cleaning, The following cell takes quite a while, be carefull and do not rerun it without a reason, results are stored at file system\n",
    "\n",
    "**please make sure that flag_rerun_words_2_terms is set to True if you want to run/re-run this preprosessing step**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if flag_rerun_words_2_terms:\n",
    "    for findex in os.listdir(dir_reports_words):\n",
    "        dir_findex = os.path.join(dir_reports_words, findex)\n",
    "        if os.path.isdir(dir_findex):\n",
    "            #for ticker in os.listdir(dir_findex):\n",
    "            #    words_2_terms_processing(dir_findex, ticker)\n",
    "                \n",
    "            with multiprocessing.Pool(processes=4) as pool:\n",
    "                pool.starmap(words_2_terms_processing, [(dir_findex, ticker) for ticker in os.listdir(dir_findex)])\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize the eliminated words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set flag_terms_filter_debug to True to print all the eliminated words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list of eliminated terms, size(201811)\n"
     ]
    }
   ],
   "source": [
    "eliminated_words = {}\n",
    "for file in os.listdir(dir_terms_eliminated):\n",
    "    if file == '.DS_Store' or file == \"all_elimintated_words\":\n",
    "        continue\n",
    "    with open(os.path.join(dir_terms_eliminated, file), \"r\") as f_r:\n",
    "        el = json.load(f_r)\n",
    "        for word, count in el.items():\n",
    "            if word not in eliminated_words:\n",
    "                eliminated_words[word] = {\"count\": 0, \"ticker\": 0}\n",
    "            eliminated_words[word][\"count\"] += count\n",
    "            eliminated_words[word][\"ticker\"] += 1\n",
    "        \n",
    "print(\"list of eliminated terms, size(%s)\" % len(eliminated_words))\n",
    "\n",
    "with open(os.path.join(dir_terms_eliminated, \"all_elimintated_words\"), \"w\") as f_w:\n",
    "    json.dump(eliminated_words, f_w)\n",
    "\n",
    "if flag_terms_filter_debug:\n",
    "    for word in sorted(eliminated_words.keys()):\n",
    "        print(\"%s, usage - total: %s, tickers %s\" % (word, eliminated_words[word][\"count\"], eliminated_words[word][\"ticker\"]))\n",
    "              \n",
    "              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Condence bigramms and trigarams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read all documents as data: list of list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_terms = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_terms(dir_findex, ticker, data):\n",
    "    dir_ticker = os.path.join(dir_findex, ticker)\n",
    "    if os.path.isdir(dir_ticker):\n",
    "        for report_file_name in os.listdir(dir_ticker):\n",
    "            file_path = os.path.join(dir_ticker, report_file_name)\n",
    "            if not os.path.isfile(file_path):\n",
    "                continue\n",
    "            terms_list = []\n",
    "            with open(file_path, 'r') as f_r:\n",
    "                for text_line in f_r:\n",
    "                    terms_list = terms_list + text_line.strip().split(\" \")\n",
    "            data.append(terms_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EDF_CAC data collected\n",
      "AC_CAC data collected\n",
      "CS_CAC data collected\n",
      "ACA_CAC data collected\n",
      "RI_CAC data collected\n",
      "AIR_CAC data collected\n",
      "CAP_CAC data collected\n",
      "MC_CAC data collected\n",
      "BNP_CAC data collected\n",
      "LR_CAC data collected\n",
      "EI_CAC data collected\n",
      "OR_CAC data collected\n",
      "AAL_FTSE data collected\n",
      "WEIR_FTSE data collected\n",
      "BARC_FTSE data collected\n",
      "MKS_FTSE data collected\n",
      "WPP_FTSE data collected\n",
      "PFC_FTSE data collected\n",
      "SSE_FTSE data collected\n",
      "SKY_FTSE data collected\n",
      ".DS_Store_FTSE data collected\n",
      "ANTO_FTSE data collected\n",
      "SHP_FTSE data collected\n",
      "PRU_FTSE data collected\n",
      "SNN_FTSE data collected\n",
      "BAB_FTSE data collected\n",
      "RSA_FTSE data collected\n",
      "MRW_FTSE data collected\n",
      "WTB_FTSE data collected\n",
      "RBS_FTSE data collected\n",
      "STAN_FTSE data collected\n",
      "VOD_FTSE data collected\n",
      "REL_FTSE data collected\n",
      "AGK_FTSE data collected\n",
      "VED_FTSE data collected\n",
      "ADM_FTSE data collected\n",
      "BA_FTSE data collected\n",
      "RRS_FTSE data collected\n",
      "LLOY_FTSE data collected\n",
      "SLA_FTSE data collected\n",
      "TSCO_FTSE data collected\n",
      "MRO_FTSE data collected\n",
      "ULVR_FTSE data collected\n",
      "SRP_FTSE data collected\n",
      "SBRY_FTSE data collected\n",
      "RB_FTSE data collected\n",
      "VZ_DJIA data collected\n",
      "PFE_DJIA data collected\n",
      "WMT_DJIA data collected\n",
      ".DS_Store_DJIA data collected\n",
      "CVX_DJIA data collected\n",
      "GS_DJIA data collected\n",
      "HD_DJIA data collected\n",
      "MCD_DJIA data collected\n",
      "PG_DJIA data collected\n",
      "HSBC_DJIA data collected\n",
      "DIS_DJIA data collected\n",
      "CSCO_DJIA data collected\n",
      "KO_DJIA data collected\n",
      "TVE_DJIA data collected\n",
      "UTX_DJIA data collected\n",
      "JPM_DJIA data collected\n",
      "HEN3_DAX data collected\n",
      "SAP_DAX data collected\n",
      "SDF_DAX data collected\n",
      "LIN_DAX data collected\n",
      "DAI_DAX data collected\n",
      "BAYN_DAX data collected\n",
      "ALV_DAX data collected\n",
      "DTE_DAX data collected\n",
      "CBK_DAX data collected\n",
      "FRE_DAX data collected\n",
      "MUV2_DAX data collected\n",
      "RWE_DAX data collected\n",
      "EOAN_DAX data collected\n",
      "DBK_DAX data collected\n",
      "SIE_DAX data collected\n",
      "LHA_DAX data collected\n",
      "BMW_DAX data collected\n",
      "TKA_DAX data collected\n",
      "DPW_DAX data collected\n",
      "IFX_DAX data collected\n"
     ]
    }
   ],
   "source": [
    "if flag_rerun_terms_2_gramms:\n",
    "    for findex in os.listdir(dir_reports_terms):\n",
    "        dir_findex = os.path.join(dir_reports_terms, findex)\n",
    "        if os.path.isdir(dir_findex):\n",
    "            for ticker in os.listdir(dir_findex):\n",
    "                ticker_code = \"%s_%s\" % (ticker, findex)\n",
    "                read_data_terms(dir_findex, ticker, data_terms) \n",
    "                print(\"%s data collected\" % ticker_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = gensim.models.Phrases(data_terms, min_count=10, threshold=80) # higher threshold fewer phrases.\n",
    "#trigram = gensim.models.Phrases(bigram[data_terms], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "#trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-read all docs and concatenate bi/trigramms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_2_gramms_processing(dir_findex, ticker):\n",
    "    dir_ticker = os.path.join(dir_findex, ticker)\n",
    "    if os.path.isdir(dir_ticker):\n",
    "        for report_file_name in os.listdir(dir_ticker):\n",
    "            file_path = os.path.join(dir_ticker, report_file_name)\n",
    "            if not os.path.isfile(file_path):\n",
    "                continue\n",
    "                \n",
    "            terms_list = []\n",
    "            with open(file_path, 'r') as f_r:\n",
    "                for text_line in f_r:\n",
    "                    terms_list = terms_list + text_line.strip().split(\" \")\n",
    "            terms_list = bigram_mod[terms_list]\n",
    "            #terms_list = trigram_mod[bigram_mod[terms_list]]\n",
    "            \n",
    "            if len(terms_list):   \n",
    "                new_path = os.path.join(dir_reports_grams, findex, ticker)\n",
    "                if not os.path.exists(new_path):\n",
    "                    os.makedirs(new_path)\n",
    "                with open(os.path.join(new_path, report_file_name), \"w\") as f_w:\n",
    "                    f_w.write(\"%s\" % ' '.join(terms_list))\n",
    "            else: \n",
    "                empty_documents_amount += 1\n",
    "                if flag_debug:\n",
    "                    print(\"report %s is empty after cleaning\" % report_file_name)\n",
    "        print(\"Done on %s\" % dir_ticker)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run previously defined function words_2_terms_processing in pool of 4 processes to speedup the cleaning, The following cell takes quite a while, be carefull and do not rerun it without a reason, results are stored at file system\n",
    "\n",
    "**please make sure that flag_rerun_terms_2_gramms is set to True if you want to run/re-run this preprosessing step**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done on ../data_processing/reports_terms/CAC/AC\n",
      "Done on ../data_processing/reports_terms/CAC/EDF\n",
      "Done on ../data_processing/reports_terms/CAC/RI\n",
      "Done on ../data_processing/reports_terms/CAC/AIR\n",
      "Done on ../data_processing/reports_terms/CAC/ACA\n",
      "Done on ../data_processing/reports_terms/CAC/CAP\n",
      "Done on ../data_processing/reports_terms/CAC/MC\n",
      "Done on ../data_processing/reports_terms/CAC/BNP\n",
      "Done on ../data_processing/reports_terms/CAC/CS\n",
      "Done on ../data_processing/reports_terms/CAC/EI\n",
      "Done on ../data_processing/reports_terms/CAC/LR\n",
      "Done on ../data_processing/reports_terms/CAC/OR\n",
      "Done on ../data_processing/reports_terms/FTSE/MKS\n",
      "Done on ../data_processing/reports_terms/FTSE/ANTO\n",
      "Done on ../data_processing/reports_terms/FTSE/AAL\n",
      "Done on ../data_processing/reports_terms/FTSE/SSE\n",
      "Done on ../data_processing/reports_terms/FTSE/WPP\n",
      "Done on ../data_processing/reports_terms/FTSE/WEIR\n",
      "Done on ../data_processing/reports_terms/FTSE/SKY\n",
      "Done on ../data_processing/reports_terms/FTSE/SHP\n",
      "Done on ../data_processing/reports_terms/FTSE/PFC\n",
      "Done on ../data_processing/reports_terms/FTSE/SNN\n",
      "Done on ../data_processing/reports_terms/FTSE/MRW\n",
      "Done on ../data_processing/reports_terms/FTSE/BAB\n",
      "Done on ../data_processing/reports_terms/FTSE/WTB\n",
      "Done on ../data_processing/reports_terms/FTSE/PRU\n",
      "Done on ../data_processing/reports_terms/FTSE/RSA\n",
      "Done on ../data_processing/reports_terms/FTSE/RBS\n",
      "Done on ../data_processing/reports_terms/FTSE/AGK\n",
      "Done on ../data_processing/reports_terms/FTSE/BARC\n",
      "Done on ../data_processing/reports_terms/FTSE/STAN\n",
      "Done on ../data_processing/reports_terms/FTSE/BA\n",
      "Done on ../data_processing/reports_terms/FTSE/SLA\n",
      "Done on ../data_processing/reports_terms/FTSE/VED\n",
      "Done on ../data_processing/reports_terms/FTSE/ADM\n",
      "Done on ../data_processing/reports_terms/FTSE/TSCO\n",
      "Done on ../data_processing/reports_terms/FTSE/RRS\n",
      "Done on ../data_processing/reports_terms/FTSE/MRO\n",
      "Done on ../data_processing/reports_terms/FTSE/ULVR\n",
      "Done on ../data_processing/reports_terms/FTSE/RB\n",
      "Done on ../data_processing/reports_terms/FTSE/VOD\n",
      "Done on ../data_processing/reports_terms/FTSE/SRP\n",
      "Done on ../data_processing/reports_terms/FTSE/REL\n",
      "Done on ../data_processing/reports_terms/FTSE/LLOY\n",
      "Done on ../data_processing/reports_terms/FTSE/SBRY\n",
      "Done on ../data_processing/reports_terms/DJIA/CVX\n",
      "Done on ../data_processing/reports_terms/DJIA/WMT\n",
      "Done on ../data_processing/reports_terms/DJIA/VZ\n",
      "Done on ../data_processing/reports_terms/DJIA/HD\n",
      "Done on ../data_processing/reports_terms/DJIA/MCD\n",
      "Done on ../data_processing/reports_terms/DJIA/PFE\n",
      "Done on ../data_processing/reports_terms/DJIA/GS\n",
      "Done on ../data_processing/reports_terms/DJIA/PG\n",
      "Done on ../data_processing/reports_terms/DJIA/CSCO\n",
      "Done on ../data_processing/reports_terms/DJIA/DIS\n",
      "Done on ../data_processing/reports_terms/DJIA/KO\n",
      "Done on ../data_processing/reports_terms/DJIA/UTX\n",
      "Done on ../data_processing/reports_terms/DJIA/TVE\n",
      "Done on ../data_processing/reports_terms/DJIA/JPM\n",
      "Done on ../data_processing/reports_terms/DJIA/HSBC\n",
      "Done on ../data_processing/reports_terms/DAX/SDF\n",
      "Done on ../data_processing/reports_terms/DAX/HEN3\n",
      "Done on ../data_processing/reports_terms/DAX/ALV\n",
      "Done on ../data_processing/reports_terms/DAX/LIN\n",
      "Done on ../data_processing/reports_terms/DAX/DAI\n",
      "Done on ../data_processing/reports_terms/DAX/SAP\n",
      "Done on ../data_processing/reports_terms/DAX/BAYN\n",
      "Done on ../data_processing/reports_terms/DAX/DTE\n",
      "Done on ../data_processing/reports_terms/DAX/CBK\n",
      "Done on ../data_processing/reports_terms/DAX/MUV2\n",
      "Done on ../data_processing/reports_terms/DAX/EOAN\n",
      "Done on ../data_processing/reports_terms/DAX/SIE\n",
      "Done on ../data_processing/reports_terms/DAX/RWE\n",
      "Done on ../data_processing/reports_terms/DAX/FRE\n",
      "Done on ../data_processing/reports_terms/DAX/LHA\n",
      "Done on ../data_processing/reports_terms/DAX/BMW\n",
      "Done on ../data_processing/reports_terms/DAX/DPW\n",
      "Done on ../data_processing/reports_terms/DAX/TKA\n",
      "Done on ../data_processing/reports_terms/DAX/IFX\n",
      "Done on ../data_processing/reports_terms/DAX/DBK\n"
     ]
    }
   ],
   "source": [
    "if flag_rerun_terms_2_gramms:\n",
    "    if not os.path.exists(dir_reports_grams):\n",
    "        os.makedirs(dir_reports_grams)\n",
    "        \n",
    "    for findex in os.listdir(dir_reports_terms):\n",
    "        dir_findex = os.path.join(dir_reports_terms, findex)\n",
    "        if os.path.isdir(dir_findex):\n",
    "            with multiprocessing.Pool(processes=4) as pool:\n",
    "                pool.starmap(term_2_gramms_processing, [(\n",
    "                    dir_findex,\n",
    "                    ticker) for ticker in os.listdir(dir_findex)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get corpus statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ticker_reports_term_counting(dir_findex, ticker, dict_terms_counts):\n",
    "    number_of_repors = 0\n",
    "    dir_ticker = os.path.join(dir_findex, ticker)\n",
    "    terms_visited_ticker = set()\n",
    "    if os.path.isdir(dir_ticker):\n",
    "        ticker_documents_amount = 0\n",
    "        for report_file_name in os.listdir(dir_ticker):\n",
    "            file_path = os.path.join(dir_ticker, report_file_name)\n",
    "            if os.path.isfile(file_path):\n",
    "                terms_visited_document = set()\n",
    "                number_of_repors += 1\n",
    "                with open(file_path, 'r') as f_r:\n",
    "                    for text in f_r:\n",
    "                        for term in text.strip().split(' '):\n",
    "                            if term not in dict_terms_counts:\n",
    "                                dict_terms_counts[term] = {\n",
    "                                    \"count\": 1, \n",
    "                                    \"document\": 1, \n",
    "                                    \"tickers\": 1}\n",
    "                            else:\n",
    "                                if term not in terms_visited_ticker:\n",
    "                                    dict_terms_counts[term][\"tickers\"] += 1\n",
    "                                if term not in terms_visited_document:\n",
    "                                    dict_terms_counts[term][\"document\"] += 1\n",
    "                                dict_terms_counts[term][\"count\"] += 1\n",
    "                            terms_visited_ticker.add(term)\n",
    "                            terms_visited_document.add(term)\n",
    "    return number_of_repors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EDF_CAC terms stats collected\n",
      "AC_CAC terms stats collected\n",
      "CS_CAC terms stats collected\n",
      "ACA_CAC terms stats collected\n",
      "RI_CAC terms stats collected\n",
      "AIR_CAC terms stats collected\n",
      "CAP_CAC terms stats collected\n",
      "MC_CAC terms stats collected\n",
      "BNP_CAC terms stats collected\n",
      "LR_CAC terms stats collected\n",
      "EI_CAC terms stats collected\n",
      "OR_CAC terms stats collected\n",
      "AAL_FTSE terms stats collected\n",
      "WEIR_FTSE terms stats collected\n",
      "BARC_FTSE terms stats collected\n",
      "MKS_FTSE terms stats collected\n",
      "WPP_FTSE terms stats collected\n",
      "PFC_FTSE terms stats collected\n",
      "SSE_FTSE terms stats collected\n",
      "SKY_FTSE terms stats collected\n",
      "ANTO_FTSE terms stats collected\n",
      "SHP_FTSE terms stats collected\n",
      "PRU_FTSE terms stats collected\n",
      "SNN_FTSE terms stats collected\n",
      "BAB_FTSE terms stats collected\n",
      "RSA_FTSE terms stats collected\n",
      "MRW_FTSE terms stats collected\n",
      "WTB_FTSE terms stats collected\n",
      "RBS_FTSE terms stats collected\n",
      "STAN_FTSE terms stats collected\n",
      "VOD_FTSE terms stats collected\n",
      "REL_FTSE terms stats collected\n",
      "AGK_FTSE terms stats collected\n",
      "VED_FTSE terms stats collected\n",
      "ADM_FTSE terms stats collected\n",
      "BA_FTSE terms stats collected\n",
      "RRS_FTSE terms stats collected\n",
      "LLOY_FTSE terms stats collected\n",
      "SLA_FTSE terms stats collected\n",
      "TSCO_FTSE terms stats collected\n",
      "MRO_FTSE terms stats collected\n",
      "ULVR_FTSE terms stats collected\n",
      "SRP_FTSE terms stats collected\n",
      "SBRY_FTSE terms stats collected\n",
      "RB_FTSE terms stats collected\n",
      "VZ_DJIA terms stats collected\n",
      "PFE_DJIA terms stats collected\n",
      "WMT_DJIA terms stats collected\n",
      "CVX_DJIA terms stats collected\n",
      "GS_DJIA terms stats collected\n",
      "HD_DJIA terms stats collected\n",
      "MCD_DJIA terms stats collected\n",
      "PG_DJIA terms stats collected\n",
      "HSBC_DJIA terms stats collected\n",
      "DIS_DJIA terms stats collected\n",
      "CSCO_DJIA terms stats collected\n",
      "KO_DJIA terms stats collected\n",
      "TVE_DJIA terms stats collected\n",
      "UTX_DJIA terms stats collected\n",
      "JPM_DJIA terms stats collected\n",
      "HEN3_DAX terms stats collected\n",
      "SAP_DAX terms stats collected\n",
      "SDF_DAX terms stats collected\n",
      "LIN_DAX terms stats collected\n",
      "DAI_DAX terms stats collected\n",
      "BAYN_DAX terms stats collected\n",
      "ALV_DAX terms stats collected\n",
      "DTE_DAX terms stats collected\n",
      "CBK_DAX terms stats collected\n",
      "FRE_DAX terms stats collected\n",
      "MUV2_DAX terms stats collected\n",
      "RWE_DAX terms stats collected\n",
      "EOAN_DAX terms stats collected\n",
      "DBK_DAX terms stats collected\n",
      "SIE_DAX terms stats collected\n",
      "LHA_DAX terms stats collected\n",
      "BMW_DAX terms stats collected\n",
      "TKA_DAX terms stats collected\n",
      "DPW_DAX terms stats collected\n",
      "IFX_DAX terms stats collected\n"
     ]
    }
   ],
   "source": [
    "terms_counts = {} #key: {count: int, document: int, tickers: int}\n",
    "number_of_documents = 0\n",
    "number_of_tickers = 0\n",
    "\n",
    "for findex in os.listdir(dir_reports_terms):\n",
    "    dir_findex = os.path.join(dir_reports_terms, findex)\n",
    "    if os.path.isdir(dir_findex):\n",
    "        for ticker in os.listdir(dir_findex):\n",
    "            if os.path.isdir(os.path.join(dir_findex,ticker)):\n",
    "                number_of_tickers += 1\n",
    "                ticker_code = \"%s_%s\" % (ticker, findex)\n",
    "                number_of_documents += ticker_reports_term_counting(dir_findex, ticker, terms_counts) \n",
    "                print(\"%s terms stats collected\" % ticker_code)\n",
    "\n",
    "\n",
    "if not os.path.exists(dir_terms_counts):\n",
    "    os.makedirs(dir_terms_counts)            \n",
    "with open(os.path.join(dir_terms_counts, 'terms.json'), 'w') as f_w:\n",
    "    json.dump(terms_counts, f_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There're 29430 unique terms for topic analysis\n"
     ]
    }
   ],
   "source": [
    "print(\"There're %s unique terms for topic analysis\" % len(terms_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There're 5617 reports for topic analysis\n"
     ]
    }
   ],
   "source": [
    "print(\"There're %s reports for topic analysis\" % number_of_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There're 80 tickers for topic analysis\n"
     ]
    }
   ],
   "source": [
    "print(\"There're %s tickers for topic analysis\" % number_of_tickers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtered term set by document frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set terms limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_min = 1 # min_number_of_doc\n",
    "max_partition_of_doc = 1\n",
    "l1_max = int(number_of_documents * max_partition_of_doc) #max_number_of_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1_min: 1\n",
      "l1_max: 5617\n"
     ]
    }
   ],
   "source": [
    "print(\"l1_min: %s\" % l1_min)\n",
    "print(\"l1_max: %s\" % l1_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_min = 1 # min_number_of_tickers\n",
    "max_partition_of_tickers = 1\n",
    "l2_max = int(number_of_tickers * max_partition_of_tickers) #max_number_of_tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l2_min: 1\n",
      "l2_min: 80\n"
     ]
    }
   ],
   "source": [
    "print(\"l2_min: %s\" % l2_min)\n",
    "print(\"l2_min: %s\" % l2_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build eliminated terms set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eliminated by l1_min: 0\n",
      "eliminated by l1_max: 0\n",
      "eliminated by l2_min: 0\n",
      "eliminated by l2_max: 0\n",
      "eliminated 0\n",
      "remain: 29430\n"
     ]
    }
   ],
   "source": [
    "set_remain = set()\n",
    "set_eliminated_by_l1_min = set()\n",
    "set_eliminated_by_l1_max = set()\n",
    "set_eliminated_by_l2_min = set()\n",
    "set_eliminated_by_l2_max = set()\n",
    "#terms_counts = {} #key: {count: int, document: int, tickers: int}\n",
    "\n",
    "for term, stats in terms_counts.items():\n",
    "    df = int(stats[\"document\"] )\n",
    "    tf = int(stats[\"tickers\"] ) \n",
    "    \n",
    "    if df < l1_min:\n",
    "        set_eliminated_by_l1_min.add(term)\n",
    "    elif df > l1_max:\n",
    "        set_eliminated_by_l1_max.add(term)\n",
    "    elif tf < l2_min:\n",
    "        set_eliminated_by_l2_min.add(term)\n",
    "    elif tf > l2_max:\n",
    "        set_eliminated_by_l2_max.add(term)\n",
    "    else:\n",
    "        set_remain.add(term)\n",
    "\n",
    "print(\"eliminated by l1_min: %s\" % len(set_eliminated_by_l1_min))\n",
    "print(\"eliminated by l1_max: %s\" % len(set_eliminated_by_l1_max))\n",
    "print(\"eliminated by l2_min: %s\" % len(set_eliminated_by_l2_min))\n",
    "print(\"eliminated by l2_max: %s\" % len(set_eliminated_by_l2_max))\n",
    "print(\"eliminated %s\" % (\n",
    "    len(set_eliminated_by_l1_min) +\n",
    "    len(set_eliminated_by_l2_min) +\n",
    "    len(set_eliminated_by_l1_max) + \n",
    "    len(set_eliminated_by_l2_max)\n",
    "    )\n",
    ")\n",
    "print(\"remain: %s\" % len(set_remain))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build predefined terms set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_to_extend_prefix = 'run_19_22'\n",
    "dir_run_to_extend = os.path.join(dir_data_runs, run_to_extend_prefix)\n",
    "file_terms = os.path.join(dir_run_to_extend, '%s-terms.dat' % run_to_extend_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of fixed dictionary: 21719 \n"
     ]
    }
   ],
   "source": [
    "set_predifined_terms = set()\n",
    "\n",
    "with open(file_terms, 'r') as f_r:\n",
    "    for text_line in f_r:\n",
    "        term = text_line.strip()\n",
    "        set_predifined_terms.add(term)\n",
    "\n",
    "print('Size of fixed dictionary: %s ' % len(set_predifined_terms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set flag_terms_filter_debug to True to print eliminated words sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "if flag_terms_filter_debug:\n",
    "    for term in sorted(set_eliminated_by_l1_min):\n",
    "        print(term)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "if flag_terms_filter_debug:    \n",
    "    for term in sorted(set_eliminated_by_l1_max):\n",
    "        print(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "if flag_terms_filter_debug:\n",
    "    for term in sorted(set_eliminated_by_l2_min):\n",
    "        print(term)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "if flag_terms_filter_debug:   \n",
    "    for term in sorted(set_eliminated_by_l2_max):\n",
    "        print(term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_fn(term):\n",
    "    if flag_filter_by_fixed_terms:\n",
    "        return (True if term in set_predifined_terms else False)\n",
    "\n",
    "    if (\n",
    "        term in set_eliminated_by_l1_min or \n",
    "        term in set_eliminated_by_l1_max or \n",
    "        term in set_eliminated_by_l2_min or \n",
    "        term in set_eliminated_by_l2_max):\n",
    "        return False\n",
    "    \n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "def filter_terms(file_report_path):\n",
    "    result = []\n",
    "    with open(file_report_path, 'r',  encoding='utf-8') as f_r:\n",
    "        for text_line in f_r:\n",
    "            terms = re.split('\\W+', text_line.strip())\n",
    "            for term in terms:\n",
    "                if filter_fn(term):\n",
    "                    result.append(term)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def terms_filtering(dir_findex, ticker, findex):\n",
    "    dict_eliminated_terms = {}\n",
    "    dir_ticker = os.path.join(dir_findex, ticker)\n",
    "    good_documents_amount = 0\n",
    "    empty_documents_amount = 0\n",
    "    ticker_code = '%s_%s' % (ticker, findex)\n",
    "    if os.path.isdir(dir_ticker):\n",
    "        for report_file_name in os.listdir(dir_ticker):\n",
    "            full_report_name = os.path.join(dir_ticker, report_file_name)\n",
    "            if os.path.isfile(full_report_name) and report_file_name != '.DS_Store':\n",
    "                terms_list = filter_terms(full_report_name)\n",
    "                if len(terms_list):\n",
    "                    good_documents_amount += 1\n",
    "                    with open(\n",
    "                        os.path.join(dir_reports_ready, '%s-%s' % (ticker_code, report_file_name)),\n",
    "                        'w') as f_w:\n",
    "                        f_w.write('%s' % ' '.join(terms_list))\n",
    "                else: \n",
    "                    empty_documents_amount += 1\n",
    "                    if flag_debug:\n",
    "                        print('report %s is empty after cleaning' % report_file_name)\n",
    "        print('Done on %s, good reports: %s, empty reports: %s' % \n",
    "              (dir_ticker, good_documents_amount, empty_documents_amount))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run previously defined function terms_filtering in pool of 4 processes to speedup the cleaning, The following cell doesn't take much time, feel free to experiment with l_min and l_max\n",
    "\n",
    "**please make sure that flag_rerun_filter_terms is set to True if you want to run/re-run this preprosessing step**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done on ../data_processing/reports_gramms/CAC/AC, good reports: 40, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/CAC/EDF, good reports: 78, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/CAC/RI, good reports: 58, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/CAC/ACA, good reports: 68, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/CAC/AIR, good reports: 78, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/CAC/CS, good reports: 66, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/CAC/CAP, good reports: 57, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/CAC/BNP, good reports: 45, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/CAC/MC, good reports: 118, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/CAC/LR, good reports: 93, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/CAC/EI, good reports: 92, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/CAC/OR, good reports: 61, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/FTSE/MKS, good reports: 45, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/FTSE/SSE, good reports: 43, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/FTSE/AAL, good reports: 45, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/FTSE/SHP, good reports: 68, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/FTSE/WPP, good reports: 67, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/FTSE/SKY, good reports: 41, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/FTSE/WEIR, good reports: 54, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/FTSE/PFC, good reports: 34, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/FTSE/PRU, good reports: 57, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/FTSE/ANTO, good reports: 114, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/FTSE/BAB, good reports: 37, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/FTSE/SNN, good reports: 68, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/FTSE/WTB, good reports: 49, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/FTSE/BARC, good reports: 66, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/FTSE/RSA, good reports: 68, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/FTSE/RBS, good reports: 58, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/FTSE/VED, good reports: 58, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/FTSE/MRW, good reports: 72, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/FTSE/ADM, good reports: 33, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/FTSE/VOD, good reports: 73, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/FTSE/STAN, good reports: 53, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/FTSE/BA, good reports: 57, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/FTSE/REL, good reports: 50, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/FTSE/RRS, good reports: 78, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/FTSE/TSCO, good reports: 54, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/FTSE/AGK, good reports: 44, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/FTSE/SRP, good reports: 41, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/FTSE/MRO, good reports: 30, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/FTSE/LLOY, good reports: 50, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/FTSE/SBRY, good reports: 77, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/FTSE/ULVR, good reports: 60, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/FTSE/RB, good reports: 27, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/FTSE/SLA, good reports: 40, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/DJIA/WMT, good reports: 66, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/DJIA/CVX, good reports: 89, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/DJIA/VZ, good reports: 75, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/DJIA/MCD, good reports: 52, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/DJIA/PFE, good reports: 117, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/DJIA/HD, good reports: 91, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/DJIA/GS, good reports: 77, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/DJIA/PG, good reports: 86, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/DJIA/CSCO, good reports: 73, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/DJIA/DIS, good reports: 186, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/DJIA/KO, good reports: 81, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/DJIA/TVE, good reports: 35, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/DJIA/HSBC, good reports: 87, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/DJIA/UTX, good reports: 59, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/DJIA/JPM, good reports: 88, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/DAX/SDF, good reports: 69, empty reports: 2\n",
      "Done on ../data_processing/reports_gramms/DAX/HEN3, good reports: 87, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/DAX/ALV, good reports: 100, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/DAX/DAI, good reports: 102, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/DAX/LIN, good reports: 84, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/DAX/SAP, good reports: 80, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/DAX/BAYN, good reports: 69, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/DAX/DTE, good reports: 63, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/DAX/MUV2, good reports: 78, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/DAX/CBK, good reports: 82, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/DAX/EOAN, good reports: 64, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/DAX/SIE, good reports: 102, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/DAX/RWE, good reports: 75, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/DAX/FRE, good reports: 87, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/DAX/LHA, good reports: 74, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/DAX/BMW, good reports: 80, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/DAX/DPW, good reports: 69, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/DAX/DBK, good reports: 137, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/DAX/TKA, good reports: 87, empty reports: 0\n",
      "Done on ../data_processing/reports_gramms/DAX/IFX, good reports: 99, empty reports: 0\n"
     ]
    }
   ],
   "source": [
    "if flag_rerun_filter_terms:\n",
    "    dir_source = dir_reports_grams if flag_filtering_with_bigramms else dir_reports_terms\n",
    "    \n",
    "    if not os.path.exists(dir_reports_ready):\n",
    "        os.makedirs(dir_reports_ready)\n",
    "        \n",
    "    for findex in os.listdir(dir_source):\n",
    "        dir_findex = os.path.join(dir_source, findex)\n",
    "        if os.path.isdir(dir_findex):\n",
    "            with multiprocessing.Pool(processes=4) as pool:\n",
    "                pool.starmap(terms_filtering, [(\n",
    "                    dir_findex,\n",
    "                    ticker,\n",
    "                    findex) for ticker in os.listdir(dir_findex)])\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get tickers for analisys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reports completeness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to find all tickers of companies which have reports for the experiment's timeframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_tickers_years = os.path.join(dir_data_processing, \"tickers\", \"tickers_years.datajson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate over all reprots and collect years of publishing for every company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_tickers = dict()\n",
    "for report_file_name in os.listdir(dir_reports_ready):\n",
    "    if report_file_name == \".DS_Store\":\n",
    "        continue\n",
    "    ticker, date = report_file_name.split('-')  \n",
    "    if ticker not in temp_tickers:\n",
    "        temp_tickers[ticker] = set()\n",
    "    year, leftovers = date.split('_')    \n",
    "    temp_tickers[ticker].add(int(year))\n",
    "    \n",
    "tickers = dict()\n",
    "for ticker, years in temp_tickers.items():\n",
    "    tickers[ticker] = sorted(years)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save years of companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_tickers_years, \"w\") as f_w:\n",
    "    for ticker in tickers:\n",
    "        f_w.write(\"%s\\n\" % json.dumps({\"ticker\": ticker, \"available_years\": tickers[ticker]}))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find tickers with reports for every year in the experiment timeframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_data_tickers = os.path.join(dir_data_processing, \"tickers\")\n",
    "file_tickers_for_analysis = os.path.join(dir_data_tickers, \"ticker_for_analysis.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_fits_for_analysis = set()\n",
    "tickers_all = 0\n",
    "for ticker, available_years in tickers.items():\n",
    "    flag_complete_series = True\n",
    "    available_years_set = set(available_years)\n",
    "    for year in year_series:\n",
    "        if year not in available_years_set:\n",
    "            flag_complete_series = False\n",
    "            print(\"Missing year: %s for %s\" % (year, ticker))\n",
    "            break\n",
    "    if flag_complete_series and os.path.exists(os.path.join(dir_ticker_prices_source, \"%s.csv\" % ticker)):\n",
    "        tickers_fits_for_analysis.add(ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 80 tickers available for the experiment\n",
      "MKS_FTSE\n",
      "PFC_FTSE\n",
      "MRW_FTSE\n",
      "GS_DJIA\n",
      "OR_CAC\n",
      "CSCO_DJIA\n",
      "SHP_FTSE\n",
      "REL_FTSE\n",
      "SAP_DAX\n",
      "VOD_FTSE\n",
      "MUV2_DAX\n",
      "VZ_DJIA\n",
      "CVX_DJIA\n",
      "SLA_FTSE\n",
      "AGK_FTSE\n",
      "RBS_FTSE\n",
      "RWE_DAX\n",
      "HSBC_DJIA\n",
      "LLOY_FTSE\n",
      "WEIR_FTSE\n",
      "TSCO_FTSE\n",
      "DBK_DAX\n",
      "ULVR_FTSE\n",
      "AIR_CAC\n",
      "CBK_DAX\n",
      "BARC_FTSE\n",
      "CS_CAC\n",
      "BAYN_DAX\n",
      "STAN_FTSE\n",
      "PRU_FTSE\n",
      "RSA_FTSE\n",
      "CAP_CAC\n",
      "DIS_DJIA\n",
      "DPW_DAX\n",
      "LIN_DAX\n",
      "ADM_FTSE\n",
      "WPP_FTSE\n",
      "WMT_DJIA\n",
      "EOAN_DAX\n",
      "SRP_FTSE\n",
      "MCD_DJIA\n",
      "MRO_FTSE\n",
      "ACA_CAC\n",
      "MC_CAC\n",
      "HD_DJIA\n",
      "TVE_DJIA\n",
      "RI_CAC\n",
      "BMW_DAX\n",
      "FRE_DAX\n",
      "RB_FTSE\n",
      "DAI_DAX\n",
      "UTX_DJIA\n",
      "AAL_FTSE\n",
      "EI_CAC\n",
      "WTB_FTSE\n",
      "PFE_DJIA\n",
      "AC_CAC\n",
      "SSE_FTSE\n",
      "IFX_DAX\n",
      "EDF_CAC\n",
      "TKA_DAX\n",
      "SIE_DAX\n",
      "PG_DJIA\n",
      "BAB_FTSE\n",
      "KO_DJIA\n",
      "JPM_DJIA\n",
      "LHA_DAX\n",
      "HEN3_DAX\n",
      "BA_FTSE\n",
      "SBRY_FTSE\n",
      "VED_FTSE\n",
      "SNN_FTSE\n",
      "BNP_CAC\n",
      "ALV_DAX\n",
      "DTE_DAX\n",
      "SDF_DAX\n",
      "ANTO_FTSE\n",
      "LR_CAC\n",
      "SKY_FTSE\n",
      "RRS_FTSE\n"
     ]
    }
   ],
   "source": [
    "print(\"there are %s tickers available for the experiment\" % len(tickers_fits_for_analysis))\n",
    "for ticker in tickers_fits_for_analysis:\n",
    "    print(ticker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save companies with complete years series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_tickers_for_analysis, \"w\") as f_w:\n",
    "    for ticker in tickers_fits_for_analysis:\n",
    "        f_w.write(\"%s\\n\" % ticker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stock data completeness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate over tickers available for analisys and build a return table, show a logs for missing stok data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data_raw/prices/ready/MKS_FTSE.csv\n",
      "Reading ../data_raw/prices/ready/MKS_FTSE.csv\n",
      "../data_raw/prices/ready/PFC_FTSE.csv\n",
      "Reading ../data_raw/prices/ready/PFC_FTSE.csv\n",
      "../data_raw/prices/ready/MRW_FTSE.csv\n",
      "Reading ../data_raw/prices/ready/MRW_FTSE.csv\n",
      "../data_raw/prices/ready/GS_DJIA.csv\n",
      "Reading ../data_raw/prices/ready/GS_DJIA.csv\n",
      "../data_raw/prices/ready/OR_CAC.csv\n",
      "Reading ../data_raw/prices/ready/OR_CAC.csv\n",
      "../data_raw/prices/ready/CSCO_DJIA.csv\n",
      "Reading ../data_raw/prices/ready/CSCO_DJIA.csv\n",
      "../data_raw/prices/ready/SHP_FTSE.csv\n",
      "Reading ../data_raw/prices/ready/SHP_FTSE.csv\n",
      "../data_raw/prices/ready/REL_FTSE.csv\n",
      "Reading ../data_raw/prices/ready/REL_FTSE.csv\n",
      "../data_raw/prices/ready/SAP_DAX.csv\n",
      "Reading ../data_raw/prices/ready/SAP_DAX.csv\n",
      "../data_raw/prices/ready/VOD_FTSE.csv\n",
      "Reading ../data_raw/prices/ready/VOD_FTSE.csv\n",
      "../data_raw/prices/ready/MUV2_DAX.csv\n",
      "Reading ../data_raw/prices/ready/MUV2_DAX.csv\n",
      "../data_raw/prices/ready/VZ_DJIA.csv\n",
      "Reading ../data_raw/prices/ready/VZ_DJIA.csv\n",
      "../data_raw/prices/ready/CVX_DJIA.csv\n",
      "Reading ../data_raw/prices/ready/CVX_DJIA.csv\n",
      "../data_raw/prices/ready/SLA_FTSE.csv\n",
      "Reading ../data_raw/prices/ready/SLA_FTSE.csv\n",
      "../data_raw/prices/ready/AGK_FTSE.csv\n",
      "Reading ../data_raw/prices/ready/AGK_FTSE.csv\n",
      "../data_raw/prices/ready/RBS_FTSE.csv\n",
      "Reading ../data_raw/prices/ready/RBS_FTSE.csv\n",
      "../data_raw/prices/ready/RWE_DAX.csv\n",
      "Reading ../data_raw/prices/ready/RWE_DAX.csv\n",
      "../data_raw/prices/ready/HSBC_DJIA.csv\n",
      "Reading ../data_raw/prices/ready/HSBC_DJIA.csv\n",
      "../data_raw/prices/ready/LLOY_FTSE.csv\n",
      "Reading ../data_raw/prices/ready/LLOY_FTSE.csv\n",
      "../data_raw/prices/ready/WEIR_FTSE.csv\n",
      "Reading ../data_raw/prices/ready/WEIR_FTSE.csv\n",
      "../data_raw/prices/ready/TSCO_FTSE.csv\n",
      "Reading ../data_raw/prices/ready/TSCO_FTSE.csv\n",
      "../data_raw/prices/ready/DBK_DAX.csv\n",
      "Reading ../data_raw/prices/ready/DBK_DAX.csv\n",
      "../data_raw/prices/ready/ULVR_FTSE.csv\n",
      "Reading ../data_raw/prices/ready/ULVR_FTSE.csv\n",
      "../data_raw/prices/ready/AIR_CAC.csv\n",
      "Reading ../data_raw/prices/ready/AIR_CAC.csv\n",
      "../data_raw/prices/ready/CBK_DAX.csv\n",
      "Reading ../data_raw/prices/ready/CBK_DAX.csv\n",
      "../data_raw/prices/ready/BARC_FTSE.csv\n",
      "Reading ../data_raw/prices/ready/BARC_FTSE.csv\n",
      "../data_raw/prices/ready/CS_CAC.csv\n",
      "Reading ../data_raw/prices/ready/CS_CAC.csv\n",
      "../data_raw/prices/ready/BAYN_DAX.csv\n",
      "Reading ../data_raw/prices/ready/BAYN_DAX.csv\n",
      "../data_raw/prices/ready/STAN_FTSE.csv\n",
      "Reading ../data_raw/prices/ready/STAN_FTSE.csv\n",
      "../data_raw/prices/ready/PRU_FTSE.csv\n",
      "Reading ../data_raw/prices/ready/PRU_FTSE.csv\n",
      "../data_raw/prices/ready/RSA_FTSE.csv\n",
      "Reading ../data_raw/prices/ready/RSA_FTSE.csv\n",
      "../data_raw/prices/ready/CAP_CAC.csv\n",
      "Reading ../data_raw/prices/ready/CAP_CAC.csv\n",
      "../data_raw/prices/ready/DIS_DJIA.csv\n",
      "Reading ../data_raw/prices/ready/DIS_DJIA.csv\n",
      "../data_raw/prices/ready/DPW_DAX.csv\n",
      "Reading ../data_raw/prices/ready/DPW_DAX.csv\n",
      "../data_raw/prices/ready/LIN_DAX.csv\n",
      "Reading ../data_raw/prices/ready/LIN_DAX.csv\n",
      "../data_raw/prices/ready/ADM_FTSE.csv\n",
      "Reading ../data_raw/prices/ready/ADM_FTSE.csv\n",
      "../data_raw/prices/ready/WPP_FTSE.csv\n",
      "Reading ../data_raw/prices/ready/WPP_FTSE.csv\n",
      "../data_raw/prices/ready/WMT_DJIA.csv\n",
      "Reading ../data_raw/prices/ready/WMT_DJIA.csv\n",
      "../data_raw/prices/ready/EOAN_DAX.csv\n",
      "Reading ../data_raw/prices/ready/EOAN_DAX.csv\n",
      "../data_raw/prices/ready/SRP_FTSE.csv\n",
      "Reading ../data_raw/prices/ready/SRP_FTSE.csv\n",
      "../data_raw/prices/ready/MCD_DJIA.csv\n",
      "Reading ../data_raw/prices/ready/MCD_DJIA.csv\n",
      "../data_raw/prices/ready/MRO_FTSE.csv\n",
      "Reading ../data_raw/prices/ready/MRO_FTSE.csv\n",
      "../data_raw/prices/ready/ACA_CAC.csv\n",
      "Reading ../data_raw/prices/ready/ACA_CAC.csv\n",
      "../data_raw/prices/ready/MC_CAC.csv\n",
      "Reading ../data_raw/prices/ready/MC_CAC.csv\n",
      "../data_raw/prices/ready/HD_DJIA.csv\n",
      "Reading ../data_raw/prices/ready/HD_DJIA.csv\n",
      "../data_raw/prices/ready/TVE_DJIA.csv\n",
      "Reading ../data_raw/prices/ready/TVE_DJIA.csv\n",
      "../data_raw/prices/ready/RI_CAC.csv\n",
      "Reading ../data_raw/prices/ready/RI_CAC.csv\n",
      "../data_raw/prices/ready/BMW_DAX.csv\n",
      "Reading ../data_raw/prices/ready/BMW_DAX.csv\n",
      "../data_raw/prices/ready/FRE_DAX.csv\n",
      "Reading ../data_raw/prices/ready/FRE_DAX.csv\n",
      "../data_raw/prices/ready/RB_FTSE.csv\n",
      "Reading ../data_raw/prices/ready/RB_FTSE.csv\n",
      "../data_raw/prices/ready/DAI_DAX.csv\n",
      "Reading ../data_raw/prices/ready/DAI_DAX.csv\n",
      "../data_raw/prices/ready/UTX_DJIA.csv\n",
      "Reading ../data_raw/prices/ready/UTX_DJIA.csv\n",
      "../data_raw/prices/ready/AAL_FTSE.csv\n",
      "Reading ../data_raw/prices/ready/AAL_FTSE.csv\n",
      "../data_raw/prices/ready/EI_CAC.csv\n",
      "Reading ../data_raw/prices/ready/EI_CAC.csv\n",
      "../data_raw/prices/ready/WTB_FTSE.csv\n",
      "Reading ../data_raw/prices/ready/WTB_FTSE.csv\n",
      "../data_raw/prices/ready/PFE_DJIA.csv\n",
      "Reading ../data_raw/prices/ready/PFE_DJIA.csv\n",
      "../data_raw/prices/ready/AC_CAC.csv\n",
      "Reading ../data_raw/prices/ready/AC_CAC.csv\n",
      "../data_raw/prices/ready/SSE_FTSE.csv\n",
      "Reading ../data_raw/prices/ready/SSE_FTSE.csv\n",
      "../data_raw/prices/ready/IFX_DAX.csv\n",
      "Reading ../data_raw/prices/ready/IFX_DAX.csv\n",
      "../data_raw/prices/ready/EDF_CAC.csv\n",
      "Reading ../data_raw/prices/ready/EDF_CAC.csv\n",
      "../data_raw/prices/ready/TKA_DAX.csv\n",
      "Reading ../data_raw/prices/ready/TKA_DAX.csv\n",
      "../data_raw/prices/ready/SIE_DAX.csv\n",
      "Reading ../data_raw/prices/ready/SIE_DAX.csv\n",
      "../data_raw/prices/ready/PG_DJIA.csv\n",
      "Reading ../data_raw/prices/ready/PG_DJIA.csv\n",
      "../data_raw/prices/ready/BAB_FTSE.csv\n",
      "Reading ../data_raw/prices/ready/BAB_FTSE.csv\n",
      "../data_raw/prices/ready/KO_DJIA.csv\n",
      "Reading ../data_raw/prices/ready/KO_DJIA.csv\n",
      "../data_raw/prices/ready/JPM_DJIA.csv\n",
      "Reading ../data_raw/prices/ready/JPM_DJIA.csv\n",
      "../data_raw/prices/ready/LHA_DAX.csv\n",
      "Reading ../data_raw/prices/ready/LHA_DAX.csv\n",
      "../data_raw/prices/ready/HEN3_DAX.csv\n",
      "Reading ../data_raw/prices/ready/HEN3_DAX.csv\n",
      "../data_raw/prices/ready/BA_FTSE.csv\n",
      "Reading ../data_raw/prices/ready/BA_FTSE.csv\n",
      "../data_raw/prices/ready/SBRY_FTSE.csv\n",
      "Reading ../data_raw/prices/ready/SBRY_FTSE.csv\n",
      "../data_raw/prices/ready/VED_FTSE.csv\n",
      "Reading ../data_raw/prices/ready/VED_FTSE.csv\n",
      "../data_raw/prices/ready/SNN_FTSE.csv\n",
      "Reading ../data_raw/prices/ready/SNN_FTSE.csv\n",
      "../data_raw/prices/ready/BNP_CAC.csv\n",
      "Reading ../data_raw/prices/ready/BNP_CAC.csv\n",
      "../data_raw/prices/ready/ALV_DAX.csv\n",
      "Reading ../data_raw/prices/ready/ALV_DAX.csv\n",
      "../data_raw/prices/ready/DTE_DAX.csv\n",
      "Reading ../data_raw/prices/ready/DTE_DAX.csv\n",
      "../data_raw/prices/ready/SDF_DAX.csv\n",
      "Reading ../data_raw/prices/ready/SDF_DAX.csv\n",
      "../data_raw/prices/ready/ANTO_FTSE.csv\n",
      "Reading ../data_raw/prices/ready/ANTO_FTSE.csv\n",
      "../data_raw/prices/ready/LR_CAC.csv\n",
      "Reading ../data_raw/prices/ready/LR_CAC.csv\n",
      "../data_raw/prices/ready/SKY_FTSE.csv\n",
      "Reading ../data_raw/prices/ready/SKY_FTSE.csv\n",
      "../data_raw/prices/ready/RRS_FTSE.csv\n",
      "Reading ../data_raw/prices/ready/RRS_FTSE.csv\n"
     ]
    }
   ],
   "source": [
    "tickers_prices_table = {}\n",
    "for ticker in tickers_fits_for_analysis:\n",
    "    file_ticker_prices = os.path.join(dir_ticker_prices_source, ticker + \".csv\")\n",
    "    print(file_ticker_prices)\n",
    "    if os.path.isfile(file_ticker_prices):\n",
    "        print(\"Reading %s\" % file_ticker_prices)\n",
    "        price_df = pd.read_csv(file_ticker_prices)\n",
    "        price_df[\"Date\"] = pd.to_datetime(price_df[\"Date\"])\n",
    "        price_df.sort_values(by=[\"Date\"], inplace=True)\n",
    "        price_df.set_index(\"Date\", inplace=True)\n",
    "        ticker_data = {}\n",
    "        \n",
    "        prev_day = None\n",
    "        date_stat_price = dt.datetime.strptime(\"%s-01-01\" % (date_start.year + 1), \"%Y-%m-%d\")\n",
    "        \n",
    "        for index, day in price_df[date_stat_price : date_end].iterrows():\n",
    "            if prev_day is None:\n",
    "                ticker_data[index] = 1\n",
    "            else:\n",
    "                ticker_data[index] = day[\"Adj Close\"] / prev_day[\"Adj Close\"]\n",
    "            prev_day = day\n",
    "        tickers_prices_table[ticker] = ticker_data\n",
    "    else:\n",
    "        print(\"Stock data is missing for %s\" % ticker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to pandas data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_return = pd.DataFrame.from_dict(tickers_prices_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_return_table = os.path.join(dir_ticker_prices_destination, \"all-returns.csv\")\n",
    "df_return.to_csv(file_return_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Industry indices data completeness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read mapping file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_ticker_mappings = os.path.join(dir_data_raw, \"topics_industries_mapping\")\n",
    "file_mapping = os.path.join(dir_ticker_mappings, \"mapping.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_industries = []\n",
    "with open(file_mapping, \"r\") as f_r:\n",
    "    mappings = json.load(f_r)\n",
    "    for mapping in mappings:\n",
    "        if mapping[\"ticker\"]:\n",
    "            ticker_industries.append(mapping[\"ticker\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "['GSPC', 'XLF', 'KIE', 'XLK', 'XTN', 'XHS', 'XME', 'PEJ', 'XLC', 'XHB', 'XLB', 'XLP', 'XLV', 'XLI', 'XLRE', 'XAR', 'CARZ', 'XLE', 'XHE', 'XLY']\n"
     ]
    }
   ],
   "source": [
    "print(len(ticker_industries))\n",
    "print(ticker_industries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_ticker_industries_prices = os.path.join(dir_data_raw, \"prices\", \"indicies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate over tickers available for mapping to topics and build a return table, show a logs for missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data_raw/prices/indicies/GSPC.csv\n",
      "../data_raw/prices/indicies/XLF.csv\n",
      "../data_raw/prices/indicies/KIE.csv\n",
      "../data_raw/prices/indicies/XLK.csv\n",
      "../data_raw/prices/indicies/XTN.csv\n",
      "../data_raw/prices/indicies/XHS.csv\n",
      "../data_raw/prices/indicies/XME.csv\n",
      "../data_raw/prices/indicies/PEJ.csv\n",
      "../data_raw/prices/indicies/XLC.csv\n",
      "../data_raw/prices/indicies/XHB.csv\n",
      "../data_raw/prices/indicies/XLB.csv\n",
      "../data_raw/prices/indicies/XLP.csv\n",
      "../data_raw/prices/indicies/XLV.csv\n",
      "../data_raw/prices/indicies/XLI.csv\n",
      "../data_raw/prices/indicies/XLRE.csv\n",
      "../data_raw/prices/indicies/XAR.csv\n",
      "../data_raw/prices/indicies/CARZ.csv\n",
      "../data_raw/prices/indicies/XLE.csv\n",
      "../data_raw/prices/indicies/XHE.csv\n",
      "../data_raw/prices/indicies/XLY.csv\n"
     ]
    }
   ],
   "source": [
    "tickers_industries_prices_table = {}\n",
    "for ticker in ticker_industries:\n",
    "    file_ticker_prices = os.path.join(dir_ticker_industries_prices, ticker + \".csv\")\n",
    "    print(file_ticker_prices)\n",
    "    if os.path.isfile(file_ticker_prices):\n",
    "        price_df = pd.read_csv(file_ticker_prices)\n",
    "        price_df[\"Date\"] = pd.to_datetime(price_df[\"Date\"])\n",
    "        price_df.sort_values(by=[\"Date\"], inplace=True)\n",
    "        price_df.set_index(\"Date\", inplace=True)\n",
    "        ticker_data = {}\n",
    "        first_date_set = False\n",
    "        first_date_value = 1\n",
    "        \n",
    "        date_stat_price = dt.datetime.strptime(\"%s-01-01\" % (date_start.year + 1), \"%Y-%m-%d\")\n",
    "        \n",
    "        for index, day in price_df[date_stat_price : date_end].iterrows():\n",
    "            day_value = day[\"Adj Close\"] if day[\"Adj Close\"] else None\n",
    "            if first_date_set == False:\n",
    "                first_date_set = True\n",
    "                first_date_value = day[\"Adj Close\"]\n",
    "                \n",
    "            ticker_data[index] = day_value / first_date_value if first_date_set else None\n",
    "        tickers_industries_prices_table[ticker] = ticker_data\n",
    "    else:\n",
    "        print(\"Stock data is missing for %s\" % ticker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take care about NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_indeces = pd.DataFrame.from_dict(tickers_industries_prices_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill empty nontradable days with a value of previos tradable day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_indeces.fillna(method='ffill', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify those with bad series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_indeces.columns[df_indeces.isna().any()].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill all non complete serieses with ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_indeces.fillna(1.0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_indices_table = os.path.join(dir_ticker_prices_destination, \"all-industries-indices.csv\")\n",
    "df_indeces.to_csv(file_indices_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build run data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_prefix = \"run_21_22\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_run = os.path.join(dir_data_runs, run_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(dir_run):\n",
    "    os.makedirs(dir_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstuct terms dictionary from report ready directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of fixed dictionary: 21719 \n"
     ]
    }
   ],
   "source": [
    "terms_set = set()\n",
    "\n",
    "if flag_filter_by_fixed_terms:\n",
    "    terms_set = set_predifined_terms\n",
    "else:   \n",
    "    for report_name in os.listdir(dir_reports_ready):\n",
    "        full_report_name = os.path.join(dir_reports_ready, report_name)\n",
    "        if os.path.isfile(full_report_name) and report_name != \".DS_Store\":\n",
    "            ticker, tail = report_name.split('-')\n",
    "            if ticker not in tickers_fits_for_analysis:\n",
    "                continue\n",
    "            with open(full_report_name, 'r') as f_r:\n",
    "                for text_line in f_r:\n",
    "                    terms = text_line.strip().split(' ')\n",
    "                    for term in terms:\n",
    "                        if term not in terms_set:\n",
    "                            terms_set.add(term)\n",
    "\n",
    "    \n",
    "print('Size of fixed dictionary: %s ' % len(terms_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "for term in sorted(set_predifined_terms - terms_set):\n",
    "    print(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "for term in sorted(terms_set - set_predifined_terms):\n",
    "    print(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_term2id = {}\n",
    "dict_id2term = {}                       \n",
    "id_counter = 0\n",
    "                  \n",
    "term_list = sorted(list(terms_set))\n",
    "for term in term_list:\n",
    "    dict_term2id[term] = id_counter\n",
    "    dict_id2term[id_counter] = term\n",
    "    id_counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define vecorization of a report function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_report_dtm(file_report):\n",
    "    vector_report = list()\n",
    "    document_bow = dict()\n",
    "    with open(file_report, 'r') as f_r:\n",
    "        for text_line in f_r:\n",
    "            terms = text_line.strip().split(' ')\n",
    "            for term in terms:\n",
    "                term_id = dict_term2id[term]\n",
    "                if term_id not in document_bow:\n",
    "                    document_bow[term_id] = 0\n",
    "                document_bow[term_id] += 1\n",
    "            \n",
    "    for term_id, term_counter in document_bow.items():\n",
    "        vector_report.append(\"%s:%s\" % (term_id, term_counter))\n",
    "    \n",
    "    return vector_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read all reports (terms quantity map) for every ticker for every year in analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "amount_documents_in_series_dict = dict()\n",
    "documents_name_list = list()\n",
    "documents_vector_list = list()\n",
    "for year in year_series:\n",
    "    amount_documents_in_series = 0\n",
    "    # generate list of files for a year\n",
    "    regExp = re.compile('[A-Z\\d]+\\_[A-Z\\d]+\\-' + str(year) + '\\_[\\d]+\\.txt$')\n",
    "    reports_of_year = [f for f in os.listdir(dir_reports_ready) if re.search(regExp, f)]\n",
    "    reports_of_year.sort()\n",
    "    # for every reports of the year\n",
    "    for report_name in reports_of_year:\n",
    "        ticker, tail = report_name.split('-')\n",
    "        if ticker not in tickers_fits_for_analysis:\n",
    "            continue\n",
    "        amount_documents_in_series += 1\n",
    "        documents_vector_list.append(vectorize_report_dtm(os.path.join(dir_reports_ready, report_name)))\n",
    "        documents_name_list.append(report_name)\n",
    "\n",
    "    #keep track of documents in series\n",
    "    amount_documents_in_series_dict[int(year)] = amount_documents_in_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write results into files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save prefix-seq.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(dir_run, run_prefix + '-seq.dat'), 'w') as f_w:\n",
    "    f_w.write(\"%s\\n\" % len(year_series))\n",
    "    for year in sorted(amount_documents_in_series_dict.keys()):\n",
    "        f_w.write(\"%s\\n\" % amount_documents_in_series_dict[year])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save prefix-mult.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(dir_run, run_prefix + '-mult.dat'), 'w') as f_w:\n",
    "    for document in documents_vector_list:\n",
    "        f_w.write(\"%s %s\\n\" % (len(document), ' '.join(document)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save prefix-documents.dat, every document the same order with mult.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(dir_run, run_prefix + '-documents.dat'), 'w') as f_w:\n",
    "    for document in documents_name_list:\n",
    "        f_w.write(\"%s\\n\" % document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save prefix-documents.dat, every document the same order with mult.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(dir_run, run_prefix + '-terms.dat'), 'w') as f_w:\n",
    "    for term in term_list:\n",
    "        f_w.write(\"%s\\n\" % term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy returns to the run folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data_runs/run_21_22/run_21_22-returns.csv'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutil.copyfile(file_return_table, os.path.join(dir_run, run_prefix + '-returns.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy industry indices returns to the run folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data_runs/run_21_22/run_21_22-industry-returns.csv'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutil.copyfile(file_indices_table, os.path.join(dir_run, run_prefix + '-industry-returns.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save run setting to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocesssing_settings = {\n",
    "    'l1_min': l1_min,\n",
    "    'max_partition_of_doc': max_partition_of_doc,\n",
    "    'l1_max': l1_max,\n",
    "    'l2_min': l2_min,\n",
    "    'max_partition_of_ticker': max_partition_of_tickers,\n",
    "    'l2_max': l2_max,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(dir_run, run_prefix + '-preprocesssing_settings.dat'), 'w') as f_w:\n",
    "    f_w.write(json.dumps(preprocesssing_settings))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy reports for the run folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_run_reports = os.path.join(dir_run, 'reports')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(dir_run_reports):\n",
    "    os.makedirs(dir_run_reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "for report_file_name in os.listdir(dir_reports_ready):\n",
    "    path_report_src = os.path.join(dir_reports_ready, report_file_name)\n",
    "    path_report_dst = os.path.join(dir_run_reports, report_file_name)\n",
    "    \n",
    "    if report_file_name != '.DS_Store' and os.path.isfile(path_report_src):\n",
    "        shutil.copyfile(path_report_src, path_report_dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create result directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_results = os.path.join(dir_run, 'results')\n",
    "\n",
    "if not os.path.exists(dir_results):\n",
    "    os.makedirs(dir_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_results = os.path.join(dir_run, 'interpretation')\n",
    "\n",
    "if not os.path.exists(dir_results):\n",
    "    os.makedirs(dir_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EnD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
