{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import operator\n",
    "import math\n",
    "import multiprocessing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set experiment dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_start = dt.datetime.strptime(\"2005-01-01\", \"%Y-%m-%d\")\n",
    "date_end = dt.datetime.strptime(\"2018-06-30\", \"%Y-%m-%d\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set root directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_root = os.path.join('..')\n",
    "dir_data_raw = os.path.join(dir_root, \"data_raw\")\n",
    "dir_data_processing = os.path.join(dir_root, \"data_processing\")\n",
    "dir_data_runs = os.path.join(dir_root, \"data_runs\")\n",
    "dir_prices = os.path.join(dir_data_processing, \"prices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set reports directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_reports_txt = os.path.join(dir_data_raw, \"reports_txt\")\n",
    "dir_reports_words = os.path.join(dir_data_processing, \"reports_words\")\n",
    "dir_reports_terms = os.path.join(dir_data_processing, \"reports_terms\")\n",
    "dir_reports_grams = os.path.join(dir_data_processing, \"reports_gramms\")\n",
    "dir_reports_ready =  os.path.join(dir_data_processing, \"reports_ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set terms directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_terms_eliminated = os.path.join(dir_data_processing, \"terms_elemenated\")\n",
    "dir_terms_counts = os.path.join(dir_data_processing, \"terms_counts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set report name RegExt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_report_name_reg_exp = \"(?P<ticker>[A-Z1-9]+)[_-](?P<type>[A-Z]+)(?P<number>[1-9]*)(?P<subnumber>-[1-9]+)?[-_](?P<year>[0-9]{4})[_-](?P<p_year>[0-9]{4})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_debug = False\n",
    "flag_terms_filter_debug = False\n",
    "\n",
    "flag_extend_stopwords = True\n",
    "flag_test_report_names = True\n",
    "flag_filtering_with_bigramms = True\n",
    "\n",
    "flag_rerun_text_2_words = False\n",
    "flag_rerun_words_2_terms = False\n",
    "flag_rerun_terms_2_gramms = False\n",
    "flag_rerun_filter_terms = False ### keep it True, it generates the last reports ready data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test reports names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_reports_names(dir_findex, ticker):\n",
    "    dir_ticker = os.path.join(dir_findex, ticker)\n",
    "    if os.path.isdir(dir_ticker):\n",
    "        ticker_documents_amount = 0\n",
    "        ticker_code = \"%s_%s\" % (ticker, findex)\n",
    "        for report_file_name in os.listdir(dir_ticker):\n",
    "            if report_file_name == \".DS_Store\":\n",
    "                continue\n",
    "\n",
    "            match = re.search(check_report_name_reg_exp, report_file_name)\n",
    "            if not match:\n",
    "                print(\"working on %s, filename %s doesn't fit pattern\" % (ticker_code, report_file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if flag_test_report_names:\n",
    "    for findex in os.listdir(dir_reports_txt):\n",
    "        dir_findex = os.path.join(dir_reports_txt, findex)\n",
    "        if os.path.isdir(dir_findex):\n",
    "            with multiprocessing.Pool(processes=4) as pool:\n",
    "                if flag_test_report_names: \n",
    "                    pool.starmap(test_reports_names, [(dir_findex, ticker) for ticker in os.listdir(dir_findex)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get tickers for analisys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reports completeness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to find all tickers of companies which have reports for the experiment's timeframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_tickers_years = os.path.join(dir_data_processing, \"tickers\", \"tickers_years.datajson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate over all reprots and collect years of publishing for every company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = dict()\n",
    "for findex in os.listdir(dir_reports_txt):\n",
    "    dir_findex = os.path.join(dir_reports_txt, findex)\n",
    "    if os.path.isdir(dir_findex):\n",
    "        for ticker in os.listdir(dir_findex):\n",
    "            dir_ticker = os.path.join(dir_findex, ticker)\n",
    "            if os.path.isdir(dir_ticker):\n",
    "                ticker_years_set = set()\n",
    "                for report_file_name in os.listdir(dir_ticker):\n",
    "                    if report_file_name == \".DS_Store\":\n",
    "                        continue\n",
    "                    match = re.search(check_report_name_reg_exp, report_file_name)\n",
    "                    ticker_years_set.add(int(match.group(\"p_year\")))\n",
    "                tickers[\"%s_%s\" % (ticker, findex)] = sorted(ticker_years_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save years of companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_tickers_years, \"w\") as f_w:\n",
    "    for ticker in tickers:\n",
    "        f_w.write(\"%s\\n\" % json.dumps({\"ticker\": ticker, \"available_years\": tickers[ticker]}))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find tickers with reports for every year in the experiment timeframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_series = list(range(date_start.year, date_end.year)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if flag_debug:\n",
    "    print(year_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_data_tickers = os.path.join(dir_data_processing, \"tickers\")\n",
    "file_tickers_for_analysis = os.path.join(dir_data_tickers, \"ticker_for_analysis.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_fits_for_analysis = set()\n",
    "tickers_all = 0\n",
    "for ticker, available_years in tickers.items():\n",
    "    flag_complete_series = True\n",
    "    available_years_set = set(available_years)\n",
    "    for year in year_series:\n",
    "        if year not in available_years_set:\n",
    "            flag_complete_series = False\n",
    "            break\n",
    "    if flag_complete_series and os.path.exists(os.path.join(dir_prices, \"%s.csv\" % ticker)):\n",
    "        tickers_fits_for_analysis.add(ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'there are 79 tickers available for the experiment'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"there are %s tickers available for the experiment\" % len(tickers_fits_for_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save companies with complete years series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_tickers_for_analysis, \"w\") as f_w:\n",
    "    for ticker in tickers_fits_for_analysis:\n",
    "        f_w.write(\"%s\\n\" % ticker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stock data completeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_ticker_prices = os.path.join(dir_data_processing, \"prices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate over tickers available for analisys and build a return table, show a logs for missing stok data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_prices_table = {}\n",
    "for ticker in tickers_fits_for_analysis:\n",
    "    file_ticker_prices = os.path.join(dir_ticker_prices, ticker + \".csv\")\n",
    "    if os.path.isfile(file_ticker_prices):\n",
    "        price_df = pd.read_csv(file_ticker_prices)\n",
    "        price_df[\"Date\"] = pd.to_datetime(price_df[\"Date\"])\n",
    "        price_df.sort_values(by=[\"Date\"], inplace=True)\n",
    "        price_df.set_index(\"Date\", inplace=True)\n",
    "        ticker_data = {}\n",
    "        \n",
    "        prev_day = None\n",
    "        date_stat_price = dt.datetime.strptime(\"%s-01-01\" % (date_start.year + 1), \"%Y-%m-%d\")\n",
    "        \n",
    "        for index, day in price_df[date_stat_price : date_end].iterrows():\n",
    "            if prev_day is None:\n",
    "                ticker_data[index] = 1\n",
    "            else:\n",
    "                ticker_data[index] = day[\"Adj Close\"] / prev_day[\"Adj Close\"]\n",
    "            prev_day = day\n",
    "        tickers_prices_table[ticker] = ticker_data\n",
    "    else:\n",
    "        print(\"Stock data is missing for %s\" % ticker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to pandas data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_return = pd.DataFrame.from_dict(tickers_prices_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_return_table = os.path.join(dir_ticker_prices, \"all-returns.csv\")\n",
    "df_return.to_csv(file_return_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reports pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First remove all but English letters and re-save reports as a sequence of lower case words consist only from letters a-z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "regexp_to_remove = re.compile(r\"[\\dâºâãï½ã\\_]\")\n",
    "regexp_to_keep = re.compile(r\"[^a-z\\s]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_raw_text_2_words(file_report_path):\n",
    "    words = []\n",
    "    with open(os.path.join(file_report_path), \"r\",  encoding=\"ISO-8859-1\") as f_r:\n",
    "        for text_line in f_r:\n",
    "            cleaned_text = re.sub(regexp_to_keep, \" \", text_line.lower())\n",
    "            words_in_line = re.split(\"\\W+\", cleaned_text)\n",
    "            for possible_word in words_in_line:\n",
    "                word = possible_word.strip()\n",
    "                if len(word) > 1:\n",
    "                    words.append(word)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reports_2_words_processing(dir_findex, ticker, findex, years_set):\n",
    "    ticker_code = \"%s_%s\" % (ticker, findex)\n",
    "    if ticker_code not in tickers_fits_for_analysis:\n",
    "        print(\"Skip %s\" % ticker_code)\n",
    "        return\n",
    "    \n",
    "    dir_ticker = os.path.join(dir_findex, ticker)\n",
    "    good_documents_amount = 0\n",
    "    empty_documents_amount = 0\n",
    "    \n",
    "    if os.path.isdir(dir_ticker):\n",
    "        for report_file_name in os.listdir(dir_ticker):\n",
    "            if report_file_name == \".DS_Store\":\n",
    "                continue\n",
    "            match = re.search(check_report_name_reg_exp, report_file_name)\n",
    "            if not match:\n",
    "                print(\"filename %s doesn't fit pattern\" % report_file_name)\n",
    "            else:\n",
    "                year = int(match.group(\"p_year\"))\n",
    "                if year not in years_set:\n",
    "                    continue\n",
    "                list_words = convert_raw_text_2_words(os.path.join(dir_ticker, report_file_name))\n",
    "                if len(list_words):\n",
    "                    good_documents_amount += 1\n",
    "                    new_file_name = \"%s_%s.txt\" % (year, good_documents_amount)\n",
    "                    new_path = os.path.join(dir_reports_words, findex, ticker)\n",
    "                    if not os.path.exists(new_path):\n",
    "                        os.makedirs(new_path)\n",
    "                    with open(os.path.join(new_path, new_file_name), \"w\") as f_w:\n",
    "                        f_w.write(\"%s\" % ' '.join(list_words))\n",
    "                else: \n",
    "                    empty_documents_amount += 1\n",
    "                    if flag_debug:\n",
    "                        print(\"report %s is empty after cleaning\" % report_file_name)\n",
    "        print(\"Done on %s, good reports: %s, empty reports: %s\" % \n",
    "              (dir_ticker, good_documents_amount, empty_documents_amount))                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run previously defined function words_2_terms_processing in pool of 4 processes to speedup the cleaning, The following cell takes quite a while, be carefull and do not rerun it without a reason, results are stored at file system\n",
    "\n",
    "**please make sure that flag_rerun_text_2_words is set to True if you want to run/re-run this preprosessing step**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if flag_rerun_text_2_words:\n",
    "    filtering_years_set = set(year_series)\n",
    "    for findex in os.listdir(dir_reports_txt):\n",
    "        dir_findex = os.path.join(dir_reports_txt, findex)\n",
    "        if os.path.isdir(dir_findex):\n",
    "            with multiprocessing.Pool(processes=4) as pool:\n",
    "                pool.starmap(reports_2_words_processing, [(dir_findex, ticker, findex, filtering_years_set) for ticker in os.listdir(dir_findex)])\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lematization and english words filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that all nltk data sets are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/Alan_Spark/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Alan_Spark/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/Alan_Spark/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create stop words set and white words set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create stop words set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "english_words = set(nltk.corpus.words.words())\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "stop_words_set = set(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extend stop words with custom stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if flag_extend_stopwords:\n",
    "    with open(os.path.join(dir_data_raw, \"english\", \"extra_stopwords.txt\"), \"r\") as f_r:\n",
    "        for text_line in f_r:\n",
    "            term = text_line.strip()\n",
    "            stop_words_set.add(term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a white list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "white_list_set = set()\n",
    "with open(os.path.join(dir_data_raw, \"english\", \"white_stopwords.txt\"), \"r\") as f_r:\n",
    "    for text_line in f_r:\n",
    "        term = text_line.strip()\n",
    "        white_list_set.add(term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a funtion checker for stop words. A word is stop word if any of the folowing true:\n",
    "- it's length shorter then 3 char\n",
    "- it contains digits\n",
    "- it appears in nltk stop words set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_stop_term(term):\n",
    "    if term in white_list_set:\n",
    "        return False\n",
    "    if len(term) < 3:\n",
    "        return True\n",
    "    return term in stop_words_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a funtion lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that en spacy data set is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /Users/Alan_Spark/opt/anaconda3/lib/python3.7/site-packages (2.2.5)\n",
      "Requirement already satisfied: spacy>=2.2.2 in /Users/Alan_Spark/opt/anaconda3/lib/python3.7/site-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/Alan_Spark/opt/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /Users/Alan_Spark/opt/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.6.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/Alan_Spark/opt/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/Alan_Spark/opt/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/Alan_Spark/opt/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /Users/Alan_Spark/opt/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.9.6)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /Users/Alan_Spark/opt/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/Alan_Spark/opt/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.44.1)\n",
      "Requirement already satisfied: thinc==7.4.0 in /Users/Alan_Spark/opt/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /Users/Alan_Spark/opt/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /Users/Alan_Spark/opt/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: setuptools in /Users/Alan_Spark/opt/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (46.1.3.post20200330)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/Alan_Spark/opt/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/Alan_Spark/opt/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.25.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/Alan_Spark/opt/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/Alan_Spark/opt/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.4.5.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/Alan_Spark/opt/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /Users/Alan_Spark/opt/anaconda3/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.6.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/Alan_Spark/opt/anaconda3/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.2.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/Users/Alan_Spark/opt/anaconda3/lib/python3.7/site-packages/en_core_web_sm -->\n",
      "/Users/Alan_Spark/opt/anaconda3/lib/python3.7/site-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "#lemmatizer = WordNetLemmatizer().lemmatize\n",
    "#stemmer = SnowballStemmer(\"english\").stem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text_line, terms, eliminated_terms):\n",
    "    doc = nlp(text_line)\n",
    "    for token in doc:\n",
    "        term = token.lemma_ if token.lemma_ != \"-PRON-\" else token.text\n",
    "        if is_stop_term(term) or term not in english_words:\n",
    "            if term not in eliminated_terms:\n",
    "                eliminated_terms[term] = 0\n",
    "            eliminated_terms[term] += 1  \n",
    "        else:\n",
    "            terms.append(term)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_words_2_terms(file_report_path, eliminated_terms):\n",
    "    terms = []\n",
    "    chunk_size = 30\n",
    "    with open(os.path.join(file_report_path), \"r\",  encoding=\"utf-8\") as f_r:\n",
    "        for text_line in f_r:\n",
    "            words_in_line = re.split(\"\\W+\", text_line)\n",
    "            size = len(words_in_line)\n",
    "            steps = int(size / chunk_size)\n",
    "            for i in range(steps):\n",
    "                tokenize(\" \".join(words_in_line[i*chunk_size:(i+1)*chunk_size]), terms, eliminated_terms)\n",
    "            tokenize(\" \".join(words_in_line[steps*chunk_size:]), terms, eliminated_terms)    \n",
    "                                 \n",
    "    return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_2_terms_processing(dir_findex, ticker):\n",
    "    dict_eliminated_terms = {}\n",
    "    dir_ticker = os.path.join(dir_findex, ticker)\n",
    "    good_documents_amount = 0\n",
    "    empty_documents_amount = 0\n",
    "    if os.path.isdir(dir_ticker):\n",
    "        for report_file_name in os.listdir(dir_ticker):\n",
    "            if report_file_name == \".DS_Store\":\n",
    "                continue\n",
    "            terms_list = convert_words_2_terms(os.path.join(dir_ticker, report_file_name), dict_eliminated_terms)\n",
    "            if len(terms_list):   \n",
    "                good_documents_amount += 1\n",
    "                new_path = os.path.join(dir_reports_terms, findex, ticker)\n",
    "                if not os.path.exists(new_path):\n",
    "                    os.makedirs(new_path)\n",
    "                with open(os.path.join(new_path, report_file_name), \"w\") as f_w:\n",
    "                    f_w.write(\"%s\" % ' '.join(terms_list))\n",
    "            else: \n",
    "                empty_documents_amount += 1\n",
    "                if flag_debug:\n",
    "                    print(\"report %s is empty after cleaning\" % report_file_name)\n",
    "        \n",
    "        if len(dict_eliminated_terms):\n",
    "            if not os.path.exists(dir_terms_eliminated):\n",
    "                os.makedirs(dir_terms_eliminated)\n",
    "            with open(os.path.join(dir_terms_eliminated, \"%s_%s.json\" % (ticker, findex)), \"w\") as f_w:\n",
    "                json.dump(dict_eliminated_terms, f_w)\n",
    "        print(\"Done on %s, good reports: %s, empty reports: %s\" % \n",
    "              (dir_ticker, good_documents_amount, empty_documents_amount))\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run previously defined function words_2_terms_processing in pool of 4 processes to speedup the cleaning, The following cell takes quite a while, be carefull and do not rerun it without a reason, results are stored at file system\n",
    "\n",
    "**please make sure that flag_rerun_words_2_terms is set to True if you want to run/re-run this preprosessing step**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "if flag_rerun_words_2_terms:\n",
    "    for findex in os.listdir(dir_reports_words):\n",
    "        dir_findex = os.path.join(dir_reports_words, findex)\n",
    "        if os.path.isdir(dir_findex):\n",
    "            with multiprocessing.Pool(processes=4) as pool:\n",
    "                pool.starmap(words_2_terms_processing, [(dir_findex, ticker) for ticker in os.listdir(dir_findex)])\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize the eliminated words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set flag_terms_filter_debug to True to print all the eliminated words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list of eliminated terms, size(170820)\n"
     ]
    }
   ],
   "source": [
    "eliminated_words = {}\n",
    "for file in os.listdir(dir_terms_eliminated):\n",
    "    if file == '.DS_Store' or file == \"all_elimintated_words\":\n",
    "        continue\n",
    "    with open(os.path.join(dir_terms_eliminated, file), \"r\") as f_r:\n",
    "        el = json.load(f_r)\n",
    "        for word, count in el.items():\n",
    "            if word not in eliminated_words:\n",
    "                eliminated_words[word] = {\"count\": 0, \"ticker\": 0}\n",
    "            eliminated_words[word][\"count\"] += count\n",
    "            eliminated_words[word][\"ticker\"] += 1\n",
    "        \n",
    "print(\"list of eliminated terms, size(%s)\" % len(eliminated_words))\n",
    "\n",
    "with open(os.path.join(dir_terms_eliminated, \"all_elimintated_words\"), \"w\") as f_w:\n",
    "    json.dump(eliminated_words, f_w)\n",
    "\n",
    "if flag_terms_filter_debug:\n",
    "    for word in sorted(eliminated_words.keys()):\n",
    "        print(\"%s, usage - total: %s, tickers %s\" % (word, eliminated_words[word][\"count\"], eliminated_words[word][\"ticker\"]))\n",
    "              \n",
    "              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Condence bigramms and trigarams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read all documents as data: list of list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_terms = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_terms(dir_findex, ticker, data):\n",
    "    dir_ticker = os.path.join(dir_findex, ticker)\n",
    "    if os.path.isdir(dir_ticker):\n",
    "        for report_file_name in os.listdir(dir_ticker):\n",
    "            file_path = os.path.join(dir_ticker, report_file_name)\n",
    "            if not os.path.isfile(file_path):\n",
    "                continue\n",
    "            terms_list = []\n",
    "            with open(file_path, 'r') as f_r:\n",
    "                for text_line in f_r:\n",
    "                    terms_list = terms_list + text_line.strip().split(\" \")\n",
    "            data.append(terms_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "if flag_rerun_terms_2_gramms:\n",
    "    for findex in os.listdir(dir_reports_terms):\n",
    "        dir_findex = os.path.join(dir_reports_terms, findex)\n",
    "        if os.path.isdir(dir_findex):\n",
    "            for ticker in os.listdir(dir_findex):\n",
    "                ticker_code = \"%s_%s\" % (ticker, findex)\n",
    "                read_data_terms(dir_findex, ticker, data_terms) \n",
    "                print(\"%s data collected\" % ticker_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = gensim.models.Phrases(data_terms, min_count=30, threshold=100) # higher threshold fewer phrases.\n",
    "#trigram = gensim.models.Phrases(bigram[data_terms], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "#trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-read all docs and concatenate bi/trigramms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_2_gramms_processing(dir_findex, ticker):\n",
    "    dir_ticker = os.path.join(dir_findex, ticker)\n",
    "    if os.path.isdir(dir_ticker):\n",
    "        for report_file_name in os.listdir(dir_ticker):\n",
    "            file_path = os.path.join(dir_ticker, report_file_name)\n",
    "            if not os.path.isfile(file_path):\n",
    "                continue\n",
    "                \n",
    "            terms_list = []\n",
    "            with open(file_path, 'r') as f_r:\n",
    "                for text_line in f_r:\n",
    "                    terms_list = terms_list + text_line.strip().split(\" \")\n",
    "            terms_list = bigram_mod[terms_list]\n",
    "            #terms_list = trigram_mod[bigram_mod[terms_list]]\n",
    "            \n",
    "            if len(terms_list):   \n",
    "                new_path = os.path.join(dir_reports_grams, findex, ticker)\n",
    "                if not os.path.exists(new_path):\n",
    "                    os.makedirs(new_path)\n",
    "                with open(os.path.join(new_path, report_file_name), \"w\") as f_w:\n",
    "                    f_w.write(\"%s\" % ' '.join(terms_list))\n",
    "            else: \n",
    "                empty_documents_amount += 1\n",
    "                if flag_debug:\n",
    "                    print(\"report %s is empty after cleaning\" % report_file_name)\n",
    "        print(\"Done on %s\" % dir_ticker)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run previously defined function words_2_terms_processing in pool of 4 processes to speedup the cleaning, The following cell takes quite a while, be carefull and do not rerun it without a reason, results are stored at file system\n",
    "\n",
    "**please make sure that flag_rerun_terms_2_gramms is set to True if you want to run/re-run this preprosessing step**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "if flag_rerun_terms_2_gramms:\n",
    "    if not os.path.exists(dir_reports_grams):\n",
    "        os.makedirs(dir_reports_grams)\n",
    "        \n",
    "    for findex in os.listdir(dir_reports_terms):\n",
    "        dir_findex = os.path.join(dir_reports_terms, findex)\n",
    "        if os.path.isdir(dir_findex):\n",
    "            with multiprocessing.Pool(processes=4) as pool:\n",
    "                pool.starmap(term_2_gramms_processing, [(\n",
    "                    dir_findex,\n",
    "                    ticker) for ticker in os.listdir(dir_findex)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get corpus statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ticker_reports_term_counting(dir_findex, ticker, dict_terms_counts):\n",
    "    number_of_repors = 0\n",
    "    dir_ticker = os.path.join(dir_findex, ticker)\n",
    "    terms_visited_ticker = set()\n",
    "    if os.path.isdir(dir_ticker):\n",
    "        ticker_documents_amount = 0\n",
    "        for report_file_name in os.listdir(dir_ticker):\n",
    "            file_path = os.path.join(dir_ticker, report_file_name)\n",
    "            if os.path.isfile(file_path):\n",
    "                terms_visited_document = set()\n",
    "                number_of_repors += 1\n",
    "                with open(file_path, 'r') as f_r:\n",
    "                    for text in f_r:\n",
    "                        for term in text.strip().split(' '):\n",
    "                            if term not in dict_terms_counts:\n",
    "                                dict_terms_counts[term] = {\n",
    "                                    \"count\": 1, \n",
    "                                    \"document\": 1, \n",
    "                                    \"tickers\": 1}\n",
    "                            else:\n",
    "                                if term not in terms_visited_ticker:\n",
    "                                    dict_terms_counts[term][\"tickers\"] += 1\n",
    "                                if term not in terms_visited_document:\n",
    "                                    dict_terms_counts[term][\"document\"] += 1\n",
    "                                dict_terms_counts[term][\"count\"] += 1\n",
    "                            terms_visited_ticker.add(term)\n",
    "                            terms_visited_document.add(term)\n",
    "    return number_of_repors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EDF_CAC terms stats collected\n",
      "AC_CAC terms stats collected\n",
      "CS_CAC terms stats collected\n",
      "ACA_CAC terms stats collected\n",
      "RI_CAC terms stats collected\n",
      "AIR_CAC terms stats collected\n",
      "CAP_CAC terms stats collected\n",
      "MC_CAC terms stats collected\n",
      "BNP_CAC terms stats collected\n",
      "LR_CAC terms stats collected\n",
      "EI_CAC terms stats collected\n",
      "OR_CAC terms stats collected\n",
      "AAL_FTSE terms stats collected\n",
      "WEIR_FTSE terms stats collected\n",
      "BARC_FTSE terms stats collected\n",
      "MKS_FTSE terms stats collected\n",
      "WPP_FTSE terms stats collected\n",
      "PFC_FTSE terms stats collected\n",
      "SSE_FTSE terms stats collected\n",
      "SKY_FTSE terms stats collected\n",
      "ANTO_FTSE terms stats collected\n",
      "SHP_FTSE terms stats collected\n",
      "PRU_FTSE terms stats collected\n",
      "SNN_FTSE terms stats collected\n",
      "BAB_FTSE terms stats collected\n",
      "RSA_FTSE terms stats collected\n",
      "MRW_FTSE terms stats collected\n",
      "WTB_FTSE terms stats collected\n",
      "RBS_FTSE terms stats collected\n",
      "VOD_FTSE terms stats collected\n",
      "REL_FTSE terms stats collected\n",
      "AGK_FTSE terms stats collected\n",
      "VED_FTSE terms stats collected\n",
      "ADM_FTSE terms stats collected\n",
      "BA_FTSE terms stats collected\n",
      "RRS_FTSE terms stats collected\n",
      "LLOY_FTSE terms stats collected\n",
      "SLA_FTSE terms stats collected\n",
      "TSCO_FTSE terms stats collected\n",
      "MRO_FTSE terms stats collected\n",
      "ULVR_FTSE terms stats collected\n",
      "SRP_FTSE terms stats collected\n",
      "SBRY_FTSE terms stats collected\n",
      "RB_FTSE terms stats collected\n",
      "VZ_DJIA terms stats collected\n",
      "PFE_DJIA terms stats collected\n",
      "WMT_DJIA terms stats collected\n",
      "CVX_DJIA terms stats collected\n",
      "GS_DJIA terms stats collected\n",
      "HD_DJIA terms stats collected\n",
      "MCD_DJIA terms stats collected\n",
      "PG_DJIA terms stats collected\n",
      "HSBC_DJIA terms stats collected\n",
      "DIS_DJIA terms stats collected\n",
      "CSCO_DJIA terms stats collected\n",
      "KO_DJIA terms stats collected\n",
      "TVE_DJIA terms stats collected\n",
      "UTX_DJIA terms stats collected\n",
      "JPM_DJIA terms stats collected\n",
      "HEN3_DAX terms stats collected\n",
      "SAP_DAX terms stats collected\n",
      "SDF_DAX terms stats collected\n",
      "LIN_DAX terms stats collected\n",
      "DAI_DAX terms stats collected\n",
      "BAYN_DAX terms stats collected\n",
      "ALV_DAX terms stats collected\n",
      "DTE_DAX terms stats collected\n",
      "CBK_DAX terms stats collected\n",
      "FRE_DAX terms stats collected\n",
      "MUV2_DAX terms stats collected\n",
      "RWE_DAX terms stats collected\n",
      "EOAN_DAX terms stats collected\n",
      "DBK_DAX terms stats collected\n",
      "SIE_DAX terms stats collected\n",
      "LHA_DAX terms stats collected\n",
      "BMW_DAX terms stats collected\n",
      "TKA_DAX terms stats collected\n",
      "DPW_DAX terms stats collected\n",
      "IFX_DAX terms stats collected\n"
     ]
    }
   ],
   "source": [
    "terms_counts = {} #key: {count: int, document: int, tickers: int}\n",
    "number_of_documents = 0\n",
    "\n",
    "for findex in os.listdir(dir_reports_terms):\n",
    "    dir_findex = os.path.join(dir_reports_terms, findex)\n",
    "    if os.path.isdir(dir_findex):\n",
    "        for ticker in os.listdir(dir_findex):\n",
    "            ticker_code = \"%s_%s\" % (ticker, findex)\n",
    "            number_of_documents += ticker_reports_term_counting(dir_findex, ticker, terms_counts) \n",
    "            print(\"%s terms stats collected\" % ticker_code)\n",
    "\n",
    "\n",
    "if not os.path.exists(dir_terms_counts):\n",
    "    os.makedirs(dir_terms_counts)            \n",
    "with open(os.path.join(dir_terms_counts, 'terms.json'), 'w') as f_w:\n",
    "    json.dump(terms_counts, f_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There're 27919 unique terms for topic analysis\n"
     ]
    }
   ],
   "source": [
    "print(\"There're %s unique terms for topic analysis\" % len(terms_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There're 4038 reports for topic analysis\n"
     ]
    }
   ],
   "source": [
    "print(\"There're %s reports for topic analysis\" % number_of_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtered term set by document frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set terms limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_min = 3 # min_number_of_doc\n",
    "max_partition_of_doc = 0.4\n",
    "l_max = int(number_of_documents * max_partition_of_doc) #max_number_of_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 3\n",
      "f2: 1615\n"
     ]
    }
   ],
   "source": [
    "print(\"f1: %s\" % l_min)\n",
    "print(\"f2: %s\" % l_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build eliminated terms set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eliminated by l_min: 6819\n",
      "eliminated by l_max: 1209\n",
      "eliminated 8028\n"
     ]
    }
   ],
   "source": [
    "set_eliminated_by_max_df = set()\n",
    "set_eliminated_by_min_df = set()\n",
    "\n",
    "#terms_counts = {} #key: {count: int, document: int, tickers: int}\n",
    "\n",
    "for term, stats in terms_counts.items():\n",
    "    df = int(stats[\"document\"] )\n",
    "    if df < l_min:\n",
    "        set_eliminated_by_min_df.add(term)\n",
    "    elif df > l_max:\n",
    "        set_eliminated_by_max_df.add(term)\n",
    "\n",
    "print(\"eliminated by l_min: %s\" % len(set_eliminated_by_min_df))\n",
    "print(\"eliminated by l_max: %s\" % len(set_eliminated_by_max_df))\n",
    "print(\"eliminated %s\" % (len(set_eliminated_by_min_df) + len(set_eliminated_by_max_df)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set flag_terms_filter_debug to True to print eliminated words sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "if flag_terms_filter_debug:\n",
    "    for term in sorted(set_eliminated_by_min_df):\n",
    "        print(term)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "if flag_terms_filter_debug:\n",
    "    for term in sorted(set_eliminated_by_max_df):\n",
    "        print(term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_terms(file_report_path):\n",
    "    result = []\n",
    "    with open(file_report_path, 'r',  encoding='utf-8') as f_r:\n",
    "        for text_line in f_r:\n",
    "            terms = re.split('\\W+', text_line.strip())\n",
    "            for term in terms:\n",
    "                if term in set_eliminated_by_min_df:\n",
    "                    continue\n",
    "                if term in set_eliminated_by_max_df:\n",
    "                    continue\n",
    "                result.append(term)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def terms_filtering(dir_findex, ticker, findex):\n",
    "    dict_eliminated_terms = {}\n",
    "    dir_ticker = os.path.join(dir_findex, ticker)\n",
    "    good_documents_amount = 0\n",
    "    empty_documents_amount = 0\n",
    "    ticker_code = '%s_%s' % (ticker, findex)\n",
    "    if os.path.isdir(dir_ticker):\n",
    "        for report_file_name in os.listdir(dir_ticker):\n",
    "            full_report_name = os.path.join(dir_ticker, report_file_name)\n",
    "            if os.path.isfile(full_report_name) and report_file_name != '.DS_Store':\n",
    "                terms_list = filter_terms(full_report_name)\n",
    "                if len(terms_list):\n",
    "                    good_documents_amount += 1\n",
    "                    with open(\n",
    "                        os.path.join(dir_reports_ready, '%s-%s' % (ticker_code, report_file_name)),\n",
    "                        'w') as f_w:\n",
    "                        f_w.write('%s' % ' '.join(terms_list))\n",
    "                else: \n",
    "                    empty_documents_amount += 1\n",
    "                    if flag_debug:\n",
    "                        print('report %s is empty after cleaning' % report_file_name)\n",
    "        print('Done on %s, good reports: %s, empty reports: %s' % \n",
    "              (dir_ticker, good_documents_amount, empty_documents_amount))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run previously defined function terms_filtering in pool of 4 processes to speedup the cleaning, The following cell doesn't take much time, feel free to experiment with l_min and l_max\n",
    "\n",
    "**please make sure that flag_rerun_filter_terms is set to True if you want to run/re-run this preprosessing step**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "if flag_rerun_filter_terms:\n",
    "    dir_source = dir_reports_grams if flag_filtering_with_bigramms else dir_reports_terms\n",
    "    \n",
    "    if not os.path.exists(dir_reports_ready):\n",
    "        os.makedirs(dir_reports_ready)\n",
    "        \n",
    "    for findex in os.listdir(dir_source):\n",
    "        dir_findex = os.path.join(dir_source, findex)\n",
    "        if os.path.isdir(dir_findex):\n",
    "            with multiprocessing.Pool(processes=4) as pool:\n",
    "                pool.starmap(terms_filtering, [(\n",
    "                    dir_findex,\n",
    "                    ticker,\n",
    "                    findex) for ticker in os.listdir(dir_findex)])\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build run data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_prefix = \"run_19_xx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_run = os.path.join(dir_data_runs, run_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(dir_run):\n",
    "    os.makedirs(dir_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstuct terms dictionary from report ready directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_term2id = {}\n",
    "dict_id2term = {}\n",
    "terms_set = set()\n",
    "id_counter = 0\n",
    "\n",
    "for report_name in os.listdir(dir_reports_ready):\n",
    "    full_report_name = os.path.join(dir_reports_ready, report_name)\n",
    "    if os.path.isfile(full_report_name) and report_name != \".DS_Store\":\n",
    "        with open(full_report_name, 'r') as f_r:\n",
    "            for text_line in f_r:\n",
    "                terms = text_line.strip().split(' ')\n",
    "                for term in terms:\n",
    "                    if term not in terms_set:\n",
    "                        terms_set.add(term)\n",
    "\n",
    "term_list = sorted(list(terms_set))\n",
    "for term in term_list:\n",
    "    dict_term2id[term] = id_counter\n",
    "    dict_id2term[id_counter] = term\n",
    "    id_counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define vecorization of a report function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_report_dtm(file_report):\n",
    "    vector_report = list()\n",
    "    document_bow = dict()\n",
    "    with open(file_report, 'r') as f_r:\n",
    "        for text_line in f_r:\n",
    "            terms = text_line.strip().split(' ')\n",
    "            for term in terms:\n",
    "                term_id = dict_term2id[term]\n",
    "                if term_id not in document_bow:\n",
    "                    document_bow[term_id] = 0\n",
    "                document_bow[term_id] += 1\n",
    "            \n",
    "    for term_id, term_counter in document_bow.items():\n",
    "        vector_report.append(\"%s:%s\" % (term_id, term_counter))\n",
    "    \n",
    "    return vector_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read all reports (terms quantity map) for every ticker for every year in analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "amount_documents_in_series_dict = dict()\n",
    "documents_name_list = list()\n",
    "documents_vector_list = list()\n",
    "for year in year_series:\n",
    "    amount_documents_in_series = 0\n",
    "    # generate list of files for a year\n",
    "    regExp = re.compile('[A-Z\\d]+\\_[A-Z\\d]+\\-' + str(year) + '\\_[\\d]+\\.txt$')\n",
    "    reports_of_year = [f for f in os.listdir(dir_reports_ready) if re.search(regExp, f)]\n",
    "    reports_of_year.sort()\n",
    "    # for every reports of the year\n",
    "    for report_name in reports_of_year:\n",
    "        amount_documents_in_series += 1\n",
    "        documents_vector_list.append(vectorize_report_dtm(os.path.join(dir_reports_ready, report_name)))\n",
    "        documents_name_list.append(report_name)\n",
    "\n",
    "    #keep track of documents in series\n",
    "    amount_documents_in_series_dict[int(year)] = amount_documents_in_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write results into files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save prefix-seq.dat\n",
    "with open(os.path.join(dir_run, run_prefix + '-seq.dat'), 'w') as f_w:\n",
    "    f_w.write(\"%s\\n\" % len(year_series))\n",
    "    for year in sorted(amount_documents_in_series_dict.keys()):\n",
    "        f_w.write(\"%s\\n\" % amount_documents_in_series_dict[year])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save prefix-mult.dat\n",
    "with open(os.path.join(dir_run, run_prefix + '-mult.dat'), 'w') as f_w:\n",
    "    for document in documents_vector_list:\n",
    "        f_w.write(\"%s %s\\n\" % (len(document), ' '.join(document)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save prefix-documents.dat, every document the same order with mult.dat\n",
    "with open(os.path.join(dir_run, run_prefix + '-documents.dat'), 'w') as f_w:\n",
    "    for document in documents_name_list:\n",
    "        f_w.write(\"%s\\n\" % document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save prefix-documents.dat, every document the same order with mult.dat\n",
    "with open(os.path.join(dir_run, run_prefix + '-terms.dat'), 'w') as f_w:\n",
    "    for term in term_list:\n",
    "        f_w.write(\"%s\\n\" % term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create result directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_results = os.path.join(dir_run, 'results')\n",
    "\n",
    "if not os.path.exists(dir_results):\n",
    "    os.makedirs(dir_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_results = os.path.join(dir_run, 'interpretation')\n",
    "\n",
    "if not os.path.exists(dir_results):\n",
    "    os.makedirs(dir_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EnD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
