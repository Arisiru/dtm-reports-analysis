{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!Important the notebook should be run after pre-processing notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime as dt\n",
    "import re\n",
    "import glob \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_start = dt.datetime.strptime(\"2005-01-01\", \"%Y-%m-%d\")\n",
    "date_end = dt.datetime.strptime(\"2018-06-30\", \"%Y-%m-%d\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_series = list(range(date_start.year, date_end.year)) \n",
    "year_set = set(year_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_prefix = 'run_%s_xx' % run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_root = os.path.join('..')\n",
    "dir_data_raw = os.path.join(dir_root, \"data_raw\")\n",
    "dir_data_processing = os.path.join(dir_root, \"data_processing\")\n",
    "\n",
    "dir_reports_txt = os.path.join(dir_data_raw, \"reports_txt\")\n",
    "\n",
    "dir_reports_words = os.path.join(dir_data_processing, \"reports_words\")\n",
    "dir_reports_terms = os.path.join(dir_data_processing, \"reports_terms\")\n",
    "dir_reports_gramms = os.path.join(dir_data_processing, \"reports_gramms\")\n",
    "dir_reports_ready = os.path.join(dir_data_processing, \"reports_ready\")\n",
    "\n",
    "dir_data_runs = os.path.join(dir_root, 'data_runs')\n",
    "dir_run = os.path.join(dir_data_runs, run_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_report_name_reg_exp = \"(?P<ticker>[A-Z1-9]+)[_-](?P<type>[A-Z]+)(?P<number>[1-9]*)(?P<subnumber>-[1-9]+)?[-_](?P<year>[0-9]{4})[_-](?P<p_year>[0-9]{4})\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Companies/reports stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_data_tickers = os.path.join(dir_data_processing, 'tickers')\n",
    "file_tickers_for_analysis = os.path.join(dir_data_tickers, 'ticker_for_analysis.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_fits_for_analysis = set()\n",
    "\n",
    "with open(file_tickers_for_analysis, 'r') as f_r:\n",
    "    for text_line in f_r:\n",
    "        ticker = text_line.strip()\n",
    "        tickers_fits_for_analysis.add(ticker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Textual stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define a function to count dictionary size and average lenght"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_stats(reports):\n",
    "    all_words_counter = 0\n",
    "    real_reports_counter = 0\n",
    "    dictionary = set()\n",
    "\n",
    "    for report in reports:\n",
    "        with open(report, 'r', encoding='ISO-8859-1') as f_r: #todo check encoding in pre-processing notebook\n",
    "            real_reports_counter += 1\n",
    "            for text_line in f_r:\n",
    "                words = re.split('\\s+', text_line.strip())\n",
    "                all_words_counter += len(re.split('\\s+', text_line.strip()))\n",
    "                for word in words:\n",
    "                    dictionary.add(word)\n",
    "    \n",
    "    return {\n",
    "        \"reports\": real_reports_counter,\n",
    "        \"average_length\": all_words_counter / real_reports_counter,\n",
    "        \"dictionary size\": len(dictionary),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the length of every document available for study, raw in words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data_raw/reports_txt/**/**/*.txt\n",
      "{'reports': 8039, 'average_length': 38169.49035949745, 'dictionary size': 3002785}\n"
     ]
    }
   ],
   "source": [
    "path_glob = os.path.join(dir_reports_txt, '**', '**','*.txt')\n",
    "print(path_glob)\n",
    "\n",
    "print(get_text_stats(sorted(glob.glob(path_glob))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the length of every document eligible for study, raw in words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data_raw/reports_txt/**/**/*.txt\n",
      "{'reports': 4053, 'average_length': 43889.377004687885, 'dictionary size': 2064821}\n"
     ]
    }
   ],
   "source": [
    "path_glob = os.path.join(dir_reports_txt, '**', '**','*.txt')\n",
    "print(path_glob)\n",
    "\n",
    "reports = []\n",
    "for report in sorted(glob.glob(path_glob)):\n",
    "    components = report.split('/')\n",
    "    ticker = '%s_%s' % (components[4], components[3])\n",
    "    if ticker not in tickers_fits_for_analysis:\n",
    "        continue\n",
    "    match = re.search(check_report_name_reg_exp, components[-1])\n",
    "    publishing_year = int(match.group(\"p_year\"))\n",
    "\n",
    "    if publishing_year not in year_set:\n",
    "        continue    \n",
    "    reports.append(report)\n",
    "\n",
    "print(get_text_stats(reports))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the length of every document eligible for study, superfluous character removaland lowercase conv in words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data_processing/reports_words/**/**/*.txt\n",
      "{'reports': 4042, 'average_length': 34792.778080158336, 'dictionary size': 206804}\n"
     ]
    }
   ],
   "source": [
    "path_glob = os.path.join(dir_reports_words, '**', '**','*.txt')\n",
    "print(path_glob)\n",
    "\n",
    "reports = []\n",
    "for report in sorted(glob.glob(path_glob)):\n",
    "    components = report.split('/')\n",
    "    \n",
    "    ticker = '%s_%s' % (components[4], components[3])\n",
    "    if ticker not in tickers_fits_for_analysis:\n",
    "        continue\n",
    "    \n",
    "    match = components[-1].split('_')\n",
    "    publishing_year = int(match[0])\n",
    "    if publishing_year not in year_set:\n",
    "        continue    \n",
    "    \n",
    "    reports.append(report)\n",
    "\n",
    "print(get_text_stats(reports))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the length of every document eligible for study, noise removaland information concentration in words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data_processing/reports_terms/**/**/*.txt\n",
      "{'reports': 4039, 'average_length': 18957.568457538993, 'dictionary size': 27918}\n"
     ]
    }
   ],
   "source": [
    "path_glob = os.path.join(dir_reports_terms, '**', '**','*.txt')\n",
    "print(path_glob)\n",
    "\n",
    "reports = []\n",
    "for report in sorted(glob.glob(path_glob)):\n",
    "    components = report.split('/')\n",
    "    \n",
    "    ticker = '%s_%s' % (components[4], components[3])\n",
    "    if ticker not in tickers_fits_for_analysis:\n",
    "        continue\n",
    "    \n",
    "    match = components[-1].split('_')\n",
    "    publishing_year = int(match[0])\n",
    "    if publishing_year not in year_set:\n",
    "        continue    \n",
    "    \n",
    "    reports.append(report)\n",
    "\n",
    "print(get_text_stats(reports))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the length of every document eligible for study, bigramms information concentration in words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data_processing/reports_gramms/**/**/*.txt\n",
      "{'reports': 4039, 'average_length': 18863.85739044318, 'dictionary size': 29746}\n"
     ]
    }
   ],
   "source": [
    "path_glob = os.path.join(dir_reports_gramms, '**', '**','*.txt')\n",
    "print(path_glob)\n",
    "\n",
    "reports = []\n",
    "for report in sorted(glob.glob(path_glob)):\n",
    "    components = report.split('/')\n",
    "    \n",
    "    ticker = '%s_%s' % (components[4], components[3])\n",
    "    if ticker not in tickers_fits_for_analysis:\n",
    "        continue\n",
    "    \n",
    "    match = components[-1].split('_')\n",
    "    publishing_year = int(match[0])\n",
    "    if publishing_year not in year_set:\n",
    "        continue    \n",
    "    \n",
    "    reports.append(report)\n",
    "\n",
    "print(get_text_stats(reports))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the length of every document eligible for study, l_min l_max filters in words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data_processing/reports_ready/*.txt\n",
      "{'reports': 4039, 'average_length': 2208.8395642485766, 'dictionary size': 13621}\n"
     ]
    }
   ],
   "source": [
    "path_glob = os.path.join(dir_reports_ready,'*.txt')\n",
    "print(path_glob)\n",
    "\n",
    "print(get_text_stats(glob.glob(path_glob)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the length of average length of a document from dtm data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reports': 4039, 'average_length': 710.177766773954}\n"
     ]
    }
   ],
   "source": [
    "dir_run\n",
    "file_mult = os.path.join(dir_run, '%s-mult.dat' % run_prefix)\n",
    "with (open(file_mult, 'r')) as f_r:\n",
    "    real_reports_counter = 0\n",
    "    all_words_counter = 0\n",
    "    for text_line in f_r:\n",
    "        real_reports_counter += 1\n",
    "        all_words_counter += int(text_line.strip().split(' ')[0])\n",
    "    print({\n",
    "        \"reports\": real_reports_counter,\n",
    "        \"average_length\": all_words_counter / real_reports_counter,\n",
    "    }) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
